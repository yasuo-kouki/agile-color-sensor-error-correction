\section*{はじめに}

\subsection{実験の目的}
現代社会においてIoT機器の普及は著しく，様々なセンサーから大量のデータが生成されている\cite{soumu2021}．
これらのデータは記録や解析，遠隔伝送といった用途で広く活用される一方で，
ストレージ容量の制約が現実上の課題となっている\cite{iot_resource_management}．
特にArduino R4 WiFi（SRAM 32KB，フラッシュメモリ256KB）のような組み込みマイコンボードでは，
大規模なデータやディープランニングを扱う上で容量が制約となる．

本研究において，カラーセンサーの補正を目的としたニューラルネットワークをArduino R4 WiFiに実装を試みた際に，
メモリ不足の問題が発生した．
具体的には，入力層3ノードは，隠れ層70ノード×2層，出力層3ノードの
3層ニューラルネットワークの全結合層を実装しようとしたところ，
重み行列だけで$(3 \times 70 + 70 \times 70 + 70 \times 3) \times 4$バイト$= 21,280$バイトを消費する．
さらにバイアス項$(70 + 70 + 3) \times 4$バイト$= 572$バイト，
活性化関数の中間出力やその他の変数を含めると，総メモリ使用量は約22KB以上となり，
SRAM容量（32KB）の約7割を占有してしまう．
加えて，プログラム本体やWiFi通信用のライブラリ，スタック領域なども考慮すると，
実際に利用可能なメモリは更に限られ，プログラムが正常に動作しない事態に陥った．

したがって，限られたメモリ資源の中でニューラルネットワークモデルを実装するためには，
重み行列を効率的に圧縮する技術が不可欠である．
特にニューラルネットワークでは，学習の過程で多くの重みがほぼ0となり，
重み行列が疎行列として表されることが多い\cite{toshiba2019}．
疎行列とは大部分の要素が0である行列であるため，0以外の値とその位置情報のみを保持することで
保存すべきデータ量を大幅に削減でき，メモリ効率の向上が期待できる\cite{toshiba2019}．

本実験では代表的なデータ圧縮方式を調査し，その性能を比較・評価することで，
削減率と復元精度の両立を目指す．
具体的には，疎データに適した行列表現（CSR形式）や値の量子化を組合せて実装し，
圧縮後のデータサイズと復元時のMSE（平均二乗誤差），さらに圧縮によるメモリ削減率を定量的に測定する．
これにより，Arduino R4 WiFi等の組み込み環境においても実用可能なニューラルネットワークモデルの実装に必要な
データ圧縮手法の有効性を明らかにし，IoTデバイス上でより大きなAIの実装に向けた知見を得ることを目的とする．

\subsection{実験の理論}
本実験で検討する3つの圧縮方式について理論的な背景と選定理由を述べる．

まず，本研究ではスパース化の導入について検討する．
スパース化とは，データ中の重要度の低い要素を選択的に0にして疎な表現へ変換する処理を指す．
ニューラルネットワークにおいては，
学習により得られた重み行列の中には出力への寄与が小さい要素が多数存在することが知られており\cite{toshiba2019}，
これらを0に設定することでモデルの表現能力を大きく損なうことなく疎行列化が可能である．

スパース化の利点は，0でない要素の割合を減らすことで，
保存すべきデータ量を大きく削減できる点にある．
密行列では多くの0を含んでいても全ての要素を保持する必要があるが，
疎行列であれば0でない要素とその位置だけを記録すればよく，
メモリ使用量を効率的に抑えられる．
また，CSR形式などを用いることで，計算面でも高速化が期待できる．
しかし，スパース化によって一部の情報が失われるため，
過度に0でない要素を減らすと推論精度が低下する可能性がある．


本研究では，疎行列の格納方式としてCSR形式の導入を検討する．
CSRは，各行における0でない要素の値とその列インデックスを配列として保持し，
さらに行の先頭位置を示すポインタ配列を併せて管理する表現方式である．
この形式を用いることで，実際に計算に関与する0でない要素のみを格納でき，
行列をそのまま保存する場合と比較してメモリ使用量を大幅に削減できる．
特にニューラルネットワークの重み行列は，スパース化によって行方向に多数の0の要素が発生することが多い\cite{toshiba2019}．
そのため，本研究で扱う重み行列にCSR形式を適用することで，メモリ削減効果が期待できる．


本研究では，値領域の削減手法として量子化を導入を検討する．
量子化は，浮動小数点値を低ビット幅の整数にし，
1つの重みを表現するために必要なビット数を削減する手法である．
これにより，重み行列全体の保存容量を効率的に抑えることができる．
本研究では，スパース化に加えて量子化を併用することで，
重み行列の表現効率をより一層高め，高い圧縮効果を実現することを目的とする．



\subsection{実験方法}
本実験では，複数の圧縮手法をまとめて適用できるように実装する．
全体の手順は，スパース化，CSR形式への変換，量子化の3段階から構成される．
各段階の実装方法を以下に示す．

\subsubsection{スパース化処理}
入力データである浮動小数点の重み行列 $W$ に対してスパース化処理を行う．
絶対値が閾値 $\tau$ 未満の要素を0に置換し，0でない要素数を削減する．

\begin{equation}
    W_{ij} =
    \begin{cases}
    W_{ij}, & \text{if } |W_{ij}| \ge \tau \\ 
    0, & \text{if } |W_{ij}| < \tau
    \end{cases}
    \end{equation}

\subsubsection{CSR形式への変換}
CSR方式の実装は以下の手順で行う．
まず、圧縮対象となる学習済みの重みを取得し，
疎行列化のしきい値を設定する．取得したテンソルはNumPy配列に変換し，
配列内の絶対値がしきい値未満の要素は0に置換する．
これにより，計算上無視できる微小な値を除去し、疎行列化の効果を高めることができる．

次に，しきい値処理後の配列をCSR形式に変換する．
CSR形式は，0でない要素を格納するdata配列、列インデックスを示すindices配列，
行ごとの先頭位置を示すindptr配列，及び行列サイズを示すshape配列で構成する．

最後に，これらの配列をC++で利用可能な形に出力することで，
組み込みシステムに組み込むことが可能となる．
上記の手順をフローチャートで示すと以下のようになる．


\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.49\columnwidth}
        \centering
        \includegraphics[width=0.9\columnwidth]{../img/tex_1/no1.jpg}
        \caption{CSR形式変換のフローチャート}
        \label{fig:a}
    \end{minipage}
\end{figure}

\clearpage


\subsubsection{量子化}
量子化は，まず層内の重みの中で絶対値が最大の値を求め，
これを用いてスケールを計算する．
スケールはint8の最大値127に対応させるように設定し，
これにより元のfloat値の比率を保持したまま整数化できる．
次に各重みをこのスケールで割り，小数点以下を四捨五入することでint8に変換する．
こうして得られたint8の値はC++側でスケールを掛けることで元のfloat値に近い形に復元できる．

\subsection{評価指標}  
本実験では，圧縮手法の性能を

\begin{itemize}
  \item MSE精度
  \item モデルサイズの削減効果
  \item 推論時間
\end{itemize}

の 3 つの観点から評価する．

まず，MSE精度の評価にはMSEを用いる．
元データを $x_i$，ニューラルネットワークの出力を $\hat{x}_i$，
データの全要素数を $N$ とすると，MSE は次式で定義される．
\[
\mathrm{MSE} = \frac{1}{N}\sum_{i=1}^{N}(x_i - \hat{x}_i)^2
\]
各条件について複数回の推論を行い，
MSE の平均値を用いることで，乱数初期値などによるばらつきの影響を低減した．

次に，モデルサイズの評価には，元のモデルサイズ $S_{\mathrm{orig}}$ と
圧縮後のモデルサイズ $S_{\mathrm{comp}}$（いずれも生成された
ヘッダファイルのバイト数）から計算される
「削減率」を用いる．削減率は
\[
  \mathrm{削減率}
  = 1 - \frac{S_{\mathrm{comp}}}{S_{\mathrm{orig}}}
\]
と定義し，元のモデルからどの程度サイズを削減できたかを表す．
削減率が 1 に近いほど，モデルサイズを大きく削減できていることを意味する．

さらに，組み込み環境での実用性を考慮して，
推論時間も重要な評価指標とする．
推論時間 $T_{\mathrm{infer}}$ は，各モデルに対して同一条件のもとで
一定回数の推論を行ったときの処理時間（ミリ秒）を測定し，
モデルサイズとの関係を比較することで，
どの程度小さいモデルで，どの程度の誤差と推論時間を実現できるかを評価する．

以上のように，本実験では
MSE精度，削減率およびモデルサイズによるメモリ効率，推論時間による実行性能を総合的に評価することで，
Arduino R4 WiFi のようなメモリ制約の厳しい環境における
圧縮手法の有効性を検証する．


