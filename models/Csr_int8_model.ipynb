{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b80c35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "883d6bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 推論データ\n",
    "X =  np.array([\n",
    "    [0.63, 0.12, 0.07], [0.24, 0.88, 0.22], [0.07, 0.28, 0.40],\n",
    "    [0.88, 0.99, 0.28], [0.40, 0.98, 0.91], [0.56, 0.20, 0.27],\n",
    "    [0.00, 0.00, 0.00], [0.28, 0.53, 0.36], [0.97, 1.00, 0.99],\n",
    "\n",
    "    [0.22, 0.13, 0.06], [0.65, 0.64, 0.36], [0.15, 0.43, 0.42], [0.15, 0.37, 0.10], [0.34, 0.55, 0.55], [0.28, 0.96, 0.68],\n",
    "    [0.68, 0.38, 0.08], [0.02, 0.25, 0.37], [0.60, 0.14, 0.11], [0.11, 0.05, 0.10], [0.49, 0.97, 0.22], [0.76, 0.74, 0.16],\n",
    "    [0.01, 0.08, 0.18], [0.08, 0.59, 0.16], [0.49, 0.09, 0.03], [0.83, 0.97, 0.23], [0.59, 0.16, 0.19], [0.09, 0.57, 0.57],\n",
    "    [1.00, 1.00, 1.00], [0.68, 0.97, 0.74], [0.44, 0.71, 0.49], [0.25, 0.45, 0.30], [0.11, 0.17, 0.13], [0.00, 0.03, 0.01],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "img1 = cv2.resize(cv2.imread(\"img/reference_image1_cmyk_large.png\", cv2.IMREAD_COLOR_RGB), (3,3), interpolation=cv2.INTER_NEAREST)\n",
    "img2 = cv2.resize(cv2.imread(\"img/reference_image2_cmyk_large.png\", cv2.IMREAD_COLOR_RGB), (6,4), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "plt.imshow(img1)\n",
    "plt.show()\n",
    "plt.imshow(img2)\n",
    "plt.show()\n",
    "\n",
    "# 正解データ\n",
    "Y = np.concatenate([img1.reshape([-1, 3])/255, img2.reshape([-1, 3])/255], axis=0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800faab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 元データ\n",
    "vals = [\n",
    "    57,20,13,166,95,71,28,65,94,36,61,21,77,90,130,64,164,164,\n",
    "    193,65,13,3,42,96,163,20,22,40,3,25,122,156,41,197,104,29,\n",
    "    0,5,39,20,90,27,136,2,0,216,163,38,155,20,43,14,74,115,\n",
    "    250,248,245,166,162,161,116,112,111,66,69,68,28,33,32,0,1,0\n",
    "]\n",
    "\n",
    "# RGBに変換（0〜1）\n",
    "colors = np.array(vals, dtype=np.float32).reshape(-1, 3) / 255.0\n",
    "\n",
    "# 必要数は4×6＝24色なので足りない分は黒で補完\n",
    "if colors.shape[0] < 24:\n",
    "    padding = np.zeros((24 - colors.shape[0], 3), dtype=np.float32)\n",
    "    colors = np.vstack([colors, padding])\n",
    "\n",
    "# 4×6に整形\n",
    "img = colors.reshape(4, 6, 3)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7270ffaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 推論データ\n",
    "X = np.array([\n",
    "    [184/255, 12/255, 7/255],\n",
    "    [83/255, 151/255, 49/255],\n",
    "    [22/255, 47/255, 91/255],\n",
    "    [255/255, 215/255, 44/255],\n",
    "    [121/255, 178/255, 212/255],\n",
    "    [178/255, 36/255, 63/255],\n",
    "    [11/255, 3/255, 0/255],\n",
    "    [100/255, 87/255, 79/255],\n",
    "    [255/255, 254/255, 255/255],\n",
    "\n",
    "    [57/255, 20/255, 13/255],\n",
    "    [166/255, 95/255, 71/255],\n",
    "    [28/255, 65/255, 94/255],\n",
    "    [36/255, 61/255, 21/255],\n",
    "    [77/255, 90/255, 130/255],\n",
    "    [64/255, 164/255, 164/255],\n",
    "\n",
    "    [193/255, 65/255, 13/255],\n",
    "    [3/255, 42/255, 96/255],\n",
    "    [163/255, 20/255, 22/255],\n",
    "    [40/255, 3/255, 25/255],\n",
    "    [122/255, 156/255, 41/255],\n",
    "    [197/255, 104/255, 29/255],\n",
    "\n",
    "    [0/255, 5/255, 39/255],\n",
    "    [20/255, 90/255, 27/255],\n",
    "    [136/255, 2/255, 0/255],\n",
    "    [216/255, 163/255, 38/255],\n",
    "    [155/255, 20/255, 43/255],\n",
    "    [14/255, 74/255, 115/255],\n",
    "\n",
    "    [250/255, 248/255, 245/255],\n",
    "    [166/255, 162/255, 161/255],\n",
    "    [116/255, 112/255, 111/255],\n",
    "    [66/255, 69/255, 68/255],\n",
    "    [28/255, 33/255, 32/255],\n",
    "    [0/255, 1/255, 0/255],\n",
    "\n",
    "], dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "img1 = cv2.resize(cv2.imread(\"img/reference_image1_cmyk_large.png\", cv2.IMREAD_COLOR_RGB), (3,3), interpolation=cv2.INTER_NEAREST)\n",
    "img2 = cv2.resize(cv2.imread(\"img/reference_image2_cmyk_large.png\", cv2.IMREAD_COLOR_RGB), (6,4), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "plt.imshow(img1)\n",
    "plt.show()\n",
    "plt.imshow(img2)\n",
    "plt.show()\n",
    "\n",
    "# 正解データ\n",
    "Y = np.concatenate([img1.reshape([-1, 3])/255, img2.reshape([-1, 3])/255], axis=0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2cf0446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAM=30  Epoch [0/50000]  MSE: 0.283419  L1: 115.87  Loss: 0.283534\n",
      "PARAM=30  Epoch [500/50000]  MSE: 0.001947  L1: 94.84  Loss: 0.002042\n",
      "PARAM=30  Epoch [1000/50000]  MSE: 0.001379  L1: 94.84  Loss: 0.001474\n",
      "PARAM=30  Epoch [1500/50000]  MSE: 0.001180  L1: 94.87  Loss: 0.001274\n",
      "PARAM=30  Epoch [2000/50000]  MSE: 0.001135  L1: 94.61  Loss: 0.001229\n",
      "PARAM=30  Epoch [2500/50000]  MSE: 0.001187  L1: 94.78  Loss: 0.001281\n",
      "PARAM=30  Epoch [3000/50000]  MSE: 0.001018  L1: 95.39  Loss: 0.001113\n",
      "PARAM=30  Epoch [3500/50000]  MSE: 0.001002  L1: 97.24  Loss: 0.001099\n",
      "PARAM=30  Epoch [4000/50000]  MSE: 0.000961  L1: 96.48  Loss: 0.001058\n",
      "PARAM=30  Epoch [4500/50000]  MSE: 0.000952  L1: 95.98  Loss: 0.001048\n",
      "PARAM=30  Epoch [5000/50000]  MSE: 0.000929  L1: 95.15  Loss: 0.001024\n",
      "PARAM=30  Epoch [5500/50000]  MSE: 0.000766  L1: 97.45  Loss: 0.000863\n",
      "PARAM=30  Epoch [6000/50000]  MSE: 0.000386  L1: 104.34  Loss: 0.000490\n",
      "PARAM=30  Epoch [6500/50000]  MSE: 0.000190  L1: 108.15  Loss: 0.000298\n",
      "PARAM=30  Epoch [7000/50000]  MSE: 0.000221  L1: 111.85  Loss: 0.000333\n",
      "PARAM=30  Epoch [7500/50000]  MSE: 0.000093  L1: 112.48  Loss: 0.000205\n",
      "PARAM=30  Epoch [8000/50000]  MSE: 0.000088  L1: 111.70  Loss: 0.000199\n",
      "PARAM=30  Epoch [8500/50000]  MSE: 0.000065  L1: 110.59  Loss: 0.000175\n",
      "PARAM=30  Epoch [9000/50000]  MSE: 0.000068  L1: 110.19  Loss: 0.000178\n",
      "PARAM=30  Epoch [9500/50000]  MSE: 0.000037  L1: 109.83  Loss: 0.000147\n",
      "PARAM=30  Epoch [10000/50000]  MSE: 0.000037  L1: 109.48  Loss: 0.000147\n",
      "PARAM=30  Epoch [10500/50000]  MSE: 0.000052  L1: 108.62  Loss: 0.000161\n",
      "PARAM=30  Epoch [11000/50000]  MSE: 0.000055  L1: 107.39  Loss: 0.000162\n",
      "PARAM=30  Epoch [11500/50000]  MSE: 0.000009  L1: 106.50  Loss: 0.000116\n",
      "PARAM=30  Epoch [12000/50000]  MSE: 0.000070  L1: 105.71  Loss: 0.000175\n",
      "PARAM=30  Epoch [12500/50000]  MSE: 0.000319  L1: 104.79  Loss: 0.000424\n",
      "PARAM=30  Epoch [13000/50000]  MSE: 0.000009  L1: 104.09  Loss: 0.000113\n",
      "PARAM=30  Epoch [13500/50000]  MSE: 0.000006  L1: 102.97  Loss: 0.000109\n",
      "PARAM=30  Epoch [14000/50000]  MSE: 0.000010  L1: 102.31  Loss: 0.000112\n",
      "PARAM=30  Epoch [14500/50000]  MSE: 0.000009  L1: 101.59  Loss: 0.000110\n",
      "PARAM=30  Epoch [15000/50000]  MSE: 0.000033  L1: 100.65  Loss: 0.000133\n",
      "PARAM=30  Epoch [15500/50000]  MSE: 0.000008  L1: 99.45  Loss: 0.000108\n",
      "PARAM=30  Epoch [16000/50000]  MSE: 0.000017  L1: 98.61  Loss: 0.000116\n",
      "PARAM=30  Epoch [16500/50000]  MSE: 0.000031  L1: 97.89  Loss: 0.000129\n",
      "PARAM=30  Epoch [17000/50000]  MSE: 0.000219  L1: 97.07  Loss: 0.000316\n",
      "PARAM=30  Epoch [17500/50000]  MSE: 0.000003  L1: 96.37  Loss: 0.000099\n",
      "PARAM=30  Epoch [18000/50000]  MSE: 0.000060  L1: 95.43  Loss: 0.000155\n",
      "PARAM=30  Epoch [18500/50000]  MSE: 0.000121  L1: 94.83  Loss: 0.000216\n",
      "PARAM=30  Epoch [19000/50000]  MSE: 0.000062  L1: 94.80  Loss: 0.000157\n",
      "PARAM=30  Epoch [19500/50000]  MSE: 0.000030  L1: 93.98  Loss: 0.000124\n",
      "PARAM=30  Epoch [20000/50000]  MSE: 0.000025  L1: 93.82  Loss: 0.000119\n",
      "PARAM=30  Epoch [20500/50000]  MSE: 0.000112  L1: 93.45  Loss: 0.000205\n",
      "PARAM=30  Epoch [21000/50000]  MSE: 0.000003  L1: 92.85  Loss: 0.000096\n",
      "PARAM=30  Epoch [21500/50000]  MSE: 0.000004  L1: 92.42  Loss: 0.000097\n",
      "PARAM=30  Epoch [22000/50000]  MSE: 0.000007  L1: 91.97  Loss: 0.000099\n",
      "PARAM=30  Epoch [22500/50000]  MSE: 0.000072  L1: 91.45  Loss: 0.000163\n",
      "PARAM=30  Epoch [23000/50000]  MSE: 0.000020  L1: 90.88  Loss: 0.000111\n",
      "PARAM=30  Epoch [23500/50000]  MSE: 0.000008  L1: 90.67  Loss: 0.000098\n",
      "PARAM=30  Epoch [24000/50000]  MSE: 0.000002  L1: 90.17  Loss: 0.000093\n",
      "PARAM=30  Epoch [24500/50000]  MSE: 0.000050  L1: 90.13  Loss: 0.000140\n",
      "PARAM=30  Epoch [25000/50000]  MSE: 0.000008  L1: 89.32  Loss: 0.000097\n",
      "PARAM=30  Epoch [25500/50000]  MSE: 0.000017  L1: 89.03  Loss: 0.000106\n",
      "PARAM=30  Epoch [26000/50000]  MSE: 0.000017  L1: 88.69  Loss: 0.000105\n",
      "PARAM=30  Epoch [26500/50000]  MSE: 0.000058  L1: 88.33  Loss: 0.000147\n",
      "PARAM=30  Epoch [27000/50000]  MSE: 0.000033  L1: 87.84  Loss: 0.000121\n",
      "PARAM=30  Epoch [27500/50000]  MSE: 0.000012  L1: 87.38  Loss: 0.000099\n",
      "PARAM=30  Epoch [28000/50000]  MSE: 0.000009  L1: 87.34  Loss: 0.000096\n",
      "PARAM=30  Epoch [28500/50000]  MSE: 0.000013  L1: 87.06  Loss: 0.000100\n",
      "PARAM=30  Epoch [29000/50000]  MSE: 0.000004  L1: 86.89  Loss: 0.000090\n",
      "PARAM=30  Epoch [29500/50000]  MSE: 0.000004  L1: 86.41  Loss: 0.000090\n",
      "PARAM=30  Epoch [30000/50000]  MSE: 0.000114  L1: 86.23  Loss: 0.000201\n",
      "PARAM=30  Epoch [30500/50000]  MSE: 0.000002  L1: 86.42  Loss: 0.000088\n",
      "PARAM=30  Epoch [31000/50000]  MSE: 0.000438  L1: 86.06  Loss: 0.000525\n",
      "PARAM=30  Epoch [31500/50000]  MSE: 0.000022  L1: 85.86  Loss: 0.000108\n",
      "PARAM=30  Epoch [32000/50000]  MSE: 0.000022  L1: 85.72  Loss: 0.000108\n",
      "PARAM=30  Epoch [32500/50000]  MSE: 0.000071  L1: 85.60  Loss: 0.000157\n",
      "PARAM=30  Epoch [33000/50000]  MSE: 0.000005  L1: 85.43  Loss: 0.000091\n",
      "PARAM=30  Epoch [33500/50000]  MSE: 0.000004  L1: 85.11  Loss: 0.000090\n",
      "PARAM=30  Epoch [34000/50000]  MSE: 0.000024  L1: 85.24  Loss: 0.000109\n",
      "PARAM=30  Epoch [34500/50000]  MSE: 0.000004  L1: 85.12  Loss: 0.000089\n",
      "PARAM=30  Epoch [35000/50000]  MSE: 0.000009  L1: 85.30  Loss: 0.000094\n",
      "PARAM=30  Epoch [35500/50000]  MSE: 0.000025  L1: 84.75  Loss: 0.000110\n",
      "PARAM=30  Epoch [36000/50000]  MSE: 0.000005  L1: 84.65  Loss: 0.000090\n",
      "PARAM=30  Epoch [36500/50000]  MSE: 0.000006  L1: 84.40  Loss: 0.000090\n",
      "PARAM=30  Epoch [37000/50000]  MSE: 0.000009  L1: 84.41  Loss: 0.000093\n",
      "PARAM=30  Epoch [37500/50000]  MSE: 0.000002  L1: 84.37  Loss: 0.000086\n",
      "PARAM=30  Epoch [38000/50000]  MSE: 0.000011  L1: 84.10  Loss: 0.000095\n",
      "PARAM=30  Epoch [38500/50000]  MSE: 0.000022  L1: 84.38  Loss: 0.000106\n",
      "PARAM=30  Epoch [39000/50000]  MSE: 0.000005  L1: 83.86  Loss: 0.000089\n",
      "PARAM=30  Epoch [39500/50000]  MSE: 0.000004  L1: 83.32  Loss: 0.000087\n",
      "PARAM=30  Epoch [40000/50000]  MSE: 0.000052  L1: 84.05  Loss: 0.000136\n",
      "PARAM=30  Epoch [40500/50000]  MSE: 0.000010  L1: 83.32  Loss: 0.000093\n",
      "PARAM=30  Epoch [41000/50000]  MSE: 0.000083  L1: 83.03  Loss: 0.000166\n",
      "PARAM=30  Epoch [41500/50000]  MSE: 0.000004  L1: 83.15  Loss: 0.000087\n",
      "PARAM=30  Epoch [42000/50000]  MSE: 0.000003  L1: 82.61  Loss: 0.000086\n",
      "PARAM=30  Epoch [42500/50000]  MSE: 0.000175  L1: 82.73  Loss: 0.000258\n",
      "PARAM=30  Epoch [43000/50000]  MSE: 0.000028  L1: 82.55  Loss: 0.000110\n",
      "PARAM=30  Epoch [43500/50000]  MSE: 0.000002  L1: 82.08  Loss: 0.000084\n",
      "PARAM=30  Epoch [44000/50000]  MSE: 0.000004  L1: 81.87  Loss: 0.000086\n",
      "PARAM=30  Epoch [44500/50000]  MSE: 0.000055  L1: 81.75  Loss: 0.000137\n",
      "PARAM=30  Epoch [45000/50000]  MSE: 0.000006  L1: 81.62  Loss: 0.000087\n",
      "PARAM=30  Epoch [45500/50000]  MSE: 0.000004  L1: 81.58  Loss: 0.000085\n",
      "PARAM=30  Epoch [46000/50000]  MSE: 0.000002  L1: 81.20  Loss: 0.000083\n",
      "PARAM=30  Epoch [46500/50000]  MSE: 0.000053  L1: 80.83  Loss: 0.000134\n",
      "PARAM=30  Epoch [47000/50000]  MSE: 0.000006  L1: 80.80  Loss: 0.000087\n",
      "PARAM=30  Epoch [47500/50000]  MSE: 0.000003  L1: 80.57  Loss: 0.000084\n",
      "PARAM=30  Epoch [48000/50000]  MSE: 0.000331  L1: 80.83  Loss: 0.000412\n",
      "PARAM=30  Epoch [48500/50000]  MSE: 0.000049  L1: 81.02  Loss: 0.000130\n",
      "PARAM=30  Epoch [49000/50000]  MSE: 0.000098  L1: 80.25  Loss: 0.000178\n",
      "PARAM=30  Epoch [49500/50000]  MSE: 0.000004  L1: 79.96  Loss: 0.000084\n",
      "\n",
      "PARAM=30 予測結果：\n",
      "[[0.93031853 0.11848651 0.13785726]\n",
      " [0.41300482 0.7402455  0.2695921 ]\n",
      " [0.2202462  0.32622612 0.64022636]\n",
      " [0.96528476 0.9210257  0.07749459]\n",
      " [0.437024   0.79923844 0.8645953 ]\n",
      " [0.72410995 0.31824255 0.618978  ]\n",
      " [0.00303195 0.001963   0.00316548]\n",
      " [0.5028283  0.49946567 0.49730617]\n",
      " [1.0011253  0.999809   0.9980688 ]\n",
      " [0.4548645  0.31038484 0.25487596]\n",
      " [0.76443416 0.56044567 0.49211216]\n",
      " [0.35394263 0.47220796 0.6108065 ]\n",
      " [0.35573286 0.42346907 0.25070208]\n",
      " [0.5177226  0.4975499  0.6866321 ]\n",
      " [0.37733722 0.7411922  0.67838997]\n",
      " [0.8710556  0.4783471  0.18561593]\n",
      " [0.26938617 0.35697612 0.6560311 ]\n",
      " [0.77369195 0.3097952  0.37846708]\n",
      " [0.3651123  0.23136696 0.4117579 ]\n",
      " [0.61538804 0.73748815 0.23052874]\n",
      " [0.89579433 0.63011473 0.15640758]\n",
      " [0.15238762 0.24654725 0.56065595]\n",
      " [0.2353205  0.58024335 0.27363503]\n",
      " [0.7038656  0.2154623  0.22454405]\n",
      " [0.919399   0.7764182  0.10814027]\n",
      " [0.7532549  0.3095936  0.5686854 ]\n",
      " [0.00520472 0.5220824  0.6458808 ]\n",
      " [0.9461403  0.94605756 0.9182567 ]\n",
      " [0.78839844 0.7924012  0.78685296]\n",
      " [0.6321288  0.6347384  0.6316143 ]\n",
      " [0.480344   0.47738412 0.4774285 ]\n",
      " [0.32993612 0.33356214 0.33795786]\n",
      " [0.19650024 0.19572046 0.1944266 ]]\n",
      "C++ 用のパラメータファイル (nomal_model_parameters_30.h) を作成しました。\n",
      "\n",
      "PARAM=40  Epoch [0/50000]  MSE: 0.312109  L1: 172.45  Loss: 0.312282\n",
      "PARAM=40  Epoch [500/50000]  MSE: 0.001182  L1: 149.07  Loss: 0.001331\n",
      "PARAM=40  Epoch [1000/50000]  MSE: 0.000269  L1: 156.35  Loss: 0.000425\n",
      "PARAM=40  Epoch [1500/50000]  MSE: 0.000131  L1: 154.91  Loss: 0.000286\n",
      "PARAM=40  Epoch [2000/50000]  MSE: 0.000062  L1: 150.31  Loss: 0.000212\n",
      "PARAM=40  Epoch [2500/50000]  MSE: 0.000204  L1: 145.64  Loss: 0.000349\n",
      "PARAM=40  Epoch [3000/50000]  MSE: 0.000028  L1: 141.07  Loss: 0.000169\n",
      "PARAM=40  Epoch [3500/50000]  MSE: 0.000006  L1: 136.23  Loss: 0.000142\n",
      "PARAM=40  Epoch [4000/50000]  MSE: 0.000050  L1: 132.03  Loss: 0.000182\n",
      "PARAM=40  Epoch [4500/50000]  MSE: 0.000068  L1: 126.56  Loss: 0.000194\n",
      "PARAM=40  Epoch [5000/50000]  MSE: 0.000017  L1: 121.46  Loss: 0.000139\n",
      "PARAM=40  Epoch [5500/50000]  MSE: 0.000006  L1: 117.00  Loss: 0.000123\n",
      "PARAM=40  Epoch [6000/50000]  MSE: 0.000099  L1: 113.39  Loss: 0.000212\n",
      "PARAM=40  Epoch [6500/50000]  MSE: 0.000103  L1: 110.19  Loss: 0.000213\n",
      "PARAM=40  Epoch [7000/50000]  MSE: 0.000048  L1: 107.59  Loss: 0.000155\n",
      "PARAM=40  Epoch [7500/50000]  MSE: 0.000056  L1: 104.40  Loss: 0.000161\n",
      "PARAM=40  Epoch [8000/50000]  MSE: 0.000004  L1: 101.63  Loss: 0.000106\n",
      "PARAM=40  Epoch [8500/50000]  MSE: 0.000002  L1: 99.15  Loss: 0.000101\n",
      "PARAM=40  Epoch [9000/50000]  MSE: 0.000004  L1: 96.73  Loss: 0.000101\n",
      "PARAM=40  Epoch [9500/50000]  MSE: 0.000008  L1: 94.41  Loss: 0.000103\n",
      "PARAM=40  Epoch [10000/50000]  MSE: 0.000001  L1: 92.88  Loss: 0.000094\n",
      "PARAM=40  Epoch [10500/50000]  MSE: 0.000005  L1: 91.45  Loss: 0.000097\n",
      "PARAM=40  Epoch [11000/50000]  MSE: 0.000017  L1: 89.80  Loss: 0.000107\n",
      "PARAM=40  Epoch [11500/50000]  MSE: 0.000001  L1: 88.97  Loss: 0.000090\n",
      "PARAM=40  Epoch [12000/50000]  MSE: 0.000032  L1: 88.05  Loss: 0.000120\n",
      "PARAM=40  Epoch [12500/50000]  MSE: 0.000056  L1: 87.08  Loss: 0.000143\n",
      "PARAM=40  Epoch [13000/50000]  MSE: 0.000021  L1: 85.60  Loss: 0.000106\n",
      "PARAM=40  Epoch [13500/50000]  MSE: 0.000028  L1: 84.87  Loss: 0.000113\n",
      "PARAM=40  Epoch [14000/50000]  MSE: 0.000006  L1: 82.78  Loss: 0.000089\n",
      "PARAM=40  Epoch [14500/50000]  MSE: 0.000019  L1: 81.69  Loss: 0.000101\n",
      "PARAM=40  Epoch [15000/50000]  MSE: 0.000019  L1: 81.32  Loss: 0.000101\n",
      "PARAM=40  Epoch [15500/50000]  MSE: 0.000020  L1: 79.83  Loss: 0.000100\n",
      "PARAM=40  Epoch [16000/50000]  MSE: 0.000004  L1: 79.16  Loss: 0.000083\n",
      "PARAM=40  Epoch [16500/50000]  MSE: 0.000001  L1: 78.11  Loss: 0.000079\n",
      "PARAM=40  Epoch [17000/50000]  MSE: 0.000023  L1: 77.54  Loss: 0.000100\n",
      "PARAM=40  Epoch [17500/50000]  MSE: 0.000010  L1: 76.87  Loss: 0.000087\n",
      "PARAM=40  Epoch [18000/50000]  MSE: 0.000044  L1: 76.45  Loss: 0.000120\n",
      "PARAM=40  Epoch [18500/50000]  MSE: 0.000006  L1: 76.03  Loss: 0.000082\n",
      "PARAM=40  Epoch [19000/50000]  MSE: 0.000010  L1: 76.01  Loss: 0.000086\n",
      "PARAM=40  Epoch [19500/50000]  MSE: 0.000026  L1: 75.29  Loss: 0.000101\n",
      "PARAM=40  Epoch [20000/50000]  MSE: 0.000038  L1: 74.87  Loss: 0.000113\n",
      "PARAM=40  Epoch [20500/50000]  MSE: 0.000014  L1: 74.66  Loss: 0.000089\n",
      "PARAM=40  Epoch [21000/50000]  MSE: 0.000003  L1: 74.22  Loss: 0.000077\n",
      "PARAM=40  Epoch [21500/50000]  MSE: 0.000011  L1: 73.77  Loss: 0.000085\n",
      "PARAM=40  Epoch [22000/50000]  MSE: 0.000005  L1: 73.55  Loss: 0.000078\n",
      "PARAM=40  Epoch [22500/50000]  MSE: 0.000004  L1: 72.89  Loss: 0.000077\n",
      "PARAM=40  Epoch [23000/50000]  MSE: 0.000005  L1: 72.93  Loss: 0.000078\n",
      "PARAM=40  Epoch [23500/50000]  MSE: 0.000004  L1: 72.43  Loss: 0.000076\n",
      "PARAM=40  Epoch [24000/50000]  MSE: 0.000012  L1: 72.30  Loss: 0.000084\n",
      "PARAM=40  Epoch [24500/50000]  MSE: 0.000004  L1: 71.24  Loss: 0.000075\n",
      "PARAM=40  Epoch [25000/50000]  MSE: 0.000011  L1: 71.48  Loss: 0.000082\n",
      "PARAM=40  Epoch [25500/50000]  MSE: 0.000002  L1: 70.64  Loss: 0.000072\n",
      "PARAM=40  Epoch [26000/50000]  MSE: 0.000001  L1: 70.58  Loss: 0.000071\n",
      "PARAM=40  Epoch [26500/50000]  MSE: 0.000004  L1: 70.37  Loss: 0.000075\n",
      "PARAM=40  Epoch [27000/50000]  MSE: 0.000014  L1: 70.23  Loss: 0.000084\n",
      "PARAM=40  Epoch [27500/50000]  MSE: 0.000001  L1: 70.19  Loss: 0.000071\n",
      "PARAM=40  Epoch [28000/50000]  MSE: 0.000006  L1: 69.94  Loss: 0.000076\n",
      "PARAM=40  Epoch [28500/50000]  MSE: 0.000003  L1: 69.81  Loss: 0.000072\n",
      "PARAM=40  Epoch [29000/50000]  MSE: 0.000002  L1: 69.43  Loss: 0.000072\n",
      "PARAM=40  Epoch [29500/50000]  MSE: 0.000013  L1: 69.81  Loss: 0.000083\n",
      "PARAM=40  Epoch [30000/50000]  MSE: 0.000013  L1: 69.24  Loss: 0.000083\n",
      "PARAM=40  Epoch [30500/50000]  MSE: 0.000007  L1: 69.24  Loss: 0.000077\n",
      "PARAM=40  Epoch [31000/50000]  MSE: 0.000014  L1: 69.30  Loss: 0.000083\n",
      "PARAM=40  Epoch [31500/50000]  MSE: 0.000006  L1: 69.51  Loss: 0.000075\n",
      "PARAM=40  Epoch [32000/50000]  MSE: 0.000002  L1: 69.21  Loss: 0.000071\n",
      "PARAM=40  Epoch [32500/50000]  MSE: 0.000002  L1: 69.22  Loss: 0.000071\n",
      "PARAM=40  Epoch [33000/50000]  MSE: 0.000000  L1: 69.27  Loss: 0.000070\n",
      "PARAM=40  Epoch [33500/50000]  MSE: 0.000005  L1: 69.20  Loss: 0.000074\n",
      "PARAM=40  Epoch [34000/50000]  MSE: 0.000001  L1: 68.76  Loss: 0.000069\n",
      "PARAM=40  Epoch [34500/50000]  MSE: 0.000001  L1: 68.63  Loss: 0.000070\n",
      "PARAM=40  Epoch [35000/50000]  MSE: 0.000000  L1: 68.77  Loss: 0.000069\n",
      "PARAM=40  Epoch [35500/50000]  MSE: 0.000016  L1: 68.72  Loss: 0.000085\n",
      "PARAM=40  Epoch [36000/50000]  MSE: 0.000029  L1: 68.61  Loss: 0.000097\n",
      "PARAM=40  Epoch [36500/50000]  MSE: 0.000033  L1: 68.40  Loss: 0.000102\n",
      "PARAM=40  Epoch [37000/50000]  MSE: 0.000005  L1: 68.47  Loss: 0.000073\n",
      "PARAM=40  Epoch [37500/50000]  MSE: 0.000003  L1: 68.36  Loss: 0.000072\n",
      "PARAM=40  Epoch [38000/50000]  MSE: 0.000001  L1: 68.10  Loss: 0.000069\n",
      "PARAM=40  Epoch [38500/50000]  MSE: 0.000026  L1: 68.59  Loss: 0.000095\n",
      "PARAM=40  Epoch [39000/50000]  MSE: 0.000006  L1: 68.18  Loss: 0.000074\n",
      "PARAM=40  Epoch [39500/50000]  MSE: 0.000002  L1: 67.66  Loss: 0.000070\n",
      "PARAM=40  Epoch [40000/50000]  MSE: 0.000001  L1: 67.78  Loss: 0.000069\n",
      "PARAM=40  Epoch [40500/50000]  MSE: 0.000009  L1: 67.87  Loss: 0.000076\n",
      "PARAM=40  Epoch [41000/50000]  MSE: 0.000001  L1: 67.84  Loss: 0.000068\n",
      "PARAM=40  Epoch [41500/50000]  MSE: 0.000005  L1: 67.57  Loss: 0.000073\n",
      "PARAM=40  Epoch [42000/50000]  MSE: 0.000001  L1: 67.29  Loss: 0.000068\n",
      "PARAM=40  Epoch [42500/50000]  MSE: 0.000003  L1: 68.01  Loss: 0.000071\n",
      "PARAM=40  Epoch [43000/50000]  MSE: 0.000002  L1: 67.40  Loss: 0.000070\n",
      "PARAM=40  Epoch [43500/50000]  MSE: 0.000003  L1: 67.36  Loss: 0.000071\n",
      "PARAM=40  Epoch [44000/50000]  MSE: 0.000001  L1: 67.18  Loss: 0.000068\n",
      "PARAM=40  Epoch [44500/50000]  MSE: 0.000009  L1: 67.64  Loss: 0.000076\n",
      "PARAM=40  Epoch [45000/50000]  MSE: 0.000002  L1: 67.12  Loss: 0.000069\n",
      "PARAM=40  Epoch [45500/50000]  MSE: 0.000005  L1: 67.05  Loss: 0.000072\n",
      "PARAM=40  Epoch [46000/50000]  MSE: 0.000019  L1: 67.83  Loss: 0.000087\n",
      "PARAM=40  Epoch [46500/50000]  MSE: 0.000007  L1: 67.50  Loss: 0.000074\n",
      "PARAM=40  Epoch [47000/50000]  MSE: 0.000001  L1: 67.06  Loss: 0.000068\n",
      "PARAM=40  Epoch [47500/50000]  MSE: 0.000003  L1: 67.18  Loss: 0.000070\n",
      "PARAM=40  Epoch [48000/50000]  MSE: 0.000012  L1: 67.33  Loss: 0.000079\n",
      "PARAM=40  Epoch [48500/50000]  MSE: 0.000006  L1: 66.82  Loss: 0.000073\n",
      "PARAM=40  Epoch [49000/50000]  MSE: 0.000000  L1: 66.79  Loss: 0.000067\n",
      "PARAM=40  Epoch [49500/50000]  MSE: 0.000000  L1: 67.08  Loss: 0.000067\n",
      "\n",
      "PARAM=40 予測結果：\n",
      "[[9.2594999e-01 1.1744857e-01 1.3924044e-01]\n",
      " [4.1044125e-01 7.4040008e-01 2.6886833e-01]\n",
      " [2.1792549e-01 3.2766432e-01 6.4058357e-01]\n",
      " [9.6288610e-01 9.2054915e-01 7.8320831e-02]\n",
      " [4.3362007e-01 7.9816449e-01 8.6412418e-01]\n",
      " [7.2137547e-01 3.1626570e-01 6.1734110e-01]\n",
      " [1.4892332e-03 3.6880374e-03 3.4073144e-03]\n",
      " [5.0010782e-01 4.9956468e-01 4.9625373e-01]\n",
      " [9.9444854e-01 9.9743152e-01 9.9527645e-01]\n",
      " [4.5247692e-01 3.1146902e-01 2.5713739e-01]\n",
      " [7.6169682e-01 5.6061351e-01 4.9165696e-01]\n",
      " [3.5459954e-01 4.7577620e-01 6.1006576e-01]\n",
      " [3.5611930e-01 4.2408395e-01 2.4984682e-01]\n",
      " [5.1482207e-01 4.9790603e-01 6.8401241e-01]\n",
      " [3.7446719e-01 7.3974133e-01 6.7569786e-01]\n",
      " [8.6846387e-01 4.7892871e-01 1.8791801e-01]\n",
      " [2.6704112e-01 3.5827059e-01 6.5366471e-01]\n",
      " [7.7456498e-01 3.1240433e-01 3.7418330e-01]\n",
      " [3.6454022e-01 2.3088194e-01 4.0925944e-01]\n",
      " [6.1381638e-01 7.3721969e-01 2.3043132e-01]\n",
      " [8.9192593e-01 6.3028395e-01 1.5623297e-01]\n",
      " [1.5133829e-01 2.4717565e-01 5.6321812e-01]\n",
      " [2.3469049e-01 5.8146632e-01 2.7285764e-01]\n",
      " [7.0052499e-01 2.1436852e-01 2.2573447e-01]\n",
      " [9.1507155e-01 7.7668536e-01 1.0849609e-01]\n",
      " [7.5035751e-01 3.1195715e-01 5.7077414e-01]\n",
      " [3.5857037e-04 5.2231950e-01 6.4548677e-01]\n",
      " [9.4107610e-01 9.4350648e-01 9.1507220e-01]\n",
      " [7.8579074e-01 7.9122555e-01 7.8538942e-01]\n",
      " [6.2894392e-01 6.3509911e-01 6.3388348e-01]\n",
      " [4.7762048e-01 4.7879097e-01 4.7710085e-01]\n",
      " [3.2985175e-01 3.3228225e-01 3.3680916e-01]\n",
      " [1.9289921e-01 1.9680059e-01 1.9568849e-01]]\n",
      "C++ 用のパラメータファイル (nomal_model_parameters_40.h) を作成しました。\n",
      "\n",
      "PARAM=50  Epoch [0/50000]  MSE: 0.381964  L1: 233.64  Loss: 0.382198\n",
      "PARAM=50  Epoch [500/50000]  MSE: 0.001189  L1: 187.23  Loss: 0.001376\n",
      "PARAM=50  Epoch [1000/50000]  MSE: 0.000313  L1: 190.68  Loss: 0.000503\n",
      "PARAM=50  Epoch [1500/50000]  MSE: 0.000171  L1: 184.05  Loss: 0.000355\n",
      "PARAM=50  Epoch [2000/50000]  MSE: 0.000103  L1: 177.36  Loss: 0.000281\n",
      "PARAM=50  Epoch [2500/50000]  MSE: 0.000065  L1: 169.59  Loss: 0.000235\n",
      "PARAM=50  Epoch [3000/50000]  MSE: 0.000046  L1: 161.00  Loss: 0.000207\n",
      "PARAM=50  Epoch [3500/50000]  MSE: 0.000050  L1: 153.91  Loss: 0.000204\n",
      "PARAM=50  Epoch [4000/50000]  MSE: 0.000060  L1: 148.33  Loss: 0.000208\n",
      "PARAM=50  Epoch [4500/50000]  MSE: 0.000036  L1: 145.02  Loss: 0.000181\n",
      "PARAM=50  Epoch [5000/50000]  MSE: 0.000033  L1: 138.55  Loss: 0.000171\n",
      "PARAM=50  Epoch [5500/50000]  MSE: 0.000031  L1: 134.40  Loss: 0.000166\n",
      "PARAM=50  Epoch [6000/50000]  MSE: 0.000108  L1: 129.90  Loss: 0.000238\n",
      "PARAM=50  Epoch [6500/50000]  MSE: 0.000043  L1: 125.58  Loss: 0.000169\n",
      "PARAM=50  Epoch [7000/50000]  MSE: 0.000030  L1: 123.09  Loss: 0.000153\n",
      "PARAM=50  Epoch [7500/50000]  MSE: 0.000027  L1: 119.50  Loss: 0.000147\n",
      "PARAM=50  Epoch [8000/50000]  MSE: 0.000040  L1: 116.39  Loss: 0.000157\n",
      "PARAM=50  Epoch [8500/50000]  MSE: 0.000031  L1: 114.24  Loss: 0.000145\n",
      "PARAM=50  Epoch [9000/50000]  MSE: 0.000174  L1: 113.11  Loss: 0.000287\n",
      "PARAM=50  Epoch [9500/50000]  MSE: 0.000027  L1: 108.02  Loss: 0.000135\n",
      "PARAM=50  Epoch [10000/50000]  MSE: 0.000031  L1: 108.23  Loss: 0.000139\n",
      "PARAM=50  Epoch [10500/50000]  MSE: 0.000029  L1: 103.68  Loss: 0.000133\n",
      "PARAM=50  Epoch [11000/50000]  MSE: 0.000290  L1: 100.45  Loss: 0.000390\n",
      "PARAM=50  Epoch [11500/50000]  MSE: 0.000028  L1: 98.98  Loss: 0.000127\n",
      "PARAM=50  Epoch [12000/50000]  MSE: 0.000031  L1: 96.14  Loss: 0.000127\n",
      "PARAM=50  Epoch [12500/50000]  MSE: 0.000016  L1: 93.36  Loss: 0.000109\n",
      "PARAM=50  Epoch [13000/50000]  MSE: 0.000019  L1: 91.96  Loss: 0.000111\n",
      "PARAM=50  Epoch [13500/50000]  MSE: 0.000017  L1: 91.32  Loss: 0.000109\n",
      "PARAM=50  Epoch [14000/50000]  MSE: 0.000013  L1: 90.87  Loss: 0.000104\n",
      "PARAM=50  Epoch [14500/50000]  MSE: 0.000012  L1: 89.53  Loss: 0.000102\n",
      "PARAM=50  Epoch [15000/50000]  MSE: 0.000049  L1: 88.77  Loss: 0.000137\n",
      "PARAM=50  Epoch [15500/50000]  MSE: 0.000010  L1: 87.51  Loss: 0.000098\n",
      "PARAM=50  Epoch [16000/50000]  MSE: 0.000009  L1: 88.81  Loss: 0.000098\n",
      "PARAM=50  Epoch [16500/50000]  MSE: 0.000008  L1: 86.78  Loss: 0.000094\n",
      "PARAM=50  Epoch [17000/50000]  MSE: 0.000009  L1: 86.58  Loss: 0.000096\n",
      "PARAM=50  Epoch [17500/50000]  MSE: 0.000007  L1: 86.97  Loss: 0.000094\n",
      "PARAM=50  Epoch [18000/50000]  MSE: 0.000061  L1: 88.50  Loss: 0.000149\n",
      "PARAM=50  Epoch [18500/50000]  MSE: 0.000009  L1: 85.32  Loss: 0.000094\n",
      "PARAM=50  Epoch [19000/50000]  MSE: 0.000047  L1: 85.70  Loss: 0.000132\n",
      "PARAM=50  Epoch [19500/50000]  MSE: 0.000005  L1: 86.00  Loss: 0.000091\n",
      "PARAM=50  Epoch [20000/50000]  MSE: 0.000021  L1: 84.62  Loss: 0.000106\n",
      "PARAM=50  Epoch [20500/50000]  MSE: 0.000006  L1: 87.15  Loss: 0.000093\n",
      "PARAM=50  Epoch [21000/50000]  MSE: 0.000006  L1: 84.56  Loss: 0.000090\n",
      "PARAM=50  Epoch [21500/50000]  MSE: 0.000005  L1: 84.70  Loss: 0.000090\n",
      "PARAM=50  Epoch [22000/50000]  MSE: 0.000006  L1: 84.88  Loss: 0.000091\n",
      "PARAM=50  Epoch [22500/50000]  MSE: 0.000003  L1: 85.04  Loss: 0.000088\n",
      "PARAM=50  Epoch [23000/50000]  MSE: 0.000004  L1: 84.55  Loss: 0.000089\n",
      "PARAM=50  Epoch [23500/50000]  MSE: 0.000007  L1: 85.41  Loss: 0.000093\n",
      "PARAM=50  Epoch [24000/50000]  MSE: 0.000028  L1: 84.21  Loss: 0.000112\n",
      "PARAM=50  Epoch [24500/50000]  MSE: 0.000004  L1: 84.94  Loss: 0.000089\n",
      "PARAM=50  Epoch [25000/50000]  MSE: 0.000005  L1: 84.45  Loss: 0.000089\n",
      "PARAM=50  Epoch [25500/50000]  MSE: 0.000004  L1: 83.96  Loss: 0.000088\n",
      "PARAM=50  Epoch [26000/50000]  MSE: 0.000033  L1: 83.67  Loss: 0.000116\n",
      "PARAM=50  Epoch [26500/50000]  MSE: 0.000018  L1: 83.80  Loss: 0.000102\n",
      "PARAM=50  Epoch [27000/50000]  MSE: 0.000008  L1: 85.29  Loss: 0.000093\n",
      "PARAM=50  Epoch [27500/50000]  MSE: 0.000003  L1: 83.54  Loss: 0.000087\n",
      "PARAM=50  Epoch [28000/50000]  MSE: 0.000002  L1: 84.11  Loss: 0.000086\n",
      "PARAM=50  Epoch [28500/50000]  MSE: 0.000003  L1: 84.57  Loss: 0.000087\n",
      "PARAM=50  Epoch [29000/50000]  MSE: 0.000002  L1: 82.73  Loss: 0.000085\n",
      "PARAM=50  Epoch [29500/50000]  MSE: 0.000003  L1: 83.34  Loss: 0.000087\n",
      "PARAM=50  Epoch [30000/50000]  MSE: 0.000005  L1: 83.57  Loss: 0.000088\n",
      "PARAM=50  Epoch [30500/50000]  MSE: 0.000005  L1: 82.91  Loss: 0.000088\n",
      "PARAM=50  Epoch [31000/50000]  MSE: 0.000005  L1: 82.68  Loss: 0.000088\n",
      "PARAM=50  Epoch [31500/50000]  MSE: 0.000027  L1: 84.63  Loss: 0.000112\n",
      "PARAM=50  Epoch [32000/50000]  MSE: 0.000012  L1: 82.45  Loss: 0.000094\n",
      "PARAM=50  Epoch [32500/50000]  MSE: 0.000002  L1: 81.46  Loss: 0.000084\n",
      "PARAM=50  Epoch [33000/50000]  MSE: 0.000006  L1: 80.91  Loss: 0.000087\n",
      "PARAM=50  Epoch [33500/50000]  MSE: 0.000001  L1: 80.26  Loss: 0.000081\n",
      "PARAM=50  Epoch [34000/50000]  MSE: 0.000002  L1: 82.32  Loss: 0.000084\n",
      "PARAM=50  Epoch [34500/50000]  MSE: 0.000001  L1: 80.64  Loss: 0.000082\n",
      "PARAM=50  Epoch [35000/50000]  MSE: 0.000042  L1: 79.55  Loss: 0.000122\n",
      "PARAM=50  Epoch [35500/50000]  MSE: 0.000001  L1: 81.25  Loss: 0.000082\n",
      "PARAM=50  Epoch [36000/50000]  MSE: 0.000001  L1: 79.03  Loss: 0.000080\n",
      "PARAM=50  Epoch [36500/50000]  MSE: 0.000001  L1: 79.96  Loss: 0.000081\n",
      "PARAM=50  Epoch [37000/50000]  MSE: 0.000009  L1: 78.53  Loss: 0.000088\n",
      "PARAM=50  Epoch [37500/50000]  MSE: 0.000008  L1: 78.73  Loss: 0.000087\n",
      "PARAM=50  Epoch [38000/50000]  MSE: 0.000011  L1: 78.32  Loss: 0.000089\n",
      "PARAM=50  Epoch [38500/50000]  MSE: 0.000002  L1: 78.87  Loss: 0.000081\n",
      "PARAM=50  Epoch [39000/50000]  MSE: 0.000019  L1: 78.44  Loss: 0.000098\n",
      "PARAM=50  Epoch [39500/50000]  MSE: 0.000003  L1: 78.00  Loss: 0.000081\n",
      "PARAM=50  Epoch [40000/50000]  MSE: 0.000064  L1: 80.31  Loss: 0.000144\n",
      "PARAM=50  Epoch [40500/50000]  MSE: 0.000007  L1: 78.24  Loss: 0.000086\n",
      "PARAM=50  Epoch [41000/50000]  MSE: 0.000012  L1: 77.51  Loss: 0.000089\n",
      "PARAM=50  Epoch [41500/50000]  MSE: 0.000002  L1: 78.42  Loss: 0.000080\n",
      "PARAM=50  Epoch [42000/50000]  MSE: 0.000080  L1: 77.73  Loss: 0.000158\n",
      "PARAM=50  Epoch [42500/50000]  MSE: 0.000029  L1: 77.84  Loss: 0.000107\n",
      "PARAM=50  Epoch [43000/50000]  MSE: 0.000015  L1: 79.01  Loss: 0.000094\n",
      "PARAM=50  Epoch [43500/50000]  MSE: 0.000018  L1: 78.89  Loss: 0.000097\n",
      "PARAM=50  Epoch [44000/50000]  MSE: 0.000003  L1: 77.63  Loss: 0.000081\n",
      "PARAM=50  Epoch [44500/50000]  MSE: 0.000012  L1: 77.40  Loss: 0.000089\n",
      "PARAM=50  Epoch [45000/50000]  MSE: 0.000019  L1: 77.50  Loss: 0.000096\n",
      "PARAM=50  Epoch [45500/50000]  MSE: 0.000004  L1: 77.52  Loss: 0.000081\n",
      "PARAM=50  Epoch [46000/50000]  MSE: 0.000004  L1: 77.40  Loss: 0.000082\n",
      "PARAM=50  Epoch [46500/50000]  MSE: 0.000061  L1: 79.63  Loss: 0.000140\n",
      "PARAM=50  Epoch [47000/50000]  MSE: 0.000002  L1: 77.30  Loss: 0.000079\n",
      "PARAM=50  Epoch [47500/50000]  MSE: 0.000006  L1: 77.07  Loss: 0.000083\n",
      "PARAM=50  Epoch [48000/50000]  MSE: 0.000006  L1: 77.13  Loss: 0.000083\n",
      "PARAM=50  Epoch [48500/50000]  MSE: 0.000003  L1: 76.80  Loss: 0.000080\n",
      "PARAM=50  Epoch [49000/50000]  MSE: 0.000001  L1: 76.98  Loss: 0.000078\n",
      "PARAM=50  Epoch [49500/50000]  MSE: 0.000001  L1: 79.17  Loss: 0.000080\n",
      "\n",
      "PARAM=50 予測結果：\n",
      "[[0.9247446  0.12274277 0.14454013]\n",
      " [0.41114387 0.74244654 0.27264395]\n",
      " [0.21814367 0.32977283 0.64526886]\n",
      " [0.962392   0.92525053 0.07971479]\n",
      " [0.43450084 0.8019817  0.868065  ]\n",
      " [0.7185195  0.31931424 0.6223741 ]\n",
      " [0.00158533 0.00493233 0.00530832]\n",
      " [0.50045043 0.5010325  0.50074685]\n",
      " [0.99668676 1.0012231  0.99953175]\n",
      " [0.45385456 0.31026554 0.2602442 ]\n",
      " [0.7612938  0.56395066 0.49643144]\n",
      " [0.35415262 0.47634926 0.61278296]\n",
      " [0.35567164 0.42577267 0.25113934]\n",
      " [0.51624197 0.5012397  0.68879026]\n",
      " [0.37597963 0.74360764 0.6793709 ]\n",
      " [0.86792743 0.48078877 0.19025615]\n",
      " [0.2696359  0.3584658  0.65627325]\n",
      " [0.7759499  0.3159992  0.3789494 ]\n",
      " [0.3614198  0.23334347 0.41374606]\n",
      " [0.61389273 0.7400204  0.23178495]\n",
      " [0.8905634  0.6355529  0.1586932 ]\n",
      " [0.15188026 0.24852031 0.5654148 ]\n",
      " [0.23531738 0.58197105 0.2748091 ]\n",
      " [0.70014536 0.21684936 0.2296616 ]\n",
      " [0.915305   0.78070563 0.10977563]\n",
      " [0.7496968  0.31164336 0.57499456]\n",
      " [0.00247142 0.5248605  0.6474138 ]\n",
      " [0.9435088  0.9534416  0.9230623 ]\n",
      " [0.78549147 0.7960719  0.79082346]\n",
      " [0.62987727 0.63870883 0.63647926]\n",
      " [0.47524083 0.48046124 0.4801682 ]\n",
      " [0.3267334  0.33495212 0.3406408 ]\n",
      " [0.19420877 0.19770148 0.19746855]]\n",
      "C++ 用のパラメータファイル (nomal_model_parameters_50.h) を作成しました。\n",
      "\n",
      "PARAM=60  Epoch [0/50000]  MSE: 0.247766  L1: 296.98  Loss: 0.248063\n",
      "PARAM=60  Epoch [500/50000]  MSE: 0.000532  L1: 229.53  Loss: 0.000762\n",
      "PARAM=60  Epoch [1000/50000]  MSE: 0.000190  L1: 208.11  Loss: 0.000398\n",
      "PARAM=60  Epoch [1500/50000]  MSE: 0.000128  L1: 190.48  Loss: 0.000319\n",
      "PARAM=60  Epoch [2000/50000]  MSE: 0.000078  L1: 178.39  Loss: 0.000257\n",
      "PARAM=60  Epoch [2500/50000]  MSE: 0.000947  L1: 167.85  Loss: 0.001115\n",
      "PARAM=60  Epoch [3000/50000]  MSE: 0.000048  L1: 155.08  Loss: 0.000203\n",
      "PARAM=60  Epoch [3500/50000]  MSE: 0.000055  L1: 147.98  Loss: 0.000203\n",
      "PARAM=60  Epoch [4000/50000]  MSE: 0.000051  L1: 137.02  Loss: 0.000188\n",
      "PARAM=60  Epoch [4500/50000]  MSE: 0.000041  L1: 130.71  Loss: 0.000172\n",
      "PARAM=60  Epoch [5000/50000]  MSE: 0.000042  L1: 125.21  Loss: 0.000167\n",
      "PARAM=60  Epoch [5500/50000]  MSE: 0.000148  L1: 120.16  Loss: 0.000268\n",
      "PARAM=60  Epoch [6000/50000]  MSE: 0.000042  L1: 116.29  Loss: 0.000158\n",
      "PARAM=60  Epoch [6500/50000]  MSE: 0.000027  L1: 111.83  Loss: 0.000139\n",
      "PARAM=60  Epoch [7000/50000]  MSE: 0.000040  L1: 109.25  Loss: 0.000149\n",
      "PARAM=60  Epoch [7500/50000]  MSE: 0.000019  L1: 108.79  Loss: 0.000127\n",
      "PARAM=60  Epoch [8000/50000]  MSE: 0.000009  L1: 108.78  Loss: 0.000118\n",
      "PARAM=60  Epoch [8500/50000]  MSE: 0.000004  L1: 105.31  Loss: 0.000109\n",
      "PARAM=60  Epoch [9000/50000]  MSE: 0.000017  L1: 102.84  Loss: 0.000120\n",
      "PARAM=60  Epoch [9500/50000]  MSE: 0.000040  L1: 100.19  Loss: 0.000140\n",
      "PARAM=60  Epoch [10000/50000]  MSE: 0.000003  L1: 97.91  Loss: 0.000101\n",
      "PARAM=60  Epoch [10500/50000]  MSE: 0.000004  L1: 97.43  Loss: 0.000101\n",
      "PARAM=60  Epoch [11000/50000]  MSE: 0.000004  L1: 95.09  Loss: 0.000099\n",
      "PARAM=60  Epoch [11500/50000]  MSE: 0.000003  L1: 93.49  Loss: 0.000096\n",
      "PARAM=60  Epoch [12000/50000]  MSE: 0.000003  L1: 92.41  Loss: 0.000095\n",
      "PARAM=60  Epoch [12500/50000]  MSE: 0.000002  L1: 91.77  Loss: 0.000094\n",
      "PARAM=60  Epoch [13000/50000]  MSE: 0.000002  L1: 90.74  Loss: 0.000093\n",
      "PARAM=60  Epoch [13500/50000]  MSE: 0.000032  L1: 89.70  Loss: 0.000121\n",
      "PARAM=60  Epoch [14000/50000]  MSE: 0.000001  L1: 89.60  Loss: 0.000091\n",
      "PARAM=60  Epoch [14500/50000]  MSE: 0.000030  L1: 91.86  Loss: 0.000122\n",
      "PARAM=60  Epoch [15000/50000]  MSE: 0.000073  L1: 86.68  Loss: 0.000159\n",
      "PARAM=60  Epoch [15500/50000]  MSE: 0.000022  L1: 86.07  Loss: 0.000108\n",
      "PARAM=60  Epoch [16000/50000]  MSE: 0.000019  L1: 85.74  Loss: 0.000105\n",
      "PARAM=60  Epoch [16500/50000]  MSE: 0.000030  L1: 85.06  Loss: 0.000115\n",
      "PARAM=60  Epoch [17000/50000]  MSE: 0.000022  L1: 85.10  Loss: 0.000107\n",
      "PARAM=60  Epoch [17500/50000]  MSE: 0.000020  L1: 86.05  Loss: 0.000106\n",
      "PARAM=60  Epoch [18000/50000]  MSE: 0.000029  L1: 84.94  Loss: 0.000114\n",
      "PARAM=60  Epoch [18500/50000]  MSE: 0.000011  L1: 84.75  Loss: 0.000096\n",
      "PARAM=60  Epoch [19000/50000]  MSE: 0.000010  L1: 84.28  Loss: 0.000095\n",
      "PARAM=60  Epoch [19500/50000]  MSE: 0.000008  L1: 84.19  Loss: 0.000092\n",
      "PARAM=60  Epoch [20000/50000]  MSE: 0.000006  L1: 83.75  Loss: 0.000090\n",
      "PARAM=60  Epoch [20500/50000]  MSE: 0.000006  L1: 83.38  Loss: 0.000089\n",
      "PARAM=60  Epoch [21000/50000]  MSE: 0.000005  L1: 83.36  Loss: 0.000089\n",
      "PARAM=60  Epoch [21500/50000]  MSE: 0.000005  L1: 83.32  Loss: 0.000088\n",
      "PARAM=60  Epoch [22000/50000]  MSE: 0.000008  L1: 82.97  Loss: 0.000091\n",
      "PARAM=60  Epoch [22500/50000]  MSE: 0.000005  L1: 82.21  Loss: 0.000087\n",
      "PARAM=60  Epoch [23000/50000]  MSE: 0.000003  L1: 81.93  Loss: 0.000085\n",
      "PARAM=60  Epoch [23500/50000]  MSE: 0.000006  L1: 82.08  Loss: 0.000089\n",
      "PARAM=60  Epoch [24000/50000]  MSE: 0.000004  L1: 83.28  Loss: 0.000088\n",
      "PARAM=60  Epoch [24500/50000]  MSE: 0.000004  L1: 81.68  Loss: 0.000085\n",
      "PARAM=60  Epoch [25000/50000]  MSE: 0.000004  L1: 80.47  Loss: 0.000085\n",
      "PARAM=60  Epoch [25500/50000]  MSE: 0.000008  L1: 80.31  Loss: 0.000088\n",
      "PARAM=60  Epoch [26000/50000]  MSE: 0.000005  L1: 79.95  Loss: 0.000085\n",
      "PARAM=60  Epoch [26500/50000]  MSE: 0.000009  L1: 79.40  Loss: 0.000089\n",
      "PARAM=60  Epoch [27000/50000]  MSE: 0.000003  L1: 79.50  Loss: 0.000082\n",
      "PARAM=60  Epoch [27500/50000]  MSE: 0.000002  L1: 80.74  Loss: 0.000083\n",
      "PARAM=60  Epoch [28000/50000]  MSE: 0.000005  L1: 79.62  Loss: 0.000084\n",
      "PARAM=60  Epoch [28500/50000]  MSE: 0.000002  L1: 80.47  Loss: 0.000083\n",
      "PARAM=60  Epoch [29000/50000]  MSE: 0.000002  L1: 79.12  Loss: 0.000081\n",
      "PARAM=60  Epoch [29500/50000]  MSE: 0.000002  L1: 78.28  Loss: 0.000080\n",
      "PARAM=60  Epoch [30000/50000]  MSE: 0.000048  L1: 79.29  Loss: 0.000127\n",
      "PARAM=60  Epoch [30500/50000]  MSE: 0.000007  L1: 79.28  Loss: 0.000086\n",
      "PARAM=60  Epoch [31000/50000]  MSE: 0.000012  L1: 80.09  Loss: 0.000092\n",
      "PARAM=60  Epoch [31500/50000]  MSE: 0.000025  L1: 77.57  Loss: 0.000103\n",
      "PARAM=60  Epoch [32000/50000]  MSE: 0.000003  L1: 77.35  Loss: 0.000081\n",
      "PARAM=60  Epoch [32500/50000]  MSE: 0.000004  L1: 77.52  Loss: 0.000082\n",
      "PARAM=60  Epoch [33000/50000]  MSE: 0.000015  L1: 77.18  Loss: 0.000092\n",
      "PARAM=60  Epoch [33500/50000]  MSE: 0.000009  L1: 76.88  Loss: 0.000085\n",
      "PARAM=60  Epoch [34000/50000]  MSE: 0.000001  L1: 76.33  Loss: 0.000078\n",
      "PARAM=60  Epoch [34500/50000]  MSE: 0.000036  L1: 80.61  Loss: 0.000117\n",
      "PARAM=60  Epoch [35000/50000]  MSE: 0.000005  L1: 76.04  Loss: 0.000081\n",
      "PARAM=60  Epoch [35500/50000]  MSE: 0.000002  L1: 76.06  Loss: 0.000078\n",
      "PARAM=60  Epoch [36000/50000]  MSE: 0.000017  L1: 76.05  Loss: 0.000093\n",
      "PARAM=60  Epoch [36500/50000]  MSE: 0.000007  L1: 75.82  Loss: 0.000082\n",
      "PARAM=60  Epoch [37000/50000]  MSE: 0.000002  L1: 76.49  Loss: 0.000078\n",
      "PARAM=60  Epoch [37500/50000]  MSE: 0.000016  L1: 75.94  Loss: 0.000091\n",
      "PARAM=60  Epoch [38000/50000]  MSE: 0.000005  L1: 75.05  Loss: 0.000081\n",
      "PARAM=60  Epoch [38500/50000]  MSE: 0.000004  L1: 74.71  Loss: 0.000079\n",
      "PARAM=60  Epoch [39000/50000]  MSE: 0.000005  L1: 74.92  Loss: 0.000080\n",
      "PARAM=60  Epoch [39500/50000]  MSE: 0.000006  L1: 77.67  Loss: 0.000083\n",
      "PARAM=60  Epoch [40000/50000]  MSE: 0.000003  L1: 74.49  Loss: 0.000077\n",
      "PARAM=60  Epoch [40500/50000]  MSE: 0.000013  L1: 75.82  Loss: 0.000089\n",
      "PARAM=60  Epoch [41000/50000]  MSE: 0.000002  L1: 74.62  Loss: 0.000076\n",
      "PARAM=60  Epoch [41500/50000]  MSE: 0.000001  L1: 74.90  Loss: 0.000076\n",
      "PARAM=60  Epoch [42000/50000]  MSE: 0.000001  L1: 75.02  Loss: 0.000076\n",
      "PARAM=60  Epoch [42500/50000]  MSE: 0.000001  L1: 73.96  Loss: 0.000075\n",
      "PARAM=60  Epoch [43000/50000]  MSE: 0.000004  L1: 73.72  Loss: 0.000078\n",
      "PARAM=60  Epoch [43500/50000]  MSE: 0.000140  L1: 75.31  Loss: 0.000215\n",
      "PARAM=60  Epoch [44000/50000]  MSE: 0.000018  L1: 73.76  Loss: 0.000092\n",
      "PARAM=60  Epoch [44500/50000]  MSE: 0.000001  L1: 74.15  Loss: 0.000075\n",
      "PARAM=60  Epoch [45000/50000]  MSE: 0.000002  L1: 73.66  Loss: 0.000075\n",
      "PARAM=60  Epoch [45500/50000]  MSE: 0.000002  L1: 73.11  Loss: 0.000075\n",
      "PARAM=60  Epoch [46000/50000]  MSE: 0.000002  L1: 73.06  Loss: 0.000075\n",
      "PARAM=60  Epoch [46500/50000]  MSE: 0.000002  L1: 73.37  Loss: 0.000075\n",
      "PARAM=60  Epoch [47000/50000]  MSE: 0.000001  L1: 72.56  Loss: 0.000073\n",
      "PARAM=60  Epoch [47500/50000]  MSE: 0.000001  L1: 72.31  Loss: 0.000073\n",
      "PARAM=60  Epoch [48000/50000]  MSE: 0.000003  L1: 72.38  Loss: 0.000075\n",
      "PARAM=60  Epoch [48500/50000]  MSE: 0.000001  L1: 71.63  Loss: 0.000073\n",
      "PARAM=60  Epoch [49000/50000]  MSE: 0.000003  L1: 71.32  Loss: 0.000074\n",
      "PARAM=60  Epoch [49500/50000]  MSE: 0.000001  L1: 71.25  Loss: 0.000073\n",
      "\n",
      "PARAM=60 予測結果：\n",
      "[[ 0.92029816  0.11735106  0.14515066]\n",
      " [ 0.40526706  0.7343401   0.26960087]\n",
      " [ 0.21139029  0.32415408  0.640719  ]\n",
      " [ 0.96146715  0.9182159   0.07726675]\n",
      " [ 0.42993242  0.7953789   0.86420673]\n",
      " [ 0.71805125  0.3148492   0.62084806]\n",
      " [-0.00346117  0.00191578  0.00401339]\n",
      " [ 0.49897027  0.49714592  0.4978796 ]\n",
      " [ 0.99130845  0.9948949   0.9940301 ]\n",
      " [ 0.4515074   0.30759567  0.2597482 ]\n",
      " [ 0.7608625   0.5580828   0.49433827]\n",
      " [ 0.3493636   0.46851492  0.60934466]\n",
      " [ 0.35277307  0.42014545  0.2506109 ]\n",
      " [ 0.51293254  0.49453643  0.6839041 ]\n",
      " [ 0.37054223  0.7362081   0.6746887 ]\n",
      " [ 0.8650769   0.47592294  0.18930125]\n",
      " [ 0.26362896  0.3535274   0.65324116]\n",
      " [ 0.768549    0.31272948  0.3788096 ]\n",
      " [ 0.36069494  0.22766516  0.41253582]\n",
      " [ 0.61252177  0.73436916  0.23077597]\n",
      " [ 0.8899771   0.6289363   0.1578722 ]\n",
      " [ 0.1527656   0.24745451  0.5635137 ]\n",
      " [ 0.22822478  0.57431126  0.27282572]\n",
      " [ 0.696355    0.21293554  0.22890879]\n",
      " [ 0.91436344  0.77535444  0.11179173]\n",
      " [ 0.74713606  0.30777588  0.574353  ]\n",
      " [-0.0026892   0.51545787  0.64449966]\n",
      " [ 0.94443446  0.9407824   0.9184537 ]\n",
      " [ 0.7836418   0.7864975   0.7853001 ]\n",
      " [ 0.62595356  0.6301858   0.6327864 ]\n",
      " [ 0.47441322  0.47609782  0.47806704]\n",
      " [ 0.3241409   0.33254087  0.33667374]\n",
      " [ 0.19407696  0.19544072  0.19620728]]\n",
      "C++ 用のパラメータファイル (nomal_model_parameters_60.h) を作成しました。\n",
      "\n",
      "PARAM=70  Epoch [0/50000]  MSE: 0.378442  L1: 370.78  Loss: 0.378813\n",
      "PARAM=70  Epoch [500/50000]  MSE: 0.001180  L1: 239.12  Loss: 0.001420\n",
      "PARAM=70  Epoch [1000/50000]  MSE: 0.000315  L1: 246.14  Loss: 0.000561\n",
      "PARAM=70  Epoch [1500/50000]  MSE: 0.000047  L1: 236.83  Loss: 0.000283\n",
      "PARAM=70  Epoch [2000/50000]  MSE: 0.000103  L1: 218.97  Loss: 0.000322\n",
      "PARAM=70  Epoch [2500/50000]  MSE: 0.000017  L1: 201.38  Loss: 0.000218\n",
      "PARAM=70  Epoch [3000/50000]  MSE: 0.000015  L1: 185.04  Loss: 0.000200\n",
      "PARAM=70  Epoch [3500/50000]  MSE: 0.000008  L1: 171.26  Loss: 0.000179\n",
      "PARAM=70  Epoch [4000/50000]  MSE: 0.000019  L1: 160.10  Loss: 0.000179\n",
      "PARAM=70  Epoch [4500/50000]  MSE: 0.000028  L1: 151.11  Loss: 0.000179\n",
      "PARAM=70  Epoch [5000/50000]  MSE: 0.000059  L1: 143.12  Loss: 0.000202\n",
      "PARAM=70  Epoch [5500/50000]  MSE: 0.000015  L1: 136.49  Loss: 0.000151\n",
      "PARAM=70  Epoch [6000/50000]  MSE: 0.000027  L1: 130.33  Loss: 0.000157\n",
      "PARAM=70  Epoch [6500/50000]  MSE: 0.000018  L1: 124.67  Loss: 0.000143\n",
      "PARAM=70  Epoch [7000/50000]  MSE: 0.000018  L1: 120.67  Loss: 0.000138\n",
      "PARAM=70  Epoch [7500/50000]  MSE: 0.000015  L1: 116.08  Loss: 0.000131\n",
      "PARAM=70  Epoch [8000/50000]  MSE: 0.000010  L1: 112.34  Loss: 0.000122\n",
      "PARAM=70  Epoch [8500/50000]  MSE: 0.000006  L1: 109.69  Loss: 0.000115\n",
      "PARAM=70  Epoch [9000/50000]  MSE: 0.000005  L1: 107.26  Loss: 0.000112\n",
      "PARAM=70  Epoch [9500/50000]  MSE: 0.000010  L1: 104.77  Loss: 0.000115\n",
      "PARAM=70  Epoch [10000/50000]  MSE: 0.000022  L1: 102.08  Loss: 0.000124\n",
      "PARAM=70  Epoch [10500/50000]  MSE: 0.000003  L1: 100.75  Loss: 0.000104\n",
      "PARAM=70  Epoch [11000/50000]  MSE: 0.000006  L1: 98.17  Loss: 0.000104\n",
      "PARAM=70  Epoch [11500/50000]  MSE: 0.000018  L1: 96.93  Loss: 0.000115\n",
      "PARAM=70  Epoch [12000/50000]  MSE: 0.000013  L1: 95.75  Loss: 0.000109\n",
      "PARAM=70  Epoch [12500/50000]  MSE: 0.000004  L1: 94.87  Loss: 0.000099\n",
      "PARAM=70  Epoch [13000/50000]  MSE: 0.000012  L1: 93.52  Loss: 0.000106\n",
      "PARAM=70  Epoch [13500/50000]  MSE: 0.000020  L1: 92.64  Loss: 0.000113\n",
      "PARAM=70  Epoch [14000/50000]  MSE: 0.000002  L1: 91.30  Loss: 0.000093\n",
      "PARAM=70  Epoch [14500/50000]  MSE: 0.000032  L1: 90.49  Loss: 0.000122\n",
      "PARAM=70  Epoch [15000/50000]  MSE: 0.000011  L1: 89.63  Loss: 0.000100\n",
      "PARAM=70  Epoch [15500/50000]  MSE: 0.000003  L1: 88.34  Loss: 0.000091\n",
      "PARAM=70  Epoch [16000/50000]  MSE: 0.000017  L1: 87.59  Loss: 0.000104\n",
      "PARAM=70  Epoch [16500/50000]  MSE: 0.000016  L1: 86.51  Loss: 0.000102\n",
      "PARAM=70  Epoch [17000/50000]  MSE: 0.000011  L1: 85.66  Loss: 0.000097\n",
      "PARAM=70  Epoch [17500/50000]  MSE: 0.000007  L1: 85.83  Loss: 0.000093\n",
      "PARAM=70  Epoch [18000/50000]  MSE: 0.000050  L1: 84.95  Loss: 0.000135\n",
      "PARAM=70  Epoch [18500/50000]  MSE: 0.000035  L1: 83.98  Loss: 0.000119\n",
      "PARAM=70  Epoch [19000/50000]  MSE: 0.000006  L1: 83.19  Loss: 0.000089\n",
      "PARAM=70  Epoch [19500/50000]  MSE: 0.000010  L1: 82.77  Loss: 0.000093\n",
      "PARAM=70  Epoch [20000/50000]  MSE: 0.000008  L1: 82.27  Loss: 0.000090\n",
      "PARAM=70  Epoch [20500/50000]  MSE: 0.000004  L1: 81.45  Loss: 0.000086\n",
      "PARAM=70  Epoch [21000/50000]  MSE: 0.000007  L1: 81.45  Loss: 0.000088\n",
      "PARAM=70  Epoch [21500/50000]  MSE: 0.000012  L1: 80.90  Loss: 0.000093\n",
      "PARAM=70  Epoch [22000/50000]  MSE: 0.000014  L1: 80.70  Loss: 0.000095\n",
      "PARAM=70  Epoch [22500/50000]  MSE: 0.000008  L1: 80.70  Loss: 0.000088\n",
      "PARAM=70  Epoch [23000/50000]  MSE: 0.000052  L1: 80.59  Loss: 0.000133\n",
      "PARAM=70  Epoch [23500/50000]  MSE: 0.000037  L1: 80.28  Loss: 0.000117\n",
      "PARAM=70  Epoch [24000/50000]  MSE: 0.000003  L1: 80.00  Loss: 0.000083\n",
      "PARAM=70  Epoch [24500/50000]  MSE: 0.000003  L1: 79.86  Loss: 0.000083\n",
      "PARAM=70  Epoch [25000/50000]  MSE: 0.000002  L1: 79.32  Loss: 0.000081\n",
      "PARAM=70  Epoch [25500/50000]  MSE: 0.000019  L1: 79.16  Loss: 0.000098\n",
      "PARAM=70  Epoch [26000/50000]  MSE: 0.000010  L1: 78.95  Loss: 0.000089\n",
      "PARAM=70  Epoch [26500/50000]  MSE: 0.000011  L1: 78.78  Loss: 0.000090\n",
      "PARAM=70  Epoch [27000/50000]  MSE: 0.000078  L1: 79.30  Loss: 0.000158\n",
      "PARAM=70  Epoch [27500/50000]  MSE: 0.000001  L1: 78.68  Loss: 0.000080\n",
      "PARAM=70  Epoch [28000/50000]  MSE: 0.000003  L1: 78.92  Loss: 0.000082\n",
      "PARAM=70  Epoch [28500/50000]  MSE: 0.000006  L1: 78.33  Loss: 0.000084\n",
      "PARAM=70  Epoch [29000/50000]  MSE: 0.000003  L1: 78.01  Loss: 0.000081\n",
      "PARAM=70  Epoch [29500/50000]  MSE: 0.000016  L1: 78.26  Loss: 0.000094\n",
      "PARAM=70  Epoch [30000/50000]  MSE: 0.000026  L1: 77.82  Loss: 0.000104\n",
      "PARAM=70  Epoch [30500/50000]  MSE: 0.000006  L1: 77.69  Loss: 0.000084\n",
      "PARAM=70  Epoch [31000/50000]  MSE: 0.000004  L1: 77.80  Loss: 0.000082\n",
      "PARAM=70  Epoch [31500/50000]  MSE: 0.000004  L1: 77.61  Loss: 0.000082\n",
      "PARAM=70  Epoch [32000/50000]  MSE: 0.000002  L1: 77.41  Loss: 0.000080\n",
      "PARAM=70  Epoch [32500/50000]  MSE: 0.000004  L1: 77.46  Loss: 0.000081\n",
      "PARAM=70  Epoch [33000/50000]  MSE: 0.000002  L1: 77.34  Loss: 0.000080\n",
      "PARAM=70  Epoch [33500/50000]  MSE: 0.000012  L1: 77.27  Loss: 0.000089\n",
      "PARAM=70  Epoch [34000/50000]  MSE: 0.000003  L1: 77.31  Loss: 0.000080\n",
      "PARAM=70  Epoch [34500/50000]  MSE: 0.000031  L1: 77.88  Loss: 0.000109\n",
      "PARAM=70  Epoch [35000/50000]  MSE: 0.000010  L1: 76.93  Loss: 0.000087\n",
      "PARAM=70  Epoch [35500/50000]  MSE: 0.000104  L1: 77.70  Loss: 0.000182\n",
      "PARAM=70  Epoch [36000/50000]  MSE: 0.000008  L1: 76.72  Loss: 0.000085\n",
      "PARAM=70  Epoch [36500/50000]  MSE: 0.000017  L1: 76.91  Loss: 0.000094\n",
      "PARAM=70  Epoch [37000/50000]  MSE: 0.000002  L1: 76.85  Loss: 0.000079\n",
      "PARAM=70  Epoch [37500/50000]  MSE: 0.000003  L1: 76.89  Loss: 0.000080\n",
      "PARAM=70  Epoch [38000/50000]  MSE: 0.000019  L1: 76.30  Loss: 0.000095\n",
      "PARAM=70  Epoch [38500/50000]  MSE: 0.000049  L1: 76.67  Loss: 0.000126\n",
      "PARAM=70  Epoch [39000/50000]  MSE: 0.000001  L1: 76.28  Loss: 0.000077\n",
      "PARAM=70  Epoch [39500/50000]  MSE: 0.000039  L1: 76.79  Loss: 0.000116\n",
      "PARAM=70  Epoch [40000/50000]  MSE: 0.000023  L1: 76.31  Loss: 0.000099\n",
      "PARAM=70  Epoch [40500/50000]  MSE: 0.000006  L1: 76.45  Loss: 0.000083\n",
      "PARAM=70  Epoch [41000/50000]  MSE: 0.000017  L1: 76.37  Loss: 0.000093\n",
      "PARAM=70  Epoch [41500/50000]  MSE: 0.000001  L1: 76.82  Loss: 0.000078\n",
      "PARAM=70  Epoch [42000/50000]  MSE: 0.000015  L1: 76.04  Loss: 0.000091\n",
      "PARAM=70  Epoch [42500/50000]  MSE: 0.000009  L1: 75.93  Loss: 0.000085\n",
      "PARAM=70  Epoch [43000/50000]  MSE: 0.000011  L1: 76.36  Loss: 0.000087\n",
      "PARAM=70  Epoch [43500/50000]  MSE: 0.000000  L1: 76.15  Loss: 0.000076\n",
      "PARAM=70  Epoch [44000/50000]  MSE: 0.000004  L1: 75.79  Loss: 0.000080\n",
      "PARAM=70  Epoch [44500/50000]  MSE: 0.000001  L1: 75.67  Loss: 0.000076\n",
      "PARAM=70  Epoch [45000/50000]  MSE: 0.000003  L1: 75.57  Loss: 0.000079\n",
      "PARAM=70  Epoch [45500/50000]  MSE: 0.000049  L1: 75.75  Loss: 0.000125\n",
      "PARAM=70  Epoch [46000/50000]  MSE: 0.000001  L1: 75.72  Loss: 0.000077\n",
      "PARAM=70  Epoch [46500/50000]  MSE: 0.000001  L1: 74.96  Loss: 0.000076\n",
      "PARAM=70  Epoch [47000/50000]  MSE: 0.000017  L1: 75.75  Loss: 0.000093\n",
      "PARAM=70  Epoch [47500/50000]  MSE: 0.000086  L1: 75.08  Loss: 0.000161\n",
      "PARAM=70  Epoch [48000/50000]  MSE: 0.000006  L1: 75.40  Loss: 0.000082\n",
      "PARAM=70  Epoch [48500/50000]  MSE: 0.000009  L1: 74.98  Loss: 0.000083\n",
      "PARAM=70  Epoch [49000/50000]  MSE: 0.000001  L1: 74.84  Loss: 0.000076\n",
      "PARAM=70  Epoch [49500/50000]  MSE: 0.000001  L1: 74.98  Loss: 0.000076\n",
      "\n",
      "PARAM=70 予測結果：\n",
      "[[0.9299249  0.11887665 0.14133728]\n",
      " [0.41305667 0.7424809  0.2699195 ]\n",
      " [0.22152025 0.3247407  0.6431936 ]\n",
      " [0.9676221  0.923375   0.08018765]\n",
      " [0.43684003 0.800859   0.86689675]\n",
      " [0.72445    0.31809148 0.61930245]\n",
      " [0.00490167 0.00437084 0.00461835]\n",
      " [0.50455093 0.4999329  0.49829215]\n",
      " [1.0024213  1.00254    1.0017182 ]\n",
      " [0.4565942  0.30887467 0.25834066]\n",
      " [0.76433647 0.5611365  0.49325705]\n",
      " [0.35843724 0.47500053 0.612169  ]\n",
      " [0.35895213 0.42410865 0.24925163]\n",
      " [0.5184654  0.49841434 0.6869665 ]\n",
      " [0.3776823  0.7421591  0.67923236]\n",
      " [0.8717446  0.47904283 0.18817282]\n",
      " [0.27239156 0.35650414 0.65525156]\n",
      " [0.77980417 0.31232142 0.37599045]\n",
      " [0.36728373 0.23147818 0.4103165 ]\n",
      " [0.6180932  0.73684645 0.23156603]\n",
      " [0.8963925  0.6318301  0.15884587]\n",
      " [0.15347233 0.24574536 0.5642712 ]\n",
      " [0.23601338 0.5789161  0.2745086 ]\n",
      " [0.7042729  0.21429488 0.22702333]\n",
      " [0.91872436 0.77720094 0.11062694]\n",
      " [0.7517556  0.31169805 0.5728452 ]\n",
      " [0.00471326 0.5229265  0.6475706 ]\n",
      " [0.94807625 0.9474683  0.9191513 ]\n",
      " [0.7901641  0.79348004 0.7880815 ]\n",
      " [0.63323987 0.63527954 0.63491476]\n",
      " [0.4800188  0.47754738 0.47868717]\n",
      " [0.3324714  0.3329163  0.33593735]\n",
      " [0.19576931 0.19510609 0.19501314]]\n",
      "C++ 用のパラメータファイル (nomal_model_parameters_70.h) を作成しました。\n",
      "\n",
      "PARAM=80  Epoch [0/50000]  MSE: 0.371571  L1: 443.85  Loss: 0.372014\n",
      "PARAM=80  Epoch [500/50000]  MSE: 0.000287  L1: 312.69  Loss: 0.000599\n",
      "PARAM=80  Epoch [1000/50000]  MSE: 0.000241  L1: 272.54  Loss: 0.000514\n",
      "PARAM=80  Epoch [1500/50000]  MSE: 0.000037  L1: 238.93  Loss: 0.000276\n",
      "PARAM=80  Epoch [2000/50000]  MSE: 0.000093  L1: 211.36  Loss: 0.000304\n",
      "PARAM=80  Epoch [2500/50000]  MSE: 0.000008  L1: 191.17  Loss: 0.000199\n",
      "PARAM=80  Epoch [3000/50000]  MSE: 0.000064  L1: 175.68  Loss: 0.000240\n",
      "PARAM=80  Epoch [3500/50000]  MSE: 0.000007  L1: 161.39  Loss: 0.000168\n",
      "PARAM=80  Epoch [4000/50000]  MSE: 0.000017  L1: 150.97  Loss: 0.000168\n",
      "PARAM=80  Epoch [4500/50000]  MSE: 0.000057  L1: 142.46  Loss: 0.000199\n",
      "PARAM=80  Epoch [5000/50000]  MSE: 0.000082  L1: 136.04  Loss: 0.000218\n",
      "PARAM=80  Epoch [5500/50000]  MSE: 0.000006  L1: 130.24  Loss: 0.000136\n",
      "PARAM=80  Epoch [6000/50000]  MSE: 0.000017  L1: 123.79  Loss: 0.000140\n",
      "PARAM=80  Epoch [6500/50000]  MSE: 0.000012  L1: 118.40  Loss: 0.000130\n",
      "PARAM=80  Epoch [7000/50000]  MSE: 0.000054  L1: 113.24  Loss: 0.000168\n",
      "PARAM=80  Epoch [7500/50000]  MSE: 0.000006  L1: 108.40  Loss: 0.000115\n",
      "PARAM=80  Epoch [8000/50000]  MSE: 0.000002  L1: 104.87  Loss: 0.000107\n",
      "PARAM=80  Epoch [8500/50000]  MSE: 0.000076  L1: 103.09  Loss: 0.000179\n",
      "PARAM=80  Epoch [9000/50000]  MSE: 0.000003  L1: 100.15  Loss: 0.000103\n",
      "PARAM=80  Epoch [9500/50000]  MSE: 0.000001  L1: 99.89  Loss: 0.000101\n",
      "PARAM=80  Epoch [10000/50000]  MSE: 0.000007  L1: 96.46  Loss: 0.000103\n",
      "PARAM=80  Epoch [10500/50000]  MSE: 0.000002  L1: 94.42  Loss: 0.000096\n",
      "PARAM=80  Epoch [11000/50000]  MSE: 0.000012  L1: 92.81  Loss: 0.000105\n",
      "PARAM=80  Epoch [11500/50000]  MSE: 0.000019  L1: 90.80  Loss: 0.000109\n",
      "PARAM=80  Epoch [12000/50000]  MSE: 0.000001  L1: 88.88  Loss: 0.000090\n",
      "PARAM=80  Epoch [12500/50000]  MSE: 0.000001  L1: 89.46  Loss: 0.000090\n",
      "PARAM=80  Epoch [13000/50000]  MSE: 0.000004  L1: 87.19  Loss: 0.000091\n",
      "PARAM=80  Epoch [13500/50000]  MSE: 0.000001  L1: 86.40  Loss: 0.000088\n",
      "PARAM=80  Epoch [14000/50000]  MSE: 0.000003  L1: 85.37  Loss: 0.000088\n",
      "PARAM=80  Epoch [14500/50000]  MSE: 0.000001  L1: 84.58  Loss: 0.000086\n",
      "PARAM=80  Epoch [15000/50000]  MSE: 0.000002  L1: 83.18  Loss: 0.000085\n",
      "PARAM=80  Epoch [15500/50000]  MSE: 0.000007  L1: 82.40  Loss: 0.000090\n",
      "PARAM=80  Epoch [16000/50000]  MSE: 0.000003  L1: 81.41  Loss: 0.000085\n",
      "PARAM=80  Epoch [16500/50000]  MSE: 0.000016  L1: 81.74  Loss: 0.000098\n",
      "PARAM=80  Epoch [17000/50000]  MSE: 0.000002  L1: 81.13  Loss: 0.000083\n",
      "PARAM=80  Epoch [17500/50000]  MSE: 0.000002  L1: 80.86  Loss: 0.000083\n",
      "PARAM=80  Epoch [18000/50000]  MSE: 0.000003  L1: 80.08  Loss: 0.000083\n",
      "PARAM=80  Epoch [18500/50000]  MSE: 0.000001  L1: 79.21  Loss: 0.000080\n",
      "PARAM=80  Epoch [19000/50000]  MSE: 0.000002  L1: 79.09  Loss: 0.000081\n",
      "PARAM=80  Epoch [19500/50000]  MSE: 0.000002  L1: 78.61  Loss: 0.000080\n",
      "PARAM=80  Epoch [20000/50000]  MSE: 0.000003  L1: 79.84  Loss: 0.000083\n",
      "PARAM=80  Epoch [20500/50000]  MSE: 0.000001  L1: 78.73  Loss: 0.000079\n",
      "PARAM=80  Epoch [21000/50000]  MSE: 0.000003  L1: 77.24  Loss: 0.000080\n",
      "PARAM=80  Epoch [21500/50000]  MSE: 0.000001  L1: 76.55  Loss: 0.000077\n",
      "PARAM=80  Epoch [22000/50000]  MSE: 0.000007  L1: 76.69  Loss: 0.000084\n",
      "PARAM=80  Epoch [22500/50000]  MSE: 0.000007  L1: 76.81  Loss: 0.000083\n",
      "PARAM=80  Epoch [23000/50000]  MSE: 0.000005  L1: 76.18  Loss: 0.000081\n",
      "PARAM=80  Epoch [23500/50000]  MSE: 0.000007  L1: 76.20  Loss: 0.000083\n",
      "PARAM=80  Epoch [24000/50000]  MSE: 0.000006  L1: 76.12  Loss: 0.000082\n",
      "PARAM=80  Epoch [24500/50000]  MSE: 0.000002  L1: 75.58  Loss: 0.000078\n",
      "PARAM=80  Epoch [25000/50000]  MSE: 0.000003  L1: 76.16  Loss: 0.000079\n",
      "PARAM=80  Epoch [25500/50000]  MSE: 0.000005  L1: 75.57  Loss: 0.000081\n",
      "PARAM=80  Epoch [26000/50000]  MSE: 0.000003  L1: 75.34  Loss: 0.000078\n",
      "PARAM=80  Epoch [26500/50000]  MSE: 0.000001  L1: 76.25  Loss: 0.000077\n",
      "PARAM=80  Epoch [27000/50000]  MSE: 0.000002  L1: 75.77  Loss: 0.000077\n",
      "PARAM=80  Epoch [27500/50000]  MSE: 0.000001  L1: 74.93  Loss: 0.000076\n",
      "PARAM=80  Epoch [28000/50000]  MSE: 0.000013  L1: 75.26  Loss: 0.000088\n",
      "PARAM=80  Epoch [28500/50000]  MSE: 0.000008  L1: 74.73  Loss: 0.000083\n",
      "PARAM=80  Epoch [29000/50000]  MSE: 0.000011  L1: 74.15  Loss: 0.000085\n",
      "PARAM=80  Epoch [29500/50000]  MSE: 0.000003  L1: 75.38  Loss: 0.000078\n",
      "PARAM=80  Epoch [30000/50000]  MSE: 0.000024  L1: 74.29  Loss: 0.000098\n",
      "PARAM=80  Epoch [30500/50000]  MSE: 0.000035  L1: 73.96  Loss: 0.000109\n",
      "PARAM=80  Epoch [31000/50000]  MSE: 0.000026  L1: 74.78  Loss: 0.000101\n",
      "PARAM=80  Epoch [31500/50000]  MSE: 0.000120  L1: 74.78  Loss: 0.000195\n",
      "PARAM=80  Epoch [32000/50000]  MSE: 0.000009  L1: 75.64  Loss: 0.000084\n",
      "PARAM=80  Epoch [32500/50000]  MSE: 0.000007  L1: 73.67  Loss: 0.000081\n",
      "PARAM=80  Epoch [33000/50000]  MSE: 0.000013  L1: 73.65  Loss: 0.000087\n",
      "PARAM=80  Epoch [33500/50000]  MSE: 0.000002  L1: 73.45  Loss: 0.000075\n",
      "PARAM=80  Epoch [34000/50000]  MSE: 0.000002  L1: 73.61  Loss: 0.000076\n",
      "PARAM=80  Epoch [34500/50000]  MSE: 0.000001  L1: 73.42  Loss: 0.000074\n",
      "PARAM=80  Epoch [35000/50000]  MSE: 0.000004  L1: 73.65  Loss: 0.000077\n",
      "PARAM=80  Epoch [35500/50000]  MSE: 0.000004  L1: 73.98  Loss: 0.000078\n",
      "PARAM=80  Epoch [36000/50000]  MSE: 0.000001  L1: 73.86  Loss: 0.000074\n",
      "PARAM=80  Epoch [36500/50000]  MSE: 0.000002  L1: 73.11  Loss: 0.000076\n",
      "PARAM=80  Epoch [37000/50000]  MSE: 0.000086  L1: 73.60  Loss: 0.000160\n",
      "PARAM=80  Epoch [37500/50000]  MSE: 0.000001  L1: 72.83  Loss: 0.000074\n",
      "PARAM=80  Epoch [38000/50000]  MSE: 0.000001  L1: 72.76  Loss: 0.000073\n",
      "PARAM=80  Epoch [38500/50000]  MSE: 0.000002  L1: 72.39  Loss: 0.000075\n",
      "PARAM=80  Epoch [39000/50000]  MSE: 0.000011  L1: 73.65  Loss: 0.000085\n",
      "PARAM=80  Epoch [39500/50000]  MSE: 0.000008  L1: 71.86  Loss: 0.000080\n",
      "PARAM=80  Epoch [40000/50000]  MSE: 0.000026  L1: 72.12  Loss: 0.000099\n",
      "PARAM=80  Epoch [40500/50000]  MSE: 0.000001  L1: 72.01  Loss: 0.000073\n",
      "PARAM=80  Epoch [41000/50000]  MSE: 0.000011  L1: 72.08  Loss: 0.000083\n",
      "PARAM=80  Epoch [41500/50000]  MSE: 0.000002  L1: 72.72  Loss: 0.000075\n",
      "PARAM=80  Epoch [42000/50000]  MSE: 0.000002  L1: 71.74  Loss: 0.000074\n",
      "PARAM=80  Epoch [42500/50000]  MSE: 0.000003  L1: 71.48  Loss: 0.000074\n",
      "PARAM=80  Epoch [43000/50000]  MSE: 0.000001  L1: 71.54  Loss: 0.000073\n",
      "PARAM=80  Epoch [43500/50000]  MSE: 0.000001  L1: 72.10  Loss: 0.000073\n",
      "PARAM=80  Epoch [44000/50000]  MSE: 0.000002  L1: 71.34  Loss: 0.000074\n",
      "PARAM=80  Epoch [44500/50000]  MSE: 0.000004  L1: 71.67  Loss: 0.000075\n",
      "PARAM=80  Epoch [45000/50000]  MSE: 0.000005  L1: 71.48  Loss: 0.000076\n",
      "PARAM=80  Epoch [45500/50000]  MSE: 0.000002  L1: 71.36  Loss: 0.000073\n",
      "PARAM=80  Epoch [46000/50000]  MSE: 0.000001  L1: 71.32  Loss: 0.000073\n",
      "PARAM=80  Epoch [46500/50000]  MSE: 0.000003  L1: 71.57  Loss: 0.000074\n",
      "PARAM=80  Epoch [47000/50000]  MSE: 0.000001  L1: 71.12  Loss: 0.000072\n",
      "PARAM=80  Epoch [47500/50000]  MSE: 0.000001  L1: 71.42  Loss: 0.000072\n",
      "PARAM=80  Epoch [48000/50000]  MSE: 0.000010  L1: 73.49  Loss: 0.000084\n",
      "PARAM=80  Epoch [48500/50000]  MSE: 0.000003  L1: 71.19  Loss: 0.000074\n",
      "PARAM=80  Epoch [49000/50000]  MSE: 0.000017  L1: 71.32  Loss: 0.000088\n",
      "PARAM=80  Epoch [49500/50000]  MSE: 0.000004  L1: 71.19  Loss: 0.000075\n",
      "\n",
      "PARAM=80 予測結果：\n",
      "[[0.9291535  0.11490819 0.14040983]\n",
      " [0.4104259  0.7380566  0.26891184]\n",
      " [0.22003251 0.32383412 0.642633  ]\n",
      " [0.96419626 0.91857505 0.07605709]\n",
      " [0.43409634 0.7975552  0.8650913 ]\n",
      " [0.7221056  0.31405795 0.6193311 ]\n",
      " [0.0043377  0.00303448 0.00413328]\n",
      " [0.50181544 0.49725583 0.4970592 ]\n",
      " [0.9979393  0.9957885  0.99663985]\n",
      " [0.45334816 0.30802548 0.25662866]\n",
      " [0.7633792  0.55974776 0.49133778]\n",
      " [0.3557535  0.47234875 0.60988826]\n",
      " [0.35675472 0.4219575  0.24819571]\n",
      " [0.5170404  0.4962913  0.6858369 ]\n",
      " [0.37544534 0.7397625  0.6770444 ]\n",
      " [0.8699519  0.47673118 0.1866664 ]\n",
      " [0.26973763 0.3554347  0.6542051 ]\n",
      " [0.7784199  0.3093685  0.37699068]\n",
      " [0.36334884 0.22972476 0.41022536]\n",
      " [0.6150085  0.7353598  0.22901314]\n",
      " [0.8935264  0.6277645  0.15548289]\n",
      " [0.15225786 0.24504238 0.5626547 ]\n",
      " [0.2336504  0.5777525  0.27292192]\n",
      " [0.7027159  0.21246901 0.22621751]\n",
      " [0.917166   0.77326727 0.10737514]\n",
      " [0.7506031  0.3098691  0.5695813 ]\n",
      " [0.00416449 0.520349   0.64611244]\n",
      " [0.94365144 0.9427378  0.9150728 ]\n",
      " [0.7880477  0.78987867 0.78646386]\n",
      " [0.6304052  0.63124174 0.63362813]\n",
      " [0.47840726 0.4757745  0.4775056 ]\n",
      " [0.32838228 0.32954615 0.33775944]\n",
      " [0.19492206 0.19361222 0.19384806]]\n",
      "C++ 用のパラメータファイル (nomal_model_parameters_80.h) を作成しました。\n",
      "\n",
      "PARAM=90  Epoch [0/50000]  MSE: 0.340916  L1: 519.66  Loss: 0.341435\n",
      "PARAM=90  Epoch [500/50000]  MSE: 0.000159  L1: 344.59  Loss: 0.000503\n",
      "PARAM=90  Epoch [1000/50000]  MSE: 0.000094  L1: 281.01  Loss: 0.000375\n",
      "PARAM=90  Epoch [1500/50000]  MSE: 0.000314  L1: 235.75  Loss: 0.000550\n",
      "PARAM=90  Epoch [2000/50000]  MSE: 0.000143  L1: 203.92  Loss: 0.000347\n",
      "PARAM=90  Epoch [2500/50000]  MSE: 0.000050  L1: 184.21  Loss: 0.000234\n",
      "PARAM=90  Epoch [3000/50000]  MSE: 0.000033  L1: 167.00  Loss: 0.000200\n",
      "PARAM=90  Epoch [3500/50000]  MSE: 0.000065  L1: 154.12  Loss: 0.000219\n",
      "PARAM=90  Epoch [4000/50000]  MSE: 0.000032  L1: 143.19  Loss: 0.000175\n",
      "PARAM=90  Epoch [4500/50000]  MSE: 0.000036  L1: 136.20  Loss: 0.000172\n",
      "PARAM=90  Epoch [5000/50000]  MSE: 0.000035  L1: 128.55  Loss: 0.000164\n",
      "PARAM=90  Epoch [5500/50000]  MSE: 0.000027  L1: 123.53  Loss: 0.000151\n",
      "PARAM=90  Epoch [6000/50000]  MSE: 0.000074  L1: 121.53  Loss: 0.000195\n",
      "PARAM=90  Epoch [6500/50000]  MSE: 0.000142  L1: 114.26  Loss: 0.000256\n",
      "PARAM=90  Epoch [7000/50000]  MSE: 0.000036  L1: 110.29  Loss: 0.000147\n",
      "PARAM=90  Epoch [7500/50000]  MSE: 0.000020  L1: 108.03  Loss: 0.000128\n",
      "PARAM=90  Epoch [8000/50000]  MSE: 0.000024  L1: 106.00  Loss: 0.000130\n",
      "PARAM=90  Epoch [8500/50000]  MSE: 0.000026  L1: 105.22  Loss: 0.000132\n",
      "PARAM=90  Epoch [9000/50000]  MSE: 0.000018  L1: 104.68  Loss: 0.000123\n",
      "PARAM=90  Epoch [9500/50000]  MSE: 0.000031  L1: 103.86  Loss: 0.000135\n",
      "PARAM=90  Epoch [10000/50000]  MSE: 0.000008  L1: 102.57  Loss: 0.000110\n",
      "PARAM=90  Epoch [10500/50000]  MSE: 0.000010  L1: 101.27  Loss: 0.000111\n",
      "PARAM=90  Epoch [11000/50000]  MSE: 0.000004  L1: 100.84  Loss: 0.000105\n",
      "PARAM=90  Epoch [11500/50000]  MSE: 0.000064  L1: 98.48  Loss: 0.000163\n",
      "PARAM=90  Epoch [12000/50000]  MSE: 0.000039  L1: 97.09  Loss: 0.000136\n",
      "PARAM=90  Epoch [12500/50000]  MSE: 0.000002  L1: 96.36  Loss: 0.000099\n",
      "PARAM=90  Epoch [13000/50000]  MSE: 0.000183  L1: 94.72  Loss: 0.000278\n",
      "PARAM=90  Epoch [13500/50000]  MSE: 0.000002  L1: 93.58  Loss: 0.000096\n",
      "PARAM=90  Epoch [14000/50000]  MSE: 0.000002  L1: 93.17  Loss: 0.000095\n",
      "PARAM=90  Epoch [14500/50000]  MSE: 0.000005  L1: 92.63  Loss: 0.000098\n",
      "PARAM=90  Epoch [15000/50000]  MSE: 0.000009  L1: 91.40  Loss: 0.000100\n",
      "PARAM=90  Epoch [15500/50000]  MSE: 0.000012  L1: 91.17  Loss: 0.000103\n",
      "PARAM=90  Epoch [16000/50000]  MSE: 0.000005  L1: 90.44  Loss: 0.000096\n",
      "PARAM=90  Epoch [16500/50000]  MSE: 0.000014  L1: 90.40  Loss: 0.000105\n",
      "PARAM=90  Epoch [17000/50000]  MSE: 0.000005  L1: 89.34  Loss: 0.000095\n",
      "PARAM=90  Epoch [17500/50000]  MSE: 0.000002  L1: 89.17  Loss: 0.000091\n",
      "PARAM=90  Epoch [18000/50000]  MSE: 0.000002  L1: 88.69  Loss: 0.000090\n",
      "PARAM=90  Epoch [18500/50000]  MSE: 0.000001  L1: 88.35  Loss: 0.000090\n",
      "PARAM=90  Epoch [19000/50000]  MSE: 0.000035  L1: 88.58  Loss: 0.000124\n",
      "PARAM=90  Epoch [19500/50000]  MSE: 0.000001  L1: 87.25  Loss: 0.000088\n",
      "PARAM=90  Epoch [20000/50000]  MSE: 0.000006  L1: 86.75  Loss: 0.000093\n",
      "PARAM=90  Epoch [20500/50000]  MSE: 0.000002  L1: 86.23  Loss: 0.000088\n",
      "PARAM=90  Epoch [21000/50000]  MSE: 0.000041  L1: 86.54  Loss: 0.000127\n",
      "PARAM=90  Epoch [21500/50000]  MSE: 0.000002  L1: 85.31  Loss: 0.000088\n",
      "PARAM=90  Epoch [22000/50000]  MSE: 0.000001  L1: 85.40  Loss: 0.000086\n",
      "PARAM=90  Epoch [22500/50000]  MSE: 0.000006  L1: 85.85  Loss: 0.000092\n",
      "PARAM=90  Epoch [23000/50000]  MSE: 0.000019  L1: 86.66  Loss: 0.000105\n",
      "PARAM=90  Epoch [23500/50000]  MSE: 0.000001  L1: 85.54  Loss: 0.000086\n",
      "PARAM=90  Epoch [24000/50000]  MSE: 0.000003  L1: 85.09  Loss: 0.000088\n",
      "PARAM=90  Epoch [24500/50000]  MSE: 0.000001  L1: 84.06  Loss: 0.000085\n",
      "PARAM=90  Epoch [25000/50000]  MSE: 0.000002  L1: 83.74  Loss: 0.000086\n",
      "PARAM=90  Epoch [25500/50000]  MSE: 0.000001  L1: 84.40  Loss: 0.000085\n",
      "PARAM=90  Epoch [26000/50000]  MSE: 0.000002  L1: 83.03  Loss: 0.000085\n",
      "PARAM=90  Epoch [26500/50000]  MSE: 0.000001  L1: 83.71  Loss: 0.000085\n",
      "PARAM=90  Epoch [27000/50000]  MSE: 0.000006  L1: 83.66  Loss: 0.000090\n",
      "PARAM=90  Epoch [27500/50000]  MSE: 0.000006  L1: 83.01  Loss: 0.000089\n",
      "PARAM=90  Epoch [28000/50000]  MSE: 0.000002  L1: 82.72  Loss: 0.000085\n",
      "PARAM=90  Epoch [28500/50000]  MSE: 0.000002  L1: 82.74  Loss: 0.000085\n",
      "PARAM=90  Epoch [29000/50000]  MSE: 0.000158  L1: 82.49  Loss: 0.000241\n",
      "PARAM=90  Epoch [29500/50000]  MSE: 0.000014  L1: 82.32  Loss: 0.000097\n",
      "PARAM=90  Epoch [30000/50000]  MSE: 0.000003  L1: 82.59  Loss: 0.000086\n",
      "PARAM=90  Epoch [30500/50000]  MSE: 0.000003  L1: 81.71  Loss: 0.000084\n",
      "PARAM=90  Epoch [31000/50000]  MSE: 0.000000  L1: 82.29  Loss: 0.000083\n",
      "PARAM=90  Epoch [31500/50000]  MSE: 0.000000  L1: 81.45  Loss: 0.000082\n",
      "PARAM=90  Epoch [32000/50000]  MSE: 0.000001  L1: 81.25  Loss: 0.000082\n",
      "PARAM=90  Epoch [32500/50000]  MSE: 0.000001  L1: 81.07  Loss: 0.000082\n",
      "PARAM=90  Epoch [33000/50000]  MSE: 0.000038  L1: 80.60  Loss: 0.000118\n",
      "PARAM=90  Epoch [33500/50000]  MSE: 0.000007  L1: 80.64  Loss: 0.000088\n",
      "PARAM=90  Epoch [34000/50000]  MSE: 0.000012  L1: 80.59  Loss: 0.000092\n",
      "PARAM=90  Epoch [34500/50000]  MSE: 0.000007  L1: 82.47  Loss: 0.000090\n",
      "PARAM=90  Epoch [35000/50000]  MSE: 0.000001  L1: 79.78  Loss: 0.000081\n",
      "PARAM=90  Epoch [35500/50000]  MSE: 0.000001  L1: 79.84  Loss: 0.000081\n",
      "PARAM=90  Epoch [36000/50000]  MSE: 0.000001  L1: 79.68  Loss: 0.000080\n",
      "PARAM=90  Epoch [36500/50000]  MSE: 0.000001  L1: 79.11  Loss: 0.000080\n",
      "PARAM=90  Epoch [37000/50000]  MSE: 0.000007  L1: 79.49  Loss: 0.000086\n",
      "PARAM=90  Epoch [37500/50000]  MSE: 0.000001  L1: 79.41  Loss: 0.000081\n",
      "PARAM=90  Epoch [38000/50000]  MSE: 0.000009  L1: 78.87  Loss: 0.000088\n",
      "PARAM=90  Epoch [38500/50000]  MSE: 0.000004  L1: 79.13  Loss: 0.000083\n",
      "PARAM=90  Epoch [39000/50000]  MSE: 0.000030  L1: 79.58  Loss: 0.000109\n",
      "PARAM=90  Epoch [39500/50000]  MSE: 0.000001  L1: 78.52  Loss: 0.000080\n",
      "PARAM=90  Epoch [40000/50000]  MSE: 0.000019  L1: 78.68  Loss: 0.000098\n",
      "PARAM=90  Epoch [40500/50000]  MSE: 0.000007  L1: 81.60  Loss: 0.000088\n",
      "PARAM=90  Epoch [41000/50000]  MSE: 0.000004  L1: 78.19  Loss: 0.000082\n",
      "PARAM=90  Epoch [41500/50000]  MSE: 0.000005  L1: 78.39  Loss: 0.000083\n",
      "PARAM=90  Epoch [42000/50000]  MSE: 0.000004  L1: 78.15  Loss: 0.000083\n",
      "PARAM=90  Epoch [42500/50000]  MSE: 0.000005  L1: 77.84  Loss: 0.000083\n",
      "PARAM=90  Epoch [43000/50000]  MSE: 0.000001  L1: 77.84  Loss: 0.000079\n",
      "PARAM=90  Epoch [43500/50000]  MSE: 0.000011  L1: 78.00  Loss: 0.000089\n",
      "PARAM=90  Epoch [44000/50000]  MSE: 0.000010  L1: 77.28  Loss: 0.000087\n",
      "PARAM=90  Epoch [44500/50000]  MSE: 0.000042  L1: 79.10  Loss: 0.000121\n",
      "PARAM=90  Epoch [45000/50000]  MSE: 0.000001  L1: 77.10  Loss: 0.000079\n",
      "PARAM=90  Epoch [45500/50000]  MSE: 0.000007  L1: 76.97  Loss: 0.000084\n",
      "PARAM=90  Epoch [46000/50000]  MSE: 0.000003  L1: 77.04  Loss: 0.000080\n",
      "PARAM=90  Epoch [46500/50000]  MSE: 0.000002  L1: 77.08  Loss: 0.000079\n",
      "PARAM=90  Epoch [47000/50000]  MSE: 0.000012  L1: 76.84  Loss: 0.000089\n",
      "PARAM=90  Epoch [47500/50000]  MSE: 0.000001  L1: 77.22  Loss: 0.000078\n",
      "PARAM=90  Epoch [48000/50000]  MSE: 0.000001  L1: 77.23  Loss: 0.000079\n",
      "PARAM=90  Epoch [48500/50000]  MSE: 0.000002  L1: 76.70  Loss: 0.000078\n",
      "PARAM=90  Epoch [49000/50000]  MSE: 0.000042  L1: 77.52  Loss: 0.000120\n",
      "PARAM=90  Epoch [49500/50000]  MSE: 0.000003  L1: 77.07  Loss: 0.000080\n",
      "\n",
      "PARAM=90 予測結果：\n",
      "[[0.932087   0.11466596 0.13753092]\n",
      " [0.41426173 0.7389604  0.2702887 ]\n",
      " [0.22294156 0.32560146 0.6417786 ]\n",
      " [0.96705914 0.92106026 0.07682194]\n",
      " [0.43896744 0.7995047  0.8645257 ]\n",
      " [0.7237028  0.31675947 0.6167492 ]\n",
      " [0.00571346 0.00322521 0.00248224]\n",
      " [0.50442576 0.49825537 0.4951893 ]\n",
      " [0.9999838  0.9974903  0.99486315]\n",
      " [0.45598352 0.30876517 0.25775704]\n",
      " [0.7664355  0.5585939  0.49226183]\n",
      " [0.35896084 0.47198907 0.6101361 ]\n",
      " [0.35711554 0.42232063 0.24866204]\n",
      " [0.5196143  0.49741447 0.6843649 ]\n",
      " [0.3792172  0.73958457 0.6762997 ]\n",
      " [0.87314427 0.4764589  0.1865009 ]\n",
      " [0.2704131  0.3548465  0.6535624 ]\n",
      " [0.7786838  0.31115228 0.37483054]\n",
      " [0.36722147 0.23043731 0.4103642 ]\n",
      " [0.6178794  0.7362554  0.23008308]\n",
      " [0.89521575 0.62974596 0.15658003]\n",
      " [0.1535562  0.24599111 0.5622121 ]\n",
      " [0.23635857 0.5783136  0.2722296 ]\n",
      " [0.70376533 0.21482038 0.22582355]\n",
      " [0.9204391  0.77490723 0.10913609]\n",
      " [0.7545478  0.30927113 0.570858  ]\n",
      " [0.00522894 0.52008784 0.645802  ]\n",
      " [0.9479753  0.9464506  0.9159212 ]\n",
      " [0.789242   0.79123926 0.78694725]\n",
      " [0.6330958  0.63310724 0.63512754]\n",
      " [0.4804767  0.47545213 0.47749382]\n",
      " [0.3317136  0.3317508  0.3357347 ]\n",
      " [0.19554463 0.19456089 0.19384895]]\n",
      "C++ 用のパラメータファイル (nomal_model_parameters_90.h) を作成しました。\n",
      "\n",
      "PARAM=100  Epoch [0/50000]  MSE: 0.274534  L1: 597.24  Loss: 0.275131\n",
      "PARAM=100  Epoch [500/50000]  MSE: 0.000319  L1: 407.08  Loss: 0.000726\n",
      "PARAM=100  Epoch [1000/50000]  MSE: 0.000073  L1: 341.07  Loss: 0.000414\n",
      "PARAM=100  Epoch [1500/50000]  MSE: 0.000045  L1: 285.64  Loss: 0.000331\n",
      "PARAM=100  Epoch [2000/50000]  MSE: 0.000100  L1: 245.69  Loss: 0.000346\n",
      "PARAM=100  Epoch [2500/50000]  MSE: 0.000361  L1: 217.70  Loss: 0.000578\n",
      "PARAM=100  Epoch [3000/50000]  MSE: 0.000031  L1: 195.59  Loss: 0.000226\n",
      "PARAM=100  Epoch [3500/50000]  MSE: 0.000050  L1: 178.16  Loss: 0.000228\n",
      "PARAM=100  Epoch [4000/50000]  MSE: 0.000030  L1: 164.10  Loss: 0.000194\n",
      "PARAM=100  Epoch [4500/50000]  MSE: 0.000010  L1: 152.61  Loss: 0.000163\n",
      "PARAM=100  Epoch [5000/50000]  MSE: 0.000004  L1: 142.07  Loss: 0.000146\n",
      "PARAM=100  Epoch [5500/50000]  MSE: 0.000003  L1: 133.29  Loss: 0.000136\n",
      "PARAM=100  Epoch [6000/50000]  MSE: 0.000005  L1: 126.59  Loss: 0.000131\n",
      "PARAM=100  Epoch [6500/50000]  MSE: 0.000007  L1: 120.46  Loss: 0.000128\n",
      "PARAM=100  Epoch [7000/50000]  MSE: 0.000002  L1: 115.43  Loss: 0.000117\n",
      "PARAM=100  Epoch [7500/50000]  MSE: 0.000012  L1: 112.14  Loss: 0.000124\n",
      "PARAM=100  Epoch [8000/50000]  MSE: 0.000008  L1: 107.30  Loss: 0.000115\n",
      "PARAM=100  Epoch [8500/50000]  MSE: 0.000010  L1: 105.16  Loss: 0.000115\n",
      "PARAM=100  Epoch [9000/50000]  MSE: 0.000001  L1: 101.01  Loss: 0.000102\n",
      "PARAM=100  Epoch [9500/50000]  MSE: 0.000000  L1: 99.01  Loss: 0.000099\n",
      "PARAM=100  Epoch [10000/50000]  MSE: 0.000032  L1: 96.44  Loss: 0.000128\n",
      "PARAM=100  Epoch [10500/50000]  MSE: 0.000001  L1: 94.39  Loss: 0.000095\n",
      "PARAM=100  Epoch [11000/50000]  MSE: 0.000001  L1: 92.19  Loss: 0.000093\n",
      "PARAM=100  Epoch [11500/50000]  MSE: 0.000001  L1: 91.15  Loss: 0.000092\n",
      "PARAM=100  Epoch [12000/50000]  MSE: 0.000026  L1: 91.44  Loss: 0.000118\n",
      "PARAM=100  Epoch [12500/50000]  MSE: 0.000038  L1: 91.54  Loss: 0.000130\n",
      "PARAM=100  Epoch [13000/50000]  MSE: 0.000023  L1: 89.47  Loss: 0.000112\n",
      "PARAM=100  Epoch [13500/50000]  MSE: 0.000002  L1: 86.41  Loss: 0.000089\n",
      "PARAM=100  Epoch [14000/50000]  MSE: 0.000016  L1: 85.86  Loss: 0.000101\n",
      "PARAM=100  Epoch [14500/50000]  MSE: 0.000006  L1: 91.36  Loss: 0.000097\n",
      "PARAM=100  Epoch [15000/50000]  MSE: 0.000002  L1: 86.80  Loss: 0.000089\n",
      "PARAM=100  Epoch [15500/50000]  MSE: 0.000033  L1: 85.79  Loss: 0.000118\n",
      "PARAM=100  Epoch [16000/50000]  MSE: 0.000006  L1: 84.74  Loss: 0.000091\n",
      "PARAM=100  Epoch [16500/50000]  MSE: 0.000001  L1: 84.04  Loss: 0.000085\n",
      "PARAM=100  Epoch [17000/50000]  MSE: 0.000004  L1: 83.17  Loss: 0.000087\n",
      "PARAM=100  Epoch [17500/50000]  MSE: 0.000004  L1: 83.59  Loss: 0.000088\n",
      "PARAM=100  Epoch [18000/50000]  MSE: 0.000001  L1: 81.77  Loss: 0.000083\n",
      "PARAM=100  Epoch [18500/50000]  MSE: 0.000005  L1: 81.65  Loss: 0.000087\n",
      "PARAM=100  Epoch [19000/50000]  MSE: 0.000001  L1: 81.95  Loss: 0.000083\n",
      "PARAM=100  Epoch [19500/50000]  MSE: 0.000001  L1: 81.28  Loss: 0.000082\n",
      "PARAM=100  Epoch [20000/50000]  MSE: 0.000001  L1: 80.94  Loss: 0.000082\n",
      "PARAM=100  Epoch [20500/50000]  MSE: 0.000001  L1: 81.32  Loss: 0.000083\n",
      "PARAM=100  Epoch [21000/50000]  MSE: 0.000001  L1: 80.66  Loss: 0.000082\n",
      "PARAM=100  Epoch [21500/50000]  MSE: 0.000001  L1: 80.59  Loss: 0.000081\n",
      "PARAM=100  Epoch [22000/50000]  MSE: 0.000001  L1: 80.27  Loss: 0.000081\n",
      "PARAM=100  Epoch [22500/50000]  MSE: 0.000001  L1: 80.06  Loss: 0.000081\n",
      "PARAM=100  Epoch [23000/50000]  MSE: 0.000002  L1: 80.11  Loss: 0.000083\n",
      "PARAM=100  Epoch [23500/50000]  MSE: 0.000002  L1: 79.86  Loss: 0.000082\n",
      "PARAM=100  Epoch [24000/50000]  MSE: 0.000001  L1: 79.83  Loss: 0.000081\n",
      "PARAM=100  Epoch [24500/50000]  MSE: 0.000001  L1: 79.22  Loss: 0.000080\n",
      "PARAM=100  Epoch [25000/50000]  MSE: 0.000002  L1: 79.40  Loss: 0.000081\n",
      "PARAM=100  Epoch [25500/50000]  MSE: 0.000002  L1: 78.90  Loss: 0.000081\n",
      "PARAM=100  Epoch [26000/50000]  MSE: 0.000002  L1: 78.48  Loss: 0.000080\n",
      "PARAM=100  Epoch [26500/50000]  MSE: 0.000002  L1: 78.68  Loss: 0.000081\n",
      "PARAM=100  Epoch [27000/50000]  MSE: 0.000003  L1: 78.01  Loss: 0.000081\n",
      "PARAM=100  Epoch [27500/50000]  MSE: 0.000001  L1: 77.68  Loss: 0.000079\n",
      "PARAM=100  Epoch [28000/50000]  MSE: 0.000001  L1: 78.95  Loss: 0.000080\n",
      "PARAM=100  Epoch [28500/50000]  MSE: 0.000005  L1: 77.59  Loss: 0.000082\n",
      "PARAM=100  Epoch [29000/50000]  MSE: 0.000001  L1: 77.20  Loss: 0.000078\n",
      "PARAM=100  Epoch [29500/50000]  MSE: 0.000011  L1: 77.59  Loss: 0.000089\n",
      "PARAM=100  Epoch [30000/50000]  MSE: 0.000001  L1: 77.06  Loss: 0.000078\n",
      "PARAM=100  Epoch [30500/50000]  MSE: 0.000001  L1: 77.28  Loss: 0.000078\n",
      "PARAM=100  Epoch [31000/50000]  MSE: 0.000002  L1: 77.17  Loss: 0.000079\n",
      "PARAM=100  Epoch [31500/50000]  MSE: 0.000001  L1: 76.91  Loss: 0.000078\n",
      "PARAM=100  Epoch [32000/50000]  MSE: 0.000001  L1: 76.36  Loss: 0.000078\n",
      "PARAM=100  Epoch [32500/50000]  MSE: 0.000005  L1: 76.67  Loss: 0.000082\n",
      "PARAM=100  Epoch [33000/50000]  MSE: 0.000003  L1: 77.27  Loss: 0.000081\n",
      "PARAM=100  Epoch [33500/50000]  MSE: 0.000006  L1: 75.88  Loss: 0.000082\n",
      "PARAM=100  Epoch [34000/50000]  MSE: 0.000014  L1: 75.97  Loss: 0.000090\n",
      "PARAM=100  Epoch [34500/50000]  MSE: 0.000001  L1: 75.66  Loss: 0.000077\n",
      "PARAM=100  Epoch [35000/50000]  MSE: 0.000004  L1: 75.44  Loss: 0.000080\n",
      "PARAM=100  Epoch [35500/50000]  MSE: 0.000004  L1: 75.39  Loss: 0.000079\n",
      "PARAM=100  Epoch [36000/50000]  MSE: 0.000005  L1: 75.78  Loss: 0.000081\n",
      "PARAM=100  Epoch [36500/50000]  MSE: 0.000005  L1: 75.29  Loss: 0.000080\n",
      "PARAM=100  Epoch [37000/50000]  MSE: 0.000006  L1: 75.85  Loss: 0.000081\n",
      "PARAM=100  Epoch [37500/50000]  MSE: 0.000002  L1: 75.58  Loss: 0.000077\n",
      "PARAM=100  Epoch [38000/50000]  MSE: 0.000024  L1: 75.81  Loss: 0.000100\n",
      "PARAM=100  Epoch [38500/50000]  MSE: 0.000002  L1: 75.36  Loss: 0.000077\n",
      "PARAM=100  Epoch [39000/50000]  MSE: 0.000004  L1: 75.24  Loss: 0.000079\n",
      "PARAM=100  Epoch [39500/50000]  MSE: 0.000009  L1: 76.47  Loss: 0.000085\n",
      "PARAM=100  Epoch [40000/50000]  MSE: 0.000013  L1: 75.50  Loss: 0.000088\n",
      "PARAM=100  Epoch [40500/50000]  MSE: 0.000024  L1: 76.44  Loss: 0.000100\n",
      "PARAM=100  Epoch [41000/50000]  MSE: 0.000006  L1: 75.52  Loss: 0.000082\n",
      "PARAM=100  Epoch [41500/50000]  MSE: 0.000001  L1: 75.58  Loss: 0.000076\n",
      "PARAM=100  Epoch [42000/50000]  MSE: 0.000004  L1: 76.13  Loss: 0.000080\n",
      "PARAM=100  Epoch [42500/50000]  MSE: 0.000006  L1: 74.86  Loss: 0.000081\n",
      "PARAM=100  Epoch [43000/50000]  MSE: 0.000015  L1: 75.46  Loss: 0.000091\n",
      "PARAM=100  Epoch [43500/50000]  MSE: 0.000001  L1: 75.97  Loss: 0.000077\n",
      "PARAM=100  Epoch [44000/50000]  MSE: 0.000001  L1: 75.01  Loss: 0.000076\n",
      "PARAM=100  Epoch [44500/50000]  MSE: 0.000001  L1: 75.50  Loss: 0.000076\n",
      "PARAM=100  Epoch [45000/50000]  MSE: 0.000005  L1: 75.73  Loss: 0.000080\n",
      "PARAM=100  Epoch [45500/50000]  MSE: 0.000001  L1: 74.79  Loss: 0.000075\n",
      "PARAM=100  Epoch [46000/50000]  MSE: 0.000002  L1: 76.34  Loss: 0.000079\n",
      "PARAM=100  Epoch [46500/50000]  MSE: 0.000002  L1: 75.00  Loss: 0.000077\n",
      "PARAM=100  Epoch [47000/50000]  MSE: 0.000000  L1: 75.52  Loss: 0.000076\n",
      "PARAM=100  Epoch [47500/50000]  MSE: 0.000001  L1: 76.79  Loss: 0.000078\n",
      "PARAM=100  Epoch [48000/50000]  MSE: 0.000018  L1: 75.16  Loss: 0.000093\n",
      "PARAM=100  Epoch [48500/50000]  MSE: 0.000004  L1: 74.65  Loss: 0.000078\n",
      "PARAM=100  Epoch [49000/50000]  MSE: 0.000001  L1: 74.91  Loss: 0.000076\n",
      "PARAM=100  Epoch [49500/50000]  MSE: 0.000002  L1: 74.91  Loss: 0.000077\n",
      "\n",
      "PARAM=100 予測結果：\n",
      "[[0.9291707  0.11651975 0.1424074 ]\n",
      " [0.41107228 0.7392385  0.27129507]\n",
      " [0.22013915 0.32539326 0.6424341 ]\n",
      " [0.964897   0.91888213 0.08028691]\n",
      " [0.43581027 0.7976184  0.8683146 ]\n",
      " [0.72109205 0.3174443  0.6203105 ]\n",
      " [0.00100932 0.00405155 0.00355218]\n",
      " [0.5027332  0.49603373 0.49849725]\n",
      " [1.000943   0.9965862  1.0006777 ]\n",
      " [0.45367998 0.30826765 0.25840193]\n",
      " [0.7657133  0.55840003 0.4944855 ]\n",
      " [0.35590568 0.47335732 0.6128739 ]\n",
      " [0.35578018 0.42267817 0.2501725 ]\n",
      " [0.5177849  0.49604774 0.68623585]\n",
      " [0.3769653  0.73933166 0.6788083 ]\n",
      " [0.87103724 0.47435844 0.18774828]\n",
      " [0.27011123 0.3567305  0.6559354 ]\n",
      " [0.7766778  0.31188273 0.37769312]\n",
      " [0.36314863 0.23218635 0.41265965]\n",
      " [0.61506265 0.73677975 0.23023163]\n",
      " [0.89425087 0.6320239  0.1577857 ]\n",
      " [0.15373078 0.24685532 0.5652581 ]\n",
      " [0.23592559 0.5775758  0.27410156]\n",
      " [0.70134443 0.21566644 0.22816621]\n",
      " [0.9184351  0.77507865 0.10867942]\n",
      " [0.7519355  0.30756032 0.5725978 ]\n",
      " [0.00557248 0.51994354 0.6461963 ]\n",
      " [0.9463641  0.94318676 0.9187382 ]\n",
      " [0.7886387  0.7899198  0.78723025]\n",
      " [0.6304101  0.633455   0.6375477 ]\n",
      " [0.47765273 0.47793937 0.47904766]\n",
      " [0.32864937 0.33361715 0.33719224]\n",
      " [0.19636549 0.19572975 0.19658531]]\n",
      "C++ 用のパラメータファイル (nomal_model_parameters_100.h) を作成しました。\n",
      "\n",
      "PARAM=200  Epoch [0/50000]  MSE: 0.301051  L1: 1611.82  Loss: 0.302663\n",
      "PARAM=200  Epoch [500/50000]  MSE: 0.000103  L1: 749.33  Loss: 0.000852\n",
      "PARAM=200  Epoch [1000/50000]  MSE: 0.000036  L1: 509.59  Loss: 0.000545\n",
      "PARAM=200  Epoch [1500/50000]  MSE: 0.000029  L1: 391.49  Loss: 0.000421\n",
      "PARAM=200  Epoch [2000/50000]  MSE: 0.000097  L1: 322.73  Loss: 0.000419\n",
      "PARAM=200  Epoch [2500/50000]  MSE: 0.000095  L1: 282.87  Loss: 0.000378\n",
      "PARAM=200  Epoch [3000/50000]  MSE: 0.000023  L1: 249.90  Loss: 0.000273\n",
      "PARAM=200  Epoch [3500/50000]  MSE: 0.000026  L1: 222.99  Loss: 0.000249\n",
      "PARAM=200  Epoch [4000/50000]  MSE: 0.000013  L1: 203.66  Loss: 0.000216\n",
      "PARAM=200  Epoch [4500/50000]  MSE: 0.000007  L1: 188.11  Loss: 0.000195\n",
      "PARAM=200  Epoch [5000/50000]  MSE: 0.000005  L1: 174.72  Loss: 0.000180\n",
      "PARAM=200  Epoch [5500/50000]  MSE: 0.000007  L1: 166.36  Loss: 0.000174\n",
      "PARAM=200  Epoch [6000/50000]  MSE: 0.000002  L1: 159.42  Loss: 0.000162\n",
      "PARAM=200  Epoch [6500/50000]  MSE: 0.000004  L1: 152.29  Loss: 0.000156\n",
      "PARAM=200  Epoch [7000/50000]  MSE: 0.000007  L1: 147.62  Loss: 0.000154\n",
      "PARAM=200  Epoch [7500/50000]  MSE: 0.000065  L1: 144.04  Loss: 0.000209\n",
      "PARAM=200  Epoch [8000/50000]  MSE: 0.000017  L1: 140.75  Loss: 0.000158\n",
      "PARAM=200  Epoch [8500/50000]  MSE: 0.000023  L1: 139.04  Loss: 0.000162\n",
      "PARAM=200  Epoch [9000/50000]  MSE: 0.000012  L1: 137.64  Loss: 0.000150\n",
      "PARAM=200  Epoch [9500/50000]  MSE: 0.000001  L1: 135.00  Loss: 0.000136\n",
      "PARAM=200  Epoch [10000/50000]  MSE: 0.000022  L1: 132.91  Loss: 0.000155\n",
      "PARAM=200  Epoch [10500/50000]  MSE: 0.000033  L1: 131.71  Loss: 0.000165\n",
      "PARAM=200  Epoch [11000/50000]  MSE: 0.000018  L1: 128.40  Loss: 0.000146\n",
      "PARAM=200  Epoch [11500/50000]  MSE: 0.000002  L1: 129.01  Loss: 0.000131\n",
      "PARAM=200  Epoch [12000/50000]  MSE: 0.000007  L1: 126.14  Loss: 0.000133\n",
      "PARAM=200  Epoch [12500/50000]  MSE: 0.000003  L1: 125.92  Loss: 0.000129\n",
      "PARAM=200  Epoch [13000/50000]  MSE: 0.000005  L1: 125.66  Loss: 0.000131\n",
      "PARAM=200  Epoch [13500/50000]  MSE: 0.000004  L1: 124.75  Loss: 0.000128\n",
      "PARAM=200  Epoch [14000/50000]  MSE: 0.000001  L1: 123.45  Loss: 0.000125\n",
      "PARAM=200  Epoch [14500/50000]  MSE: 0.000002  L1: 122.33  Loss: 0.000124\n",
      "PARAM=200  Epoch [15000/50000]  MSE: 0.000004  L1: 121.89  Loss: 0.000126\n",
      "PARAM=200  Epoch [15500/50000]  MSE: 0.000008  L1: 120.06  Loss: 0.000128\n",
      "PARAM=200  Epoch [16000/50000]  MSE: 0.000005  L1: 120.18  Loss: 0.000125\n",
      "PARAM=200  Epoch [16500/50000]  MSE: 0.000005  L1: 119.51  Loss: 0.000125\n",
      "PARAM=200  Epoch [17000/50000]  MSE: 0.000003  L1: 117.99  Loss: 0.000121\n",
      "PARAM=200  Epoch [17500/50000]  MSE: 0.000001  L1: 119.15  Loss: 0.000120\n",
      "PARAM=200  Epoch [18000/50000]  MSE: 0.000009  L1: 117.91  Loss: 0.000127\n",
      "PARAM=200  Epoch [18500/50000]  MSE: 0.000003  L1: 116.85  Loss: 0.000120\n",
      "PARAM=200  Epoch [19000/50000]  MSE: 0.000074  L1: 116.03  Loss: 0.000190\n",
      "PARAM=200  Epoch [19500/50000]  MSE: 0.000005  L1: 116.11  Loss: 0.000121\n",
      "PARAM=200  Epoch [20000/50000]  MSE: 0.000001  L1: 114.75  Loss: 0.000115\n",
      "PARAM=200  Epoch [20500/50000]  MSE: 0.000007  L1: 114.73  Loss: 0.000121\n",
      "PARAM=200  Epoch [21000/50000]  MSE: 0.000001  L1: 114.20  Loss: 0.000115\n",
      "PARAM=200  Epoch [21500/50000]  MSE: 0.000001  L1: 111.95  Loss: 0.000113\n",
      "PARAM=200  Epoch [22000/50000]  MSE: 0.000003  L1: 112.97  Loss: 0.000116\n",
      "PARAM=200  Epoch [22500/50000]  MSE: 0.000002  L1: 112.12  Loss: 0.000114\n",
      "PARAM=200  Epoch [23000/50000]  MSE: 0.000003  L1: 112.24  Loss: 0.000115\n",
      "PARAM=200  Epoch [23500/50000]  MSE: 0.000002  L1: 112.62  Loss: 0.000115\n",
      "PARAM=200  Epoch [24000/50000]  MSE: 0.000002  L1: 112.01  Loss: 0.000114\n",
      "PARAM=200  Epoch [24500/50000]  MSE: 0.000022  L1: 111.78  Loss: 0.000133\n",
      "PARAM=200  Epoch [25000/50000]  MSE: 0.000001  L1: 111.07  Loss: 0.000112\n",
      "PARAM=200  Epoch [25500/50000]  MSE: 0.000001  L1: 111.84  Loss: 0.000113\n",
      "PARAM=200  Epoch [26000/50000]  MSE: 0.000076  L1: 110.10  Loss: 0.000186\n",
      "PARAM=200  Epoch [26500/50000]  MSE: 0.000001  L1: 112.01  Loss: 0.000113\n",
      "PARAM=200  Epoch [27000/50000]  MSE: 0.000002  L1: 111.38  Loss: 0.000113\n",
      "PARAM=200  Epoch [27500/50000]  MSE: 0.000002  L1: 110.52  Loss: 0.000113\n",
      "PARAM=200  Epoch [28000/50000]  MSE: 0.000004  L1: 111.54  Loss: 0.000116\n",
      "PARAM=200  Epoch [28500/50000]  MSE: 0.000004  L1: 110.58  Loss: 0.000115\n",
      "PARAM=200  Epoch [29000/50000]  MSE: 0.000001  L1: 110.36  Loss: 0.000112\n",
      "PARAM=200  Epoch [29500/50000]  MSE: 0.000004  L1: 110.41  Loss: 0.000115\n",
      "PARAM=200  Epoch [30000/50000]  MSE: 0.000002  L1: 110.65  Loss: 0.000112\n",
      "PARAM=200  Epoch [30500/50000]  MSE: 0.000006  L1: 109.90  Loss: 0.000116\n",
      "PARAM=200  Epoch [31000/50000]  MSE: 0.000003  L1: 110.45  Loss: 0.000114\n",
      "PARAM=200  Epoch [31500/50000]  MSE: 0.000001  L1: 111.02  Loss: 0.000112\n",
      "PARAM=200  Epoch [32000/50000]  MSE: 0.000000  L1: 109.25  Loss: 0.000110\n",
      "PARAM=200  Epoch [32500/50000]  MSE: 0.000014  L1: 110.45  Loss: 0.000125\n",
      "PARAM=200  Epoch [33000/50000]  MSE: 0.000011  L1: 109.48  Loss: 0.000120\n",
      "PARAM=200  Epoch [33500/50000]  MSE: 0.000003  L1: 109.31  Loss: 0.000112\n",
      "PARAM=200  Epoch [34000/50000]  MSE: 0.000003  L1: 109.60  Loss: 0.000113\n",
      "PARAM=200  Epoch [34500/50000]  MSE: 0.000002  L1: 109.37  Loss: 0.000111\n",
      "PARAM=200  Epoch [35000/50000]  MSE: 0.000001  L1: 109.11  Loss: 0.000110\n",
      "PARAM=200  Epoch [35500/50000]  MSE: 0.000007  L1: 109.16  Loss: 0.000116\n",
      "PARAM=200  Epoch [36000/50000]  MSE: 0.000003  L1: 109.46  Loss: 0.000113\n",
      "PARAM=200  Epoch [36500/50000]  MSE: 0.000004  L1: 108.26  Loss: 0.000112\n",
      "PARAM=200  Epoch [37000/50000]  MSE: 0.000003  L1: 108.92  Loss: 0.000112\n",
      "PARAM=200  Epoch [37500/50000]  MSE: 0.000001  L1: 109.39  Loss: 0.000110\n",
      "PARAM=200  Epoch [38000/50000]  MSE: 0.000002  L1: 107.98  Loss: 0.000110\n",
      "PARAM=200  Epoch [38500/50000]  MSE: 0.000023  L1: 109.39  Loss: 0.000132\n",
      "PARAM=200  Epoch [39000/50000]  MSE: 0.000003  L1: 108.03  Loss: 0.000111\n",
      "PARAM=200  Epoch [39500/50000]  MSE: 0.000002  L1: 107.96  Loss: 0.000110\n",
      "PARAM=200  Epoch [40000/50000]  MSE: 0.000008  L1: 108.17  Loss: 0.000116\n",
      "PARAM=200  Epoch [40500/50000]  MSE: 0.000003  L1: 108.11  Loss: 0.000111\n",
      "PARAM=200  Epoch [41000/50000]  MSE: 0.000002  L1: 106.74  Loss: 0.000109\n",
      "PARAM=200  Epoch [41500/50000]  MSE: 0.000000  L1: 106.97  Loss: 0.000107\n",
      "PARAM=200  Epoch [42000/50000]  MSE: 0.000012  L1: 107.69  Loss: 0.000120\n",
      "PARAM=200  Epoch [42500/50000]  MSE: 0.000001  L1: 105.63  Loss: 0.000107\n",
      "PARAM=200  Epoch [43000/50000]  MSE: 0.000007  L1: 106.90  Loss: 0.000114\n",
      "PARAM=200  Epoch [43500/50000]  MSE: 0.000009  L1: 106.26  Loss: 0.000116\n",
      "PARAM=200  Epoch [44000/50000]  MSE: 0.000022  L1: 106.02  Loss: 0.000128\n",
      "PARAM=200  Epoch [44500/50000]  MSE: 0.000000  L1: 106.80  Loss: 0.000107\n",
      "PARAM=200  Epoch [45000/50000]  MSE: 0.000012  L1: 106.76  Loss: 0.000119\n",
      "PARAM=200  Epoch [45500/50000]  MSE: 0.000000  L1: 106.14  Loss: 0.000107\n",
      "PARAM=200  Epoch [46000/50000]  MSE: 0.000007  L1: 105.72  Loss: 0.000112\n",
      "PARAM=200  Epoch [46500/50000]  MSE: 0.000001  L1: 106.70  Loss: 0.000108\n",
      "PARAM=200  Epoch [47000/50000]  MSE: 0.000005  L1: 104.95  Loss: 0.000110\n",
      "PARAM=200  Epoch [47500/50000]  MSE: 0.000004  L1: 106.36  Loss: 0.000110\n",
      "PARAM=200  Epoch [48000/50000]  MSE: 0.000003  L1: 106.06  Loss: 0.000109\n",
      "PARAM=200  Epoch [48500/50000]  MSE: 0.000008  L1: 105.49  Loss: 0.000113\n",
      "PARAM=200  Epoch [49000/50000]  MSE: 0.000005  L1: 106.34  Loss: 0.000111\n",
      "PARAM=200  Epoch [49500/50000]  MSE: 0.000002  L1: 105.61  Loss: 0.000108\n",
      "\n",
      "PARAM=200 予測結果：\n",
      "[[0.92873484 0.11654182 0.14329633]\n",
      " [0.41182348 0.7406491  0.27092448]\n",
      " [0.21959852 0.32439694 0.64406306]\n",
      " [0.9633456  0.92137074 0.07972959]\n",
      " [0.4344707  0.799904   0.8674865 ]\n",
      " [0.72068906 0.31634524 0.6233847 ]\n",
      " [0.00404098 0.00297949 0.00608674]\n",
      " [0.5020848  0.49713132 0.49982297]\n",
      " [1.0008695  1.0001316  1.000267  ]\n",
      " [0.45278242 0.30858684 0.2610467 ]\n",
      " [0.7631348  0.55923474 0.49483168]\n",
      " [0.35520798 0.47358218 0.6122522 ]\n",
      " [0.35724655 0.42347986 0.25281316]\n",
      " [0.51749456 0.4965052  0.6877555 ]\n",
      " [0.37639037 0.74090075 0.6797906 ]\n",
      " [0.86981785 0.47647023 0.19061965]\n",
      " [0.26992142 0.3546321  0.65560555]\n",
      " [0.77729225 0.3124247  0.37989774]\n",
      " [0.36462688 0.23005031 0.41388062]\n",
      " [0.61461455 0.7365992  0.23172776]\n",
      " [0.8929832  0.6302245  0.16066691]\n",
      " [0.15377721 0.24598968 0.56566554]\n",
      " [0.23471472 0.57961994 0.27621043]\n",
      " [0.7014369  0.21531549 0.22895534]\n",
      " [0.9178999  0.77605087 0.11325327]\n",
      " [0.7506507  0.3086043  0.57431895]\n",
      " [0.00438476 0.5204364  0.6483893 ]\n",
      " [0.94707614 0.9455145  0.920431  ]\n",
      " [0.78777915 0.7922938  0.79063773]\n",
      " [0.6298465  0.6330583  0.63806516]\n",
      " [0.47838953 0.47599715 0.48004484]\n",
      " [0.3288889  0.33121288 0.3389498 ]\n",
      " [0.19546092 0.19332096 0.1976121 ]]\n",
      "C++ 用のパラメータファイル (nomal_model_parameters_200.h) を作成しました。\n",
      "\n",
      "PARAM=500  Epoch [0/50000]  MSE: 0.269907  L1: 6059.41  Loss: 0.275967\n",
      "PARAM=500  Epoch [500/50000]  MSE: 0.000121  L1: 4134.71  Loss: 0.004256\n",
      "PARAM=500  Epoch [1000/50000]  MSE: 0.000249  L1: 2817.52  Loss: 0.003067\n",
      "PARAM=500  Epoch [1500/50000]  MSE: 0.000026  L1: 2103.99  Loss: 0.002130\n",
      "PARAM=500  Epoch [2000/50000]  MSE: 0.000023  L1: 1655.68  Loss: 0.001679\n",
      "PARAM=500  Epoch [2500/50000]  MSE: 0.000030  L1: 1339.02  Loss: 0.001369\n",
      "PARAM=500  Epoch [3000/50000]  MSE: 0.000748  L1: 1065.82  Loss: 0.001814\n",
      "PARAM=500  Epoch [3500/50000]  MSE: 0.000051  L1: 875.72  Loss: 0.000926\n",
      "PARAM=500  Epoch [4000/50000]  MSE: 0.000024  L1: 728.07  Loss: 0.000753\n",
      "PARAM=500  Epoch [4500/50000]  MSE: 0.000005  L1: 630.92  Loss: 0.000636\n",
      "PARAM=500  Epoch [5000/50000]  MSE: 0.000011  L1: 557.77  Loss: 0.000569\n",
      "PARAM=500  Epoch [5500/50000]  MSE: 0.000006  L1: 528.09  Loss: 0.000534\n",
      "PARAM=500  Epoch [6000/50000]  MSE: 0.000136  L1: 495.30  Loss: 0.000631\n",
      "PARAM=500  Epoch [6500/50000]  MSE: 0.000010  L1: 470.31  Loss: 0.000480\n",
      "PARAM=500  Epoch [7000/50000]  MSE: 0.000019  L1: 458.70  Loss: 0.000477\n",
      "PARAM=500  Epoch [7500/50000]  MSE: 0.000145  L1: 429.18  Loss: 0.000574\n",
      "PARAM=500  Epoch [8000/50000]  MSE: 0.000002  L1: 416.37  Loss: 0.000418\n",
      "PARAM=500  Epoch [8500/50000]  MSE: 0.000005  L1: 407.22  Loss: 0.000412\n",
      "PARAM=500  Epoch [9000/50000]  MSE: 0.000073  L1: 395.97  Loss: 0.000469\n",
      "PARAM=500  Epoch [9500/50000]  MSE: 0.000044  L1: 389.19  Loss: 0.000433\n",
      "PARAM=500  Epoch [10000/50000]  MSE: 0.000048  L1: 386.91  Loss: 0.000435\n",
      "PARAM=500  Epoch [10500/50000]  MSE: 0.000004  L1: 385.42  Loss: 0.000389\n",
      "PARAM=500  Epoch [11000/50000]  MSE: 0.000006  L1: 374.99  Loss: 0.000381\n",
      "PARAM=500  Epoch [11500/50000]  MSE: 0.000027  L1: 389.81  Loss: 0.000417\n",
      "PARAM=500  Epoch [12000/50000]  MSE: 0.000022  L1: 375.44  Loss: 0.000398\n",
      "PARAM=500  Epoch [12500/50000]  MSE: 0.000003  L1: 382.42  Loss: 0.000386\n",
      "PARAM=500  Epoch [13000/50000]  MSE: 0.000005  L1: 390.09  Loss: 0.000395\n",
      "PARAM=500  Epoch [13500/50000]  MSE: 0.000007  L1: 385.79  Loss: 0.000393\n",
      "PARAM=500  Epoch [14000/50000]  MSE: 0.000007  L1: 386.27  Loss: 0.000393\n",
      "PARAM=500  Epoch [14500/50000]  MSE: 0.000007  L1: 391.70  Loss: 0.000399\n",
      "PARAM=500  Epoch [15000/50000]  MSE: 0.000004  L1: 394.47  Loss: 0.000398\n",
      "PARAM=500  Epoch [15500/50000]  MSE: 0.000012  L1: 389.98  Loss: 0.000402\n",
      "PARAM=500  Epoch [16000/50000]  MSE: 0.000003  L1: 399.76  Loss: 0.000403\n",
      "PARAM=500  Epoch [16500/50000]  MSE: 0.000009  L1: 394.90  Loss: 0.000404\n",
      "PARAM=500  Epoch [17000/50000]  MSE: 0.000020  L1: 390.43  Loss: 0.000410\n",
      "PARAM=500  Epoch [17500/50000]  MSE: 0.000002  L1: 404.14  Loss: 0.000406\n",
      "PARAM=500  Epoch [18000/50000]  MSE: 0.000012  L1: 391.08  Loss: 0.000403\n",
      "PARAM=500  Epoch [18500/50000]  MSE: 0.000032  L1: 394.19  Loss: 0.000426\n",
      "PARAM=500  Epoch [19000/50000]  MSE: 0.000003  L1: 392.97  Loss: 0.000396\n",
      "PARAM=500  Epoch [19500/50000]  MSE: 0.000005  L1: 394.39  Loss: 0.000400\n",
      "PARAM=500  Epoch [20000/50000]  MSE: 0.000007  L1: 392.50  Loss: 0.000399\n",
      "PARAM=500  Epoch [20500/50000]  MSE: 0.000043  L1: 395.67  Loss: 0.000439\n",
      "PARAM=500  Epoch [21000/50000]  MSE: 0.000008  L1: 392.31  Loss: 0.000401\n",
      "PARAM=500  Epoch [21500/50000]  MSE: 0.000011  L1: 385.98  Loss: 0.000397\n",
      "PARAM=500  Epoch [22000/50000]  MSE: 0.000009  L1: 399.65  Loss: 0.000409\n",
      "PARAM=500  Epoch [22500/50000]  MSE: 0.000001  L1: 386.30  Loss: 0.000387\n",
      "PARAM=500  Epoch [23000/50000]  MSE: 0.000002  L1: 386.49  Loss: 0.000388\n",
      "PARAM=500  Epoch [23500/50000]  MSE: 0.000016  L1: 392.87  Loss: 0.000408\n",
      "PARAM=500  Epoch [24000/50000]  MSE: 0.000001  L1: 384.70  Loss: 0.000386\n",
      "PARAM=500  Epoch [24500/50000]  MSE: 0.000006  L1: 390.05  Loss: 0.000396\n",
      "PARAM=500  Epoch [25000/50000]  MSE: 0.000002  L1: 384.39  Loss: 0.000386\n",
      "PARAM=500  Epoch [25500/50000]  MSE: 0.000006  L1: 387.12  Loss: 0.000393\n",
      "PARAM=500  Epoch [26000/50000]  MSE: 0.000011  L1: 377.42  Loss: 0.000388\n",
      "PARAM=500  Epoch [26500/50000]  MSE: 0.000006  L1: 391.63  Loss: 0.000398\n",
      "PARAM=500  Epoch [27000/50000]  MSE: 0.000006  L1: 383.41  Loss: 0.000389\n",
      "PARAM=500  Epoch [27500/50000]  MSE: 0.000006  L1: 379.81  Loss: 0.000386\n",
      "PARAM=500  Epoch [28000/50000]  MSE: 0.000070  L1: 389.99  Loss: 0.000460\n",
      "PARAM=500  Epoch [28500/50000]  MSE: 0.000010  L1: 378.56  Loss: 0.000389\n",
      "PARAM=500  Epoch [29000/50000]  MSE: 0.000019  L1: 384.50  Loss: 0.000403\n",
      "PARAM=500  Epoch [29500/50000]  MSE: 0.000007  L1: 385.26  Loss: 0.000392\n",
      "PARAM=500  Epoch [30000/50000]  MSE: 0.000146  L1: 382.21  Loss: 0.000528\n",
      "PARAM=500  Epoch [30500/50000]  MSE: 0.000020  L1: 382.90  Loss: 0.000403\n",
      "PARAM=500  Epoch [31000/50000]  MSE: 0.000004  L1: 385.79  Loss: 0.000390\n",
      "PARAM=500  Epoch [31500/50000]  MSE: 0.000013  L1: 386.15  Loss: 0.000399\n",
      "PARAM=500  Epoch [32000/50000]  MSE: 0.000008  L1: 374.89  Loss: 0.000382\n",
      "PARAM=500  Epoch [32500/50000]  MSE: 0.000014  L1: 386.64  Loss: 0.000401\n",
      "PARAM=500  Epoch [33000/50000]  MSE: 0.000011  L1: 376.91  Loss: 0.000388\n",
      "PARAM=500  Epoch [33500/50000]  MSE: 0.000035  L1: 382.38  Loss: 0.000418\n",
      "PARAM=500  Epoch [34000/50000]  MSE: 0.000008  L1: 384.83  Loss: 0.000393\n",
      "PARAM=500  Epoch [34500/50000]  MSE: 0.000004  L1: 379.65  Loss: 0.000383\n",
      "PARAM=500  Epoch [35000/50000]  MSE: 0.000003  L1: 380.90  Loss: 0.000383\n",
      "PARAM=500  Epoch [35500/50000]  MSE: 0.000004  L1: 382.93  Loss: 0.000387\n",
      "PARAM=500  Epoch [36000/50000]  MSE: 0.000007  L1: 381.62  Loss: 0.000389\n",
      "PARAM=500  Epoch [36500/50000]  MSE: 0.000001  L1: 374.29  Loss: 0.000376\n",
      "PARAM=500  Epoch [37000/50000]  MSE: 0.000045  L1: 384.39  Loss: 0.000429\n",
      "PARAM=500  Epoch [37500/50000]  MSE: 0.000109  L1: 382.39  Loss: 0.000492\n",
      "PARAM=500  Epoch [38000/50000]  MSE: 0.000010  L1: 375.66  Loss: 0.000386\n",
      "PARAM=500  Epoch [38500/50000]  MSE: 0.000004  L1: 388.26  Loss: 0.000392\n",
      "PARAM=500  Epoch [39000/50000]  MSE: 0.000003  L1: 374.25  Loss: 0.000377\n",
      "PARAM=500  Epoch [39500/50000]  MSE: 0.000002  L1: 379.54  Loss: 0.000382\n",
      "PARAM=500  Epoch [40000/50000]  MSE: 0.000004  L1: 381.69  Loss: 0.000386\n",
      "PARAM=500  Epoch [40500/50000]  MSE: 0.000005  L1: 379.81  Loss: 0.000385\n",
      "PARAM=500  Epoch [41000/50000]  MSE: 0.000006  L1: 377.11  Loss: 0.000384\n",
      "PARAM=500  Epoch [41500/50000]  MSE: 0.000014  L1: 381.11  Loss: 0.000395\n",
      "PARAM=500  Epoch [42000/50000]  MSE: 0.000015  L1: 382.48  Loss: 0.000397\n",
      "PARAM=500  Epoch [42500/50000]  MSE: 0.000003  L1: 375.19  Loss: 0.000378\n",
      "PARAM=500  Epoch [43000/50000]  MSE: 0.000004  L1: 384.45  Loss: 0.000388\n",
      "PARAM=500  Epoch [43500/50000]  MSE: 0.000001  L1: 373.32  Loss: 0.000374\n",
      "PARAM=500  Epoch [44000/50000]  MSE: 0.000004  L1: 376.16  Loss: 0.000380\n",
      "PARAM=500  Epoch [44500/50000]  MSE: 0.000002  L1: 385.78  Loss: 0.000388\n",
      "PARAM=500  Epoch [45000/50000]  MSE: 0.000000  L1: 375.93  Loss: 0.000376\n",
      "PARAM=500  Epoch [45500/50000]  MSE: 0.000002  L1: 378.28  Loss: 0.000380\n",
      "PARAM=500  Epoch [46000/50000]  MSE: 0.000001  L1: 376.42  Loss: 0.000377\n",
      "PARAM=500  Epoch [46500/50000]  MSE: 0.000011  L1: 381.03  Loss: 0.000392\n",
      "PARAM=500  Epoch [47000/50000]  MSE: 0.000001  L1: 371.18  Loss: 0.000372\n",
      "PARAM=500  Epoch [47500/50000]  MSE: 0.000003  L1: 384.92  Loss: 0.000388\n",
      "PARAM=500  Epoch [48000/50000]  MSE: 0.000009  L1: 376.84  Loss: 0.000385\n",
      "PARAM=500  Epoch [48500/50000]  MSE: 0.000001  L1: 374.96  Loss: 0.000376\n",
      "PARAM=500  Epoch [49000/50000]  MSE: 0.000001  L1: 384.77  Loss: 0.000386\n",
      "PARAM=500  Epoch [49500/50000]  MSE: 0.000001  L1: 372.76  Loss: 0.000374\n",
      "\n",
      "PARAM=500 予測結果：\n",
      "[[0.9291617  0.1173265  0.14013499]\n",
      " [0.41168007 0.74100417 0.27054965]\n",
      " [0.21933278 0.32507843 0.64272714]\n",
      " [0.9646289  0.9203343  0.07652602]\n",
      " [0.43523732 0.79907113 0.8666882 ]\n",
      " [0.7220675  0.31634402 0.61894876]\n",
      " [0.00346854 0.00405173 0.00416426]\n",
      " [0.5027101  0.49912336 0.49666694]\n",
      " [0.99854404 0.9984017  0.9981345 ]\n",
      " [0.45420924 0.3089335  0.25819063]\n",
      " [0.7638217  0.5617268  0.49324283]\n",
      " [0.35614693 0.47383398 0.61064106]\n",
      " [0.35698634 0.4244222  0.2504258 ]\n",
      " [0.5170965  0.49798396 0.68659145]\n",
      " [0.3762697  0.7412625  0.67745006]\n",
      " [0.8710682  0.47859827 0.18654773]\n",
      " [0.26943523 0.35628027 0.65475166]\n",
      " [0.77578235 0.3123351  0.3757602 ]\n",
      " [0.3648676  0.23089415 0.41051364]\n",
      " [0.6154922  0.7362231  0.23065102]\n",
      " [0.89360183 0.62881553 0.15663801]\n",
      " [0.15324157 0.24641538 0.5639607 ]\n",
      " [0.23484644 0.579347   0.2736901 ]\n",
      " [0.7020155  0.2147215  0.22744152]\n",
      " [0.9172056  0.776966   0.10985692]\n",
      " [0.75222915 0.31040388 0.57100284]\n",
      " [0.00389348 0.5211914  0.6462236 ]\n",
      " [0.94414985 0.94494945 0.9165115 ]\n",
      " [0.787566   0.79185015 0.7865688 ]\n",
      " [0.6311892  0.6335686  0.6340233 ]\n",
      " [0.4771599  0.4762453  0.47896075]\n",
      " [0.3289466  0.33302128 0.33622876]\n",
      " [0.19565831 0.19444641 0.19473724]]\n",
      "C++ 用のパラメータファイル (nomal_model_parameters_500.h) を作成しました。\n",
      "\n",
      "PARAM=700  Epoch [0/50000]  MSE: 0.285866  L1: 9892.41  Loss: 0.295758\n",
      "PARAM=700  Epoch [500/50000]  MSE: 0.000587  L1: 8343.30  Loss: 0.008930\n",
      "PARAM=700  Epoch [1000/50000]  MSE: 0.000628  L1: 6022.88  Loss: 0.006651\n",
      "PARAM=700  Epoch [1500/50000]  MSE: 0.000187  L1: 4748.29  Loss: 0.004935\n",
      "PARAM=700  Epoch [2000/50000]  MSE: 0.000177  L1: 3925.96  Loss: 0.004103\n",
      "PARAM=700  Epoch [2500/50000]  MSE: 0.000451  L1: 3307.65  Loss: 0.003758\n",
      "PARAM=700  Epoch [3000/50000]  MSE: 0.000113  L1: 3310.28  Loss: 0.003423\n",
      "PARAM=700  Epoch [3500/50000]  MSE: 0.000093  L1: 2868.34  Loss: 0.002961\n",
      "PARAM=700  Epoch [4000/50000]  MSE: 0.000047  L1: 2212.94  Loss: 0.002260\n",
      "PARAM=700  Epoch [4500/50000]  MSE: 0.000033  L1: 1740.08  Loss: 0.001773\n",
      "PARAM=700  Epoch [5000/50000]  MSE: 0.000024  L1: 1370.89  Loss: 0.001395\n",
      "PARAM=700  Epoch [5500/50000]  MSE: 0.000057  L1: 1158.80  Loss: 0.001215\n",
      "PARAM=700  Epoch [6000/50000]  MSE: 0.000009  L1: 988.18  Loss: 0.000998\n",
      "PARAM=700  Epoch [6500/50000]  MSE: 0.000179  L1: 887.81  Loss: 0.001066\n",
      "PARAM=700  Epoch [7000/50000]  MSE: 0.000009  L1: 855.72  Loss: 0.000865\n",
      "PARAM=700  Epoch [7500/50000]  MSE: 0.000014  L1: 792.17  Loss: 0.000807\n",
      "PARAM=700  Epoch [8000/50000]  MSE: 0.000048  L1: 777.36  Loss: 0.000825\n",
      "PARAM=700  Epoch [8500/50000]  MSE: 0.000007  L1: 760.28  Loss: 0.000767\n",
      "PARAM=700  Epoch [9000/50000]  MSE: 0.000143  L1: 729.93  Loss: 0.000873\n",
      "PARAM=700  Epoch [9500/50000]  MSE: 0.000008  L1: 715.09  Loss: 0.000723\n",
      "PARAM=700  Epoch [10000/50000]  MSE: 0.000025  L1: 705.21  Loss: 0.000730\n",
      "PARAM=700  Epoch [10500/50000]  MSE: 0.000030  L1: 696.14  Loss: 0.000726\n",
      "PARAM=700  Epoch [11000/50000]  MSE: 0.000037  L1: 662.33  Loss: 0.000699\n",
      "PARAM=700  Epoch [11500/50000]  MSE: 0.000002  L1: 688.32  Loss: 0.000690\n",
      "PARAM=700  Epoch [12000/50000]  MSE: 0.000007  L1: 649.99  Loss: 0.000657\n",
      "PARAM=700  Epoch [12500/50000]  MSE: 0.000031  L1: 661.59  Loss: 0.000693\n",
      "PARAM=700  Epoch [13000/50000]  MSE: 0.000009  L1: 671.71  Loss: 0.000681\n",
      "PARAM=700  Epoch [13500/50000]  MSE: 0.000005  L1: 658.68  Loss: 0.000664\n",
      "PARAM=700  Epoch [14000/50000]  MSE: 0.000012  L1: 664.70  Loss: 0.000676\n",
      "PARAM=700  Epoch [14500/50000]  MSE: 0.000006  L1: 673.31  Loss: 0.000679\n",
      "PARAM=700  Epoch [15000/50000]  MSE: 0.000005  L1: 676.22  Loss: 0.000681\n",
      "PARAM=700  Epoch [15500/50000]  MSE: 0.000017  L1: 670.54  Loss: 0.000688\n",
      "PARAM=700  Epoch [16000/50000]  MSE: 0.000004  L1: 696.23  Loss: 0.000700\n",
      "PARAM=700  Epoch [16500/50000]  MSE: 0.000004  L1: 685.84  Loss: 0.000690\n",
      "PARAM=700  Epoch [17000/50000]  MSE: 0.000015  L1: 681.36  Loss: 0.000696\n",
      "PARAM=700  Epoch [17500/50000]  MSE: 0.000007  L1: 715.56  Loss: 0.000722\n",
      "PARAM=700  Epoch [18000/50000]  MSE: 0.000042  L1: 686.03  Loss: 0.000728\n",
      "PARAM=700  Epoch [18500/50000]  MSE: 0.000002  L1: 700.25  Loss: 0.000703\n",
      "PARAM=700  Epoch [19000/50000]  MSE: 0.000009  L1: 698.32  Loss: 0.000708\n",
      "PARAM=700  Epoch [19500/50000]  MSE: 0.000035  L1: 693.26  Loss: 0.000728\n",
      "PARAM=700  Epoch [20000/50000]  MSE: 0.000003  L1: 693.07  Loss: 0.000696\n",
      "PARAM=700  Epoch [20500/50000]  MSE: 0.000003  L1: 706.41  Loss: 0.000710\n",
      "PARAM=700  Epoch [21000/50000]  MSE: 0.000015  L1: 696.06  Loss: 0.000711\n",
      "PARAM=700  Epoch [21500/50000]  MSE: 0.000004  L1: 679.95  Loss: 0.000684\n",
      "PARAM=700  Epoch [22000/50000]  MSE: 0.000004  L1: 707.07  Loss: 0.000711\n",
      "PARAM=700  Epoch [22500/50000]  MSE: 0.000002  L1: 680.21  Loss: 0.000682\n",
      "PARAM=700  Epoch [23000/50000]  MSE: 0.000002  L1: 686.10  Loss: 0.000689\n",
      "PARAM=700  Epoch [23500/50000]  MSE: 0.000003  L1: 702.37  Loss: 0.000706\n",
      "PARAM=700  Epoch [24000/50000]  MSE: 0.000002  L1: 681.23  Loss: 0.000683\n",
      "PARAM=700  Epoch [24500/50000]  MSE: 0.000005  L1: 698.85  Loss: 0.000704\n",
      "PARAM=700  Epoch [25000/50000]  MSE: 0.000003  L1: 689.05  Loss: 0.000692\n",
      "PARAM=700  Epoch [25500/50000]  MSE: 0.000002  L1: 690.55  Loss: 0.000692\n",
      "PARAM=700  Epoch [26000/50000]  MSE: 0.000003  L1: 672.04  Loss: 0.000675\n",
      "PARAM=700  Epoch [26500/50000]  MSE: 0.000006  L1: 704.54  Loss: 0.000710\n",
      "PARAM=700  Epoch [27000/50000]  MSE: 0.000004  L1: 684.81  Loss: 0.000689\n",
      "PARAM=700  Epoch [27500/50000]  MSE: 0.000008  L1: 682.32  Loss: 0.000691\n",
      "PARAM=700  Epoch [28000/50000]  MSE: 0.000001  L1: 704.72  Loss: 0.000706\n",
      "PARAM=700  Epoch [28500/50000]  MSE: 0.000022  L1: 669.64  Loss: 0.000692\n",
      "PARAM=700  Epoch [29000/50000]  MSE: 0.000014  L1: 687.13  Loss: 0.000701\n",
      "PARAM=700  Epoch [29500/50000]  MSE: 0.000018  L1: 695.15  Loss: 0.000713\n",
      "PARAM=700  Epoch [30000/50000]  MSE: 0.000007  L1: 682.20  Loss: 0.000689\n",
      "PARAM=700  Epoch [30500/50000]  MSE: 0.000003  L1: 685.41  Loss: 0.000688\n",
      "PARAM=700  Epoch [31000/50000]  MSE: 0.000002  L1: 693.30  Loss: 0.000696\n",
      "PARAM=700  Epoch [31500/50000]  MSE: 0.000003  L1: 692.09  Loss: 0.000695\n",
      "PARAM=700  Epoch [32000/50000]  MSE: 0.000005  L1: 666.44  Loss: 0.000672\n",
      "PARAM=700  Epoch [32500/50000]  MSE: 0.000030  L1: 697.72  Loss: 0.000728\n",
      "PARAM=700  Epoch [33000/50000]  MSE: 0.000004  L1: 672.74  Loss: 0.000677\n",
      "PARAM=700  Epoch [33500/50000]  MSE: 0.000002  L1: 687.39  Loss: 0.000689\n",
      "PARAM=700  Epoch [34000/50000]  MSE: 0.000005  L1: 695.17  Loss: 0.000700\n",
      "PARAM=700  Epoch [34500/50000]  MSE: 0.000044  L1: 677.13  Loss: 0.000721\n",
      "PARAM=700  Epoch [35000/50000]  MSE: 0.000003  L1: 684.80  Loss: 0.000688\n",
      "PARAM=700  Epoch [35500/50000]  MSE: 0.000002  L1: 687.64  Loss: 0.000690\n",
      "PARAM=700  Epoch [36000/50000]  MSE: 0.000009  L1: 686.76  Loss: 0.000696\n",
      "PARAM=700  Epoch [36500/50000]  MSE: 0.000008  L1: 672.94  Loss: 0.000681\n",
      "PARAM=700  Epoch [37000/50000]  MSE: 0.000013  L1: 692.32  Loss: 0.000705\n",
      "PARAM=700  Epoch [37500/50000]  MSE: 0.000010  L1: 681.38  Loss: 0.000691\n",
      "PARAM=700  Epoch [38000/50000]  MSE: 0.000006  L1: 673.09  Loss: 0.000679\n",
      "PARAM=700  Epoch [38500/50000]  MSE: 0.000009  L1: 708.08  Loss: 0.000717\n",
      "PARAM=700  Epoch [39000/50000]  MSE: 0.000004  L1: 669.64  Loss: 0.000674\n",
      "PARAM=700  Epoch [39500/50000]  MSE: 0.000003  L1: 685.27  Loss: 0.000688\n",
      "PARAM=700  Epoch [40000/50000]  MSE: 0.000006  L1: 687.39  Loss: 0.000693\n",
      "PARAM=700  Epoch [40500/50000]  MSE: 0.000002  L1: 681.14  Loss: 0.000683\n",
      "PARAM=700  Epoch [41000/50000]  MSE: 0.000001  L1: 677.57  Loss: 0.000679\n",
      "PARAM=700  Epoch [41500/50000]  MSE: 0.000001  L1: 691.51  Loss: 0.000693\n",
      "PARAM=700  Epoch [42000/50000]  MSE: 0.000002  L1: 685.17  Loss: 0.000687\n",
      "PARAM=700  Epoch [42500/50000]  MSE: 0.000007  L1: 674.70  Loss: 0.000682\n",
      "PARAM=700  Epoch [43000/50000]  MSE: 0.000003  L1: 697.88  Loss: 0.000701\n",
      "PARAM=700  Epoch [43500/50000]  MSE: 0.000008  L1: 669.03  Loss: 0.000677\n",
      "PARAM=700  Epoch [44000/50000]  MSE: 0.000004  L1: 677.61  Loss: 0.000681\n",
      "PARAM=700  Epoch [44500/50000]  MSE: 0.000012  L1: 700.93  Loss: 0.000713\n",
      "PARAM=700  Epoch [45000/50000]  MSE: 0.000004  L1: 676.78  Loss: 0.000680\n",
      "PARAM=700  Epoch [45500/50000]  MSE: 0.000013  L1: 686.25  Loss: 0.000699\n",
      "PARAM=700  Epoch [46000/50000]  MSE: 0.000010  L1: 680.45  Loss: 0.000691\n",
      "PARAM=700  Epoch [46500/50000]  MSE: 0.000002  L1: 683.80  Loss: 0.000686\n",
      "PARAM=700  Epoch [47000/50000]  MSE: 0.000044  L1: 667.33  Loss: 0.000711\n",
      "PARAM=700  Epoch [47500/50000]  MSE: 0.000006  L1: 699.04  Loss: 0.000705\n",
      "PARAM=700  Epoch [48000/50000]  MSE: 0.000007  L1: 678.51  Loss: 0.000685\n",
      "PARAM=700  Epoch [48500/50000]  MSE: 0.000001  L1: 677.82  Loss: 0.000679\n",
      "PARAM=700  Epoch [49000/50000]  MSE: 0.000008  L1: 699.87  Loss: 0.000708\n",
      "PARAM=700  Epoch [49500/50000]  MSE: 0.000001  L1: 668.72  Loss: 0.000670\n",
      "\n",
      "PARAM=700 予測結果：\n",
      "[[0.92883843 0.11766657 0.14094217]\n",
      " [0.4109606  0.74115914 0.27123082]\n",
      " [0.22302374 0.3214334  0.64135253]\n",
      " [0.96372634 0.9215365  0.07788225]\n",
      " [0.43365252 0.79991174 0.86643976]\n",
      " [0.72230655 0.31814402 0.6189021 ]\n",
      " [0.00401942 0.00412861 0.00425675]\n",
      " [0.5028163  0.4994711  0.49831754]\n",
      " [0.9967132  1.0008363  0.9984502 ]\n",
      " [0.45399275 0.30966657 0.2577598 ]\n",
      " [0.76349837 0.561459   0.49235383]\n",
      " [0.35476238 0.47419077 0.61233217]\n",
      " [0.35613537 0.4236959  0.25018495]\n",
      " [0.516221   0.49810258 0.6863127 ]\n",
      " [0.37531638 0.74092686 0.6783589 ]\n",
      " [0.8692264  0.47909403 0.18679099]\n",
      " [0.26758283 0.35969853 0.6551865 ]\n",
      " [0.7771805  0.3119224  0.37677974]\n",
      " [0.36397395 0.23140536 0.41023946]\n",
      " [0.6141917  0.7377176  0.23021953]\n",
      " [0.8935808  0.63006604 0.1566961 ]\n",
      " [0.15083592 0.24852347 0.56345946]\n",
      " [0.23436233 0.58025277 0.2735072 ]\n",
      " [0.7019607  0.21656743 0.22616863]\n",
      " [0.9163327  0.776178   0.10952149]\n",
      " [0.7508655  0.3094945  0.57165664]\n",
      " [0.00300055 0.52208793 0.6465579 ]\n",
      " [0.9452348  0.9420246  0.91753405]\n",
      " [0.787942   0.79066175 0.78786635]\n",
      " [0.63039446 0.6335546  0.635916  ]\n",
      " [0.47733253 0.47835678 0.47688377]\n",
      " [0.32917693 0.33268777 0.3378578 ]\n",
      " [0.19542539 0.19526191 0.19495378]]\n",
      "C++ 用のパラメータファイル (nomal_model_parameters_700.h) を作成しました。\n",
      "\n",
      "PARAM=1000  Epoch [0/50000]  MSE: 0.362182  L1: 16716.17  Loss: 0.378898\n",
      "PARAM=1000  Epoch [500/50000]  MSE: 0.000499  L1: 17539.02  Loss: 0.018038\n",
      "PARAM=1000  Epoch [1000/50000]  MSE: 0.000126  L1: 12644.64  Loss: 0.012770\n",
      "PARAM=1000  Epoch [1500/50000]  MSE: 0.000068  L1: 10046.10  Loss: 0.010114\n",
      "PARAM=1000  Epoch [2000/50000]  MSE: 0.000055  L1: 8366.70  Loss: 0.008421\n",
      "PARAM=1000  Epoch [2500/50000]  MSE: 0.000038  L1: 7124.62  Loss: 0.007162\n",
      "PARAM=1000  Epoch [3000/50000]  MSE: 0.000148  L1: 5973.36  Loss: 0.006122\n",
      "PARAM=1000  Epoch [3500/50000]  MSE: 0.000016  L1: 5010.33  Loss: 0.005026\n",
      "PARAM=1000  Epoch [4000/50000]  MSE: 0.000114  L1: 4170.02  Loss: 0.004284\n",
      "PARAM=1000  Epoch [4500/50000]  MSE: 0.000019  L1: 3475.50  Loss: 0.003495\n",
      "PARAM=1000  Epoch [5000/50000]  MSE: 0.000029  L1: 2791.89  Loss: 0.002821\n",
      "PARAM=1000  Epoch [5500/50000]  MSE: 0.000140  L1: 2271.20  Loss: 0.002411\n",
      "PARAM=1000  Epoch [6000/50000]  MSE: 0.000083  L1: 1864.27  Loss: 0.001947\n",
      "PARAM=1000  Epoch [6500/50000]  MSE: 0.000017  L1: 1624.68  Loss: 0.001642\n",
      "PARAM=1000  Epoch [7000/50000]  MSE: 0.000310  L1: 1544.09  Loss: 0.001854\n",
      "PARAM=1000  Epoch [7500/50000]  MSE: 0.000186  L1: 1433.82  Loss: 0.001620\n",
      "PARAM=1000  Epoch [8000/50000]  MSE: 0.000020  L1: 1390.56  Loss: 0.001410\n",
      "PARAM=1000  Epoch [8500/50000]  MSE: 0.000071  L1: 1418.51  Loss: 0.001489\n",
      "PARAM=1000  Epoch [9000/50000]  MSE: 0.000057  L1: 1367.09  Loss: 0.001424\n",
      "PARAM=1000  Epoch [9500/50000]  MSE: 0.000042  L1: 1338.74  Loss: 0.001381\n",
      "PARAM=1000  Epoch [10000/50000]  MSE: 0.000211  L1: 1351.06  Loss: 0.001563\n",
      "PARAM=1000  Epoch [10500/50000]  MSE: 0.000018  L1: 1299.56  Loss: 0.001318\n",
      "PARAM=1000  Epoch [11000/50000]  MSE: 0.000114  L1: 1268.95  Loss: 0.001383\n",
      "PARAM=1000  Epoch [11500/50000]  MSE: 0.000054  L1: 1299.81  Loss: 0.001354\n",
      "PARAM=1000  Epoch [12000/50000]  MSE: 0.000008  L1: 1228.27  Loss: 0.001237\n",
      "PARAM=1000  Epoch [12500/50000]  MSE: 0.000018  L1: 1238.38  Loss: 0.001256\n",
      "PARAM=1000  Epoch [13000/50000]  MSE: 0.000006  L1: 1272.34  Loss: 0.001278\n",
      "PARAM=1000  Epoch [13500/50000]  MSE: 0.000014  L1: 1248.15  Loss: 0.001262\n",
      "PARAM=1000  Epoch [14000/50000]  MSE: 0.000154  L1: 1239.44  Loss: 0.001393\n",
      "PARAM=1000  Epoch [14500/50000]  MSE: 0.000058  L1: 1299.68  Loss: 0.001358\n",
      "PARAM=1000  Epoch [15000/50000]  MSE: 0.000008  L1: 1270.95  Loss: 0.001279\n",
      "PARAM=1000  Epoch [15500/50000]  MSE: 0.000011  L1: 1272.75  Loss: 0.001284\n",
      "PARAM=1000  Epoch [16000/50000]  MSE: 0.000007  L1: 1332.50  Loss: 0.001340\n",
      "PARAM=1000  Epoch [16500/50000]  MSE: 0.000014  L1: 1305.06  Loss: 0.001320\n",
      "PARAM=1000  Epoch [17000/50000]  MSE: 0.000017  L1: 1303.80  Loss: 0.001321\n",
      "PARAM=1000  Epoch [17500/50000]  MSE: 0.000010  L1: 1377.66  Loss: 0.001388\n",
      "PARAM=1000  Epoch [18000/50000]  MSE: 0.000007  L1: 1340.21  Loss: 0.001347\n",
      "PARAM=1000  Epoch [18500/50000]  MSE: 0.000008  L1: 1331.12  Loss: 0.001340\n",
      "PARAM=1000  Epoch [19000/50000]  MSE: 0.000272  L1: 1357.93  Loss: 0.001630\n",
      "PARAM=1000  Epoch [19500/50000]  MSE: 0.000021  L1: 1339.96  Loss: 0.001361\n",
      "PARAM=1000  Epoch [20000/50000]  MSE: 0.000025  L1: 1340.28  Loss: 0.001365\n",
      "PARAM=1000  Epoch [20500/50000]  MSE: 0.000068  L1: 1393.70  Loss: 0.001462\n",
      "PARAM=1000  Epoch [21000/50000]  MSE: 0.000033  L1: 1349.79  Loss: 0.001383\n",
      "PARAM=1000  Epoch [21500/50000]  MSE: 0.000027  L1: 1339.40  Loss: 0.001367\n",
      "PARAM=1000  Epoch [22000/50000]  MSE: 0.000016  L1: 1375.35  Loss: 0.001391\n",
      "PARAM=1000  Epoch [22500/50000]  MSE: 0.000006  L1: 1329.46  Loss: 0.001336\n",
      "PARAM=1000  Epoch [23000/50000]  MSE: 0.000073  L1: 1325.26  Loss: 0.001399\n",
      "PARAM=1000  Epoch [23500/50000]  MSE: 0.000010  L1: 1373.28  Loss: 0.001383\n",
      "PARAM=1000  Epoch [24000/50000]  MSE: 0.000006  L1: 1333.69  Loss: 0.001340\n",
      "PARAM=1000  Epoch [24500/50000]  MSE: 0.000006  L1: 1348.02  Loss: 0.001354\n",
      "PARAM=1000  Epoch [25000/50000]  MSE: 0.000010  L1: 1367.98  Loss: 0.001378\n",
      "PARAM=1000  Epoch [25500/50000]  MSE: 0.000011  L1: 1335.51  Loss: 0.001347\n",
      "PARAM=1000  Epoch [26000/50000]  MSE: 0.000006  L1: 1306.90  Loss: 0.001313\n",
      "PARAM=1000  Epoch [26500/50000]  MSE: 0.000009  L1: 1378.03  Loss: 0.001387\n",
      "PARAM=1000  Epoch [27000/50000]  MSE: 0.000005  L1: 1331.41  Loss: 0.001336\n",
      "PARAM=1000  Epoch [27500/50000]  MSE: 0.000168  L1: 1326.56  Loss: 0.001495\n",
      "PARAM=1000  Epoch [28000/50000]  MSE: 0.000005  L1: 1368.49  Loss: 0.001373\n",
      "PARAM=1000  Epoch [28500/50000]  MSE: 0.000028  L1: 1321.44  Loss: 0.001349\n",
      "PARAM=1000  Epoch [29000/50000]  MSE: 0.000006  L1: 1325.21  Loss: 0.001331\n",
      "PARAM=1000  Epoch [29500/50000]  MSE: 0.000010  L1: 1365.85  Loss: 0.001376\n",
      "PARAM=1000  Epoch [30000/50000]  MSE: 0.000033  L1: 1322.54  Loss: 0.001356\n",
      "PARAM=1000  Epoch [30500/50000]  MSE: 0.000007  L1: 1325.92  Loss: 0.001333\n",
      "PARAM=1000  Epoch [31000/50000]  MSE: 0.000012  L1: 1365.26  Loss: 0.001377\n",
      "PARAM=1000  Epoch [31500/50000]  MSE: 0.000015  L1: 1340.54  Loss: 0.001355\n",
      "PARAM=1000  Epoch [32000/50000]  MSE: 0.000005  L1: 1308.84  Loss: 0.001314\n",
      "PARAM=1000  Epoch [32500/50000]  MSE: 0.000006  L1: 1354.13  Loss: 0.001360\n",
      "PARAM=1000  Epoch [33000/50000]  MSE: 0.000006  L1: 1310.35  Loss: 0.001317\n",
      "PARAM=1000  Epoch [33500/50000]  MSE: 0.000005  L1: 1325.19  Loss: 0.001330\n",
      "PARAM=1000  Epoch [34000/50000]  MSE: 0.000012  L1: 1355.36  Loss: 0.001367\n",
      "PARAM=1000  Epoch [34500/50000]  MSE: 0.000017  L1: 1323.59  Loss: 0.001341\n",
      "PARAM=1000  Epoch [35000/50000]  MSE: 0.000005  L1: 1318.78  Loss: 0.001323\n",
      "PARAM=1000  Epoch [35500/50000]  MSE: 0.000002  L1: 1367.03  Loss: 0.001369\n",
      "PARAM=1000  Epoch [36000/50000]  MSE: 0.000007  L1: 1325.80  Loss: 0.001332\n",
      "PARAM=1000  Epoch [36500/50000]  MSE: 0.000011  L1: 1301.18  Loss: 0.001312\n",
      "PARAM=1000  Epoch [37000/50000]  MSE: 0.000013  L1: 1341.80  Loss: 0.001355\n",
      "PARAM=1000  Epoch [37500/50000]  MSE: 0.000010  L1: 1317.20  Loss: 0.001327\n",
      "PARAM=1000  Epoch [38000/50000]  MSE: 0.000007  L1: 1308.60  Loss: 0.001315\n",
      "PARAM=1000  Epoch [38500/50000]  MSE: 0.000003  L1: 1375.83  Loss: 0.001379\n",
      "PARAM=1000  Epoch [39000/50000]  MSE: 0.000002  L1: 1316.87  Loss: 0.001319\n",
      "PARAM=1000  Epoch [39500/50000]  MSE: 0.000006  L1: 1311.72  Loss: 0.001318\n",
      "PARAM=1000  Epoch [40000/50000]  MSE: 0.000002  L1: 1346.72  Loss: 0.001349\n",
      "PARAM=1000  Epoch [40500/50000]  MSE: 0.000017  L1: 1316.95  Loss: 0.001334\n",
      "PARAM=1000  Epoch [41000/50000]  MSE: 0.000018  L1: 1307.78  Loss: 0.001325\n",
      "PARAM=1000  Epoch [41500/50000]  MSE: 0.000006  L1: 1353.97  Loss: 0.001360\n",
      "PARAM=1000  Epoch [42000/50000]  MSE: 0.000034  L1: 1321.27  Loss: 0.001355\n",
      "PARAM=1000  Epoch [42500/50000]  MSE: 0.000010  L1: 1322.61  Loss: 0.001333\n",
      "PARAM=1000  Epoch [43000/50000]  MSE: 0.000006  L1: 1350.91  Loss: 0.001357\n",
      "PARAM=1000  Epoch [43500/50000]  MSE: 0.000020  L1: 1297.08  Loss: 0.001317\n",
      "PARAM=1000  Epoch [44000/50000]  MSE: 0.000037  L1: 1302.24  Loss: 0.001340\n",
      "PARAM=1000  Epoch [44500/50000]  MSE: 0.000016  L1: 1362.36  Loss: 0.001378\n",
      "PARAM=1000  Epoch [45000/50000]  MSE: 0.000007  L1: 1318.08  Loss: 0.001325\n",
      "PARAM=1000  Epoch [45500/50000]  MSE: 0.000003  L1: 1315.14  Loss: 0.001318\n",
      "PARAM=1000  Epoch [46000/50000]  MSE: 0.000104  L1: 1341.34  Loss: 0.001445\n",
      "PARAM=1000  Epoch [46500/50000]  MSE: 0.000017  L1: 1317.40  Loss: 0.001334\n",
      "PARAM=1000  Epoch [47000/50000]  MSE: 0.000006  L1: 1292.54  Loss: 0.001299\n",
      "PARAM=1000  Epoch [47500/50000]  MSE: 0.000010  L1: 1360.51  Loss: 0.001370\n",
      "PARAM=1000  Epoch [48000/50000]  MSE: 0.000008  L1: 1309.45  Loss: 0.001317\n",
      "PARAM=1000  Epoch [48500/50000]  MSE: 0.000001  L1: 1311.46  Loss: 0.001313\n",
      "PARAM=1000  Epoch [49000/50000]  MSE: 0.000003  L1: 1355.17  Loss: 0.001359\n",
      "PARAM=1000  Epoch [49500/50000]  MSE: 0.000001  L1: 1317.08  Loss: 0.001318\n",
      "\n",
      "PARAM=1000 予測結果：\n",
      "[[9.30502415e-01 1.18137725e-01 1.41997054e-01]\n",
      " [4.15543675e-01 7.37089336e-01 2.74646610e-01]\n",
      " [2.25427121e-01 3.22228670e-01 6.43781841e-01]\n",
      " [9.71596956e-01 9.13586736e-01 8.06412250e-02]\n",
      " [4.39680636e-01 7.96471179e-01 8.72389734e-01]\n",
      " [7.25536108e-01 3.17736030e-01 6.21999145e-01]\n",
      " [5.08730672e-03 4.61384654e-04 5.11492323e-03]\n",
      " [5.07232606e-01 4.98081625e-01 5.00203252e-01]\n",
      " [1.00878811e+00 9.91781473e-01 1.00771940e+00]\n",
      " [4.57052171e-01 3.08907717e-01 2.64500737e-01]\n",
      " [7.67262280e-01 5.61099410e-01 4.97938544e-01]\n",
      " [3.59007895e-01 4.71814454e-01 6.15826547e-01]\n",
      " [3.59172374e-01 4.21723485e-01 2.54720300e-01]\n",
      " [5.21855295e-01 4.98072147e-01 6.90108597e-01]\n",
      " [3.79211783e-01 7.37075627e-01 6.84022665e-01]\n",
      " [8.71042013e-01 4.79439318e-01 1.90799892e-01]\n",
      " [2.76377439e-01 3.53669822e-01 6.57062829e-01]\n",
      " [7.80918837e-01 3.12090605e-01 3.78955215e-01]\n",
      " [3.69643539e-01 2.32078642e-01 4.14840698e-01]\n",
      " [6.19794548e-01 7.34962761e-01 2.34682471e-01]\n",
      " [8.97832334e-01 6.29101753e-01 1.63762882e-01]\n",
      " [1.58820152e-01 2.41605669e-01 5.70527613e-01]\n",
      " [2.38793731e-01 5.76150179e-01 2.76780397e-01]\n",
      " [7.06053555e-01 2.13108838e-01 2.29755834e-01]\n",
      " [9.21510339e-01 7.72577167e-01 1.15362085e-01]\n",
      " [7.55096972e-01 3.11247796e-01 5.75335443e-01]\n",
      " [1.10409893e-02 5.18935502e-01 6.50275290e-01]\n",
      " [9.54500854e-01 9.40524459e-01 9.27605748e-01]\n",
      " [7.93696940e-01 7.88494587e-01 7.92488337e-01]\n",
      " [6.35331273e-01 6.33542299e-01 6.37977958e-01]\n",
      " [4.80479360e-01 4.72583413e-01 4.82691258e-01]\n",
      " [3.32062215e-01 3.33351225e-01 3.41350704e-01]\n",
      " [2.00533003e-01 1.93431795e-01 1.97575793e-01]]\n",
      "C++ 用のパラメータファイル (nomal_model_parameters_1000.h) を作成しました。\n",
      "\n",
      "PARAM=1100  Epoch [0/50000]  MSE: 0.339443  L1: 19236.09  Loss: 0.358679\n",
      "PARAM=1100  Epoch [500/50000]  MSE: 0.000706  L1: 21861.70  Loss: 0.022567\n",
      "PARAM=1100  Epoch [1000/50000]  MSE: 0.000180  L1: 15875.77  Loss: 0.016055\n",
      "PARAM=1100  Epoch [1500/50000]  MSE: 0.000212  L1: 12731.97  Loss: 0.012943\n",
      "PARAM=1100  Epoch [2000/50000]  MSE: 0.000863  L1: 10644.00  Loss: 0.011507\n",
      "PARAM=1100  Epoch [2500/50000]  MSE: 0.000079  L1: 9050.45  Loss: 0.009129\n",
      "PARAM=1100  Epoch [3000/50000]  MSE: 0.000732  L1: 7603.78  Loss: 0.008336\n",
      "PARAM=1100  Epoch [3500/50000]  MSE: 0.000064  L1: 6428.83  Loss: 0.006492\n",
      "PARAM=1100  Epoch [4000/50000]  MSE: 0.000773  L1: 6367.08  Loss: 0.007141\n",
      "PARAM=1100  Epoch [4500/50000]  MSE: 0.000059  L1: 4891.76  Loss: 0.004951\n",
      "PARAM=1100  Epoch [5000/50000]  MSE: 0.001108  L1: 4422.82  Loss: 0.005530\n",
      "PARAM=1100  Epoch [5500/50000]  MSE: 0.000566  L1: 3554.58  Loss: 0.004121\n",
      "PARAM=1100  Epoch [6000/50000]  MSE: 0.017702  L1: 3066.27  Loss: 0.020769\n",
      "PARAM=1100  Epoch [6500/50000]  MSE: 0.001797  L1: 2546.03  Loss: 0.004343\n",
      "PARAM=1100  Epoch [7000/50000]  MSE: 0.001698  L1: 2252.08  Loss: 0.003950\n",
      "PARAM=1100  Epoch [7500/50000]  MSE: 0.001689  L1: 1975.65  Loss: 0.003665\n",
      "PARAM=1100  Epoch [8000/50000]  MSE: 0.001666  L1: 1830.10  Loss: 0.003496\n",
      "PARAM=1100  Epoch [8500/50000]  MSE: 0.000470  L1: 1862.01  Loss: 0.002332\n",
      "PARAM=1100  Epoch [9000/50000]  MSE: 0.000413  L1: 1734.41  Loss: 0.002148\n",
      "PARAM=1100  Epoch [9500/50000]  MSE: 0.000174  L1: 1693.11  Loss: 0.001867\n",
      "PARAM=1100  Epoch [10000/50000]  MSE: 0.000109  L1: 1674.61  Loss: 0.001783\n",
      "PARAM=1100  Epoch [10500/50000]  MSE: 0.000084  L1: 1584.40  Loss: 0.001668\n",
      "PARAM=1100  Epoch [11000/50000]  MSE: 0.000059  L1: 1528.43  Loss: 0.001587\n",
      "PARAM=1100  Epoch [11500/50000]  MSE: 0.000042  L1: 1553.61  Loss: 0.001595\n",
      "PARAM=1100  Epoch [12000/50000]  MSE: 0.000039  L1: 1447.23  Loss: 0.001486\n",
      "PARAM=1100  Epoch [12500/50000]  MSE: 0.000022  L1: 1444.00  Loss: 0.001466\n",
      "PARAM=1100  Epoch [13000/50000]  MSE: 0.000036  L1: 1476.79  Loss: 0.001513\n",
      "PARAM=1100  Epoch [13500/50000]  MSE: 0.000016  L1: 1440.02  Loss: 0.001456\n",
      "PARAM=1100  Epoch [14000/50000]  MSE: 0.000020  L1: 1435.51  Loss: 0.001456\n",
      "PARAM=1100  Epoch [14500/50000]  MSE: 0.000009  L1: 1509.37  Loss: 0.001518\n",
      "PARAM=1100  Epoch [15000/50000]  MSE: 0.000021  L1: 1473.01  Loss: 0.001494\n",
      "PARAM=1100  Epoch [15500/50000]  MSE: 0.000006  L1: 1473.34  Loss: 0.001479\n",
      "PARAM=1100  Epoch [16000/50000]  MSE: 0.000019  L1: 1556.14  Loss: 0.001575\n",
      "PARAM=1100  Epoch [16500/50000]  MSE: 0.000005  L1: 1517.71  Loss: 0.001522\n",
      "PARAM=1100  Epoch [17000/50000]  MSE: 0.000003  L1: 1525.07  Loss: 0.001528\n",
      "PARAM=1100  Epoch [17500/50000]  MSE: 0.000010  L1: 1621.14  Loss: 0.001631\n",
      "PARAM=1100  Epoch [18000/50000]  MSE: 0.000004  L1: 1572.47  Loss: 0.001576\n",
      "PARAM=1100  Epoch [18500/50000]  MSE: 0.000019  L1: 1565.39  Loss: 0.001584\n",
      "PARAM=1100  Epoch [19000/50000]  MSE: 0.000039  L1: 1606.64  Loss: 0.001645\n",
      "PARAM=1100  Epoch [19500/50000]  MSE: 0.000006  L1: 1584.98  Loss: 0.001591\n",
      "PARAM=1100  Epoch [20000/50000]  MSE: 0.000002  L1: 1582.36  Loss: 0.001585\n",
      "PARAM=1100  Epoch [20500/50000]  MSE: 0.000003  L1: 1659.77  Loss: 0.001663\n",
      "PARAM=1100  Epoch [21000/50000]  MSE: 0.000004  L1: 1603.02  Loss: 0.001607\n",
      "PARAM=1100  Epoch [21500/50000]  MSE: 0.000003  L1: 1588.28  Loss: 0.001591\n",
      "PARAM=1100  Epoch [22000/50000]  MSE: 0.000007  L1: 1639.43  Loss: 0.001647\n",
      "PARAM=1100  Epoch [22500/50000]  MSE: 0.000013  L1: 1582.92  Loss: 0.001596\n",
      "PARAM=1100  Epoch [23000/50000]  MSE: 0.000003  L1: 1581.67  Loss: 0.001585\n",
      "PARAM=1100  Epoch [23500/50000]  MSE: 0.000017  L1: 1639.02  Loss: 0.001656\n",
      "PARAM=1100  Epoch [24000/50000]  MSE: 0.000003  L1: 1593.01  Loss: 0.001596\n",
      "PARAM=1100  Epoch [24500/50000]  MSE: 0.000005  L1: 1603.95  Loss: 0.001609\n",
      "PARAM=1100  Epoch [25000/50000]  MSE: 0.000002  L1: 1638.72  Loss: 0.001640\n",
      "PARAM=1100  Epoch [25500/50000]  MSE: 0.000030  L1: 1598.11  Loss: 0.001628\n",
      "PARAM=1100  Epoch [26000/50000]  MSE: 0.000009  L1: 1567.84  Loss: 0.001577\n",
      "PARAM=1100  Epoch [26500/50000]  MSE: 0.000007  L1: 1653.91  Loss: 0.001661\n",
      "PARAM=1100  Epoch [27000/50000]  MSE: 0.000019  L1: 1585.10  Loss: 0.001605\n",
      "PARAM=1100  Epoch [27500/50000]  MSE: 0.000040  L1: 1579.83  Loss: 0.001620\n",
      "PARAM=1100  Epoch [28000/50000]  MSE: 0.000003  L1: 1637.36  Loss: 0.001640\n",
      "PARAM=1100  Epoch [28500/50000]  MSE: 0.000008  L1: 1581.22  Loss: 0.001589\n",
      "PARAM=1100  Epoch [29000/50000]  MSE: 0.000005  L1: 1586.02  Loss: 0.001591\n",
      "PARAM=1100  Epoch [29500/50000]  MSE: 0.000024  L1: 1645.44  Loss: 0.001669\n",
      "PARAM=1100  Epoch [30000/50000]  MSE: 0.000002  L1: 1585.27  Loss: 0.001587\n",
      "PARAM=1100  Epoch [30500/50000]  MSE: 0.000002  L1: 1579.54  Loss: 0.001581\n",
      "PARAM=1100  Epoch [31000/50000]  MSE: 0.000005  L1: 1634.33  Loss: 0.001639\n",
      "PARAM=1100  Epoch [31500/50000]  MSE: 0.000002  L1: 1602.67  Loss: 0.001604\n",
      "PARAM=1100  Epoch [32000/50000]  MSE: 0.000001  L1: 1570.33  Loss: 0.001572\n",
      "PARAM=1100  Epoch [32500/50000]  MSE: 0.000005  L1: 1629.25  Loss: 0.001634\n",
      "PARAM=1100  Epoch [33000/50000]  MSE: 0.000007  L1: 1575.40  Loss: 0.001583\n",
      "PARAM=1100  Epoch [33500/50000]  MSE: 0.000002  L1: 1583.56  Loss: 0.001585\n",
      "PARAM=1100  Epoch [34000/50000]  MSE: 0.000015  L1: 1623.21  Loss: 0.001638\n",
      "PARAM=1100  Epoch [34500/50000]  MSE: 0.000006  L1: 1582.88  Loss: 0.001589\n",
      "PARAM=1100  Epoch [35000/50000]  MSE: 0.000001  L1: 1581.31  Loss: 0.001582\n",
      "PARAM=1100  Epoch [35500/50000]  MSE: 0.000025  L1: 1645.45  Loss: 0.001670\n",
      "PARAM=1100  Epoch [36000/50000]  MSE: 0.000009  L1: 1590.35  Loss: 0.001599\n",
      "PARAM=1100  Epoch [36500/50000]  MSE: 0.000007  L1: 1561.60  Loss: 0.001569\n",
      "PARAM=1100  Epoch [37000/50000]  MSE: 0.000001  L1: 1614.52  Loss: 0.001616\n",
      "PARAM=1100  Epoch [37500/50000]  MSE: 0.000003  L1: 1576.68  Loss: 0.001580\n",
      "PARAM=1100  Epoch [38000/50000]  MSE: 0.000002  L1: 1570.26  Loss: 0.001573\n",
      "PARAM=1100  Epoch [38500/50000]  MSE: 0.000003  L1: 1659.11  Loss: 0.001662\n",
      "PARAM=1100  Epoch [39000/50000]  MSE: 0.000000  L1: 1583.46  Loss: 0.001584\n",
      "PARAM=1100  Epoch [39500/50000]  MSE: 0.000003  L1: 1572.86  Loss: 0.001575\n",
      "PARAM=1100  Epoch [40000/50000]  MSE: 0.000001  L1: 1622.85  Loss: 0.001624\n",
      "PARAM=1100  Epoch [40500/50000]  MSE: 0.000032  L1: 1579.42  Loss: 0.001612\n",
      "PARAM=1100  Epoch [41000/50000]  MSE: 0.000008  L1: 1567.96  Loss: 0.001576\n",
      "PARAM=1100  Epoch [41500/50000]  MSE: 0.000042  L1: 1631.72  Loss: 0.001674\n",
      "PARAM=1100  Epoch [42000/50000]  MSE: 0.000017  L1: 1588.88  Loss: 0.001606\n",
      "PARAM=1100  Epoch [42500/50000]  MSE: 0.000003  L1: 1584.41  Loss: 0.001588\n",
      "PARAM=1100  Epoch [43000/50000]  MSE: 0.000008  L1: 1627.82  Loss: 0.001636\n",
      "PARAM=1100  Epoch [43500/50000]  MSE: 0.000002  L1: 1561.95  Loss: 0.001564\n",
      "PARAM=1100  Epoch [44000/50000]  MSE: 0.000003  L1: 1569.44  Loss: 0.001572\n",
      "PARAM=1100  Epoch [44500/50000]  MSE: 0.000001  L1: 1640.77  Loss: 0.001641\n",
      "PARAM=1100  Epoch [45000/50000]  MSE: 0.000004  L1: 1583.08  Loss: 0.001587\n",
      "PARAM=1100  Epoch [45500/50000]  MSE: 0.000028  L1: 1577.53  Loss: 0.001605\n",
      "PARAM=1100  Epoch [46000/50000]  MSE: 0.000001  L1: 1615.30  Loss: 0.001616\n",
      "PARAM=1100  Epoch [46500/50000]  MSE: 0.000011  L1: 1584.54  Loss: 0.001596\n",
      "PARAM=1100  Epoch [47000/50000]  MSE: 0.000021  L1: 1558.50  Loss: 0.001580\n",
      "PARAM=1100  Epoch [47500/50000]  MSE: 0.000003  L1: 1644.71  Loss: 0.001647\n",
      "PARAM=1100  Epoch [48000/50000]  MSE: 0.000003  L1: 1574.01  Loss: 0.001577\n",
      "PARAM=1100  Epoch [48500/50000]  MSE: 0.000001  L1: 1573.31  Loss: 0.001574\n",
      "PARAM=1100  Epoch [49000/50000]  MSE: 0.000002  L1: 1632.24  Loss: 0.001634\n",
      "PARAM=1100  Epoch [49500/50000]  MSE: 0.000001  L1: 1582.55  Loss: 0.001583\n",
      "\n",
      "PARAM=1100 予測結果：\n",
      "[[0.9317237  0.1192735  0.14098436]\n",
      " [0.4141267  0.7408851  0.27068985]\n",
      " [0.22176556 0.32757294 0.64257956]\n",
      " [0.97015214 0.92058986 0.07876594]\n",
      " [0.43864775 0.79974234 0.866675  ]\n",
      " [0.723849   0.3183513  0.6202391 ]\n",
      " [0.00403088 0.00435749 0.00380112]\n",
      " [0.5045447  0.4994681  0.49883956]\n",
      " [1.0034931  0.9977707  1.0003792 ]\n",
      " [0.45854065 0.31035912 0.25895646]\n",
      " [0.7670354  0.561675   0.49451536]\n",
      " [0.35929674 0.47537878 0.6116984 ]\n",
      " [0.3587409  0.42413363 0.2512732 ]\n",
      " [0.5193435  0.49908343 0.6871424 ]\n",
      " [0.37920028 0.741078   0.6791597 ]\n",
      " [0.8728118  0.47878528 0.18854782]\n",
      " [0.2689357  0.35714966 0.6551106 ]\n",
      " [0.7795367  0.31486183 0.37819976]\n",
      " [0.3680611  0.23259094 0.41169298]\n",
      " [0.6180738  0.7383274  0.23054335]\n",
      " [0.8970486  0.63107574 0.15686919]\n",
      " [0.15433785 0.24789143 0.56435186]\n",
      " [0.23838726 0.58082026 0.27463442]\n",
      " [0.70504034 0.21810088 0.2285988 ]\n",
      " [0.92166674 0.7755648  0.1106779 ]\n",
      " [0.75505716 0.31062046 0.57310766]\n",
      " [0.00566478 0.5217216  0.64689636]\n",
      " [0.94899267 0.9459404  0.9195037 ]\n",
      " [0.7908434  0.79173404 0.78854436]\n",
      " [0.63282555 0.6347839  0.63546836]\n",
      " [0.47892448 0.4783277  0.47829953]\n",
      " [0.3332244  0.33436888 0.33788717]\n",
      " [0.19689807 0.19707319 0.19490579]]\n",
      "C++ 用のパラメータファイル (nomal_model_parameters_1100.h) を作成しました。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# X, Y はすでに定義済みとします\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "PARAMETER_list = [30,40,50,60,70,80,90,100,200,500,700,1000,1100]\n",
    "\n",
    "# 保存用ディレクトリ\n",
    "os.makedirs(\"h_faile_data\", exist_ok=True)\n",
    "\n",
    "for PARAMETER in PARAMETER_list:\n",
    "\n",
    "    # モデル定義\n",
    "    class ColorNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ColorNet, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(3, PARAMETER),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(PARAMETER, PARAMETER),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(PARAMETER, 3),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "    model = ColorNet()\n",
    "\n",
    "    # 損失関数（MSE）\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    lambda_l1 = 1e-6\n",
    "\n",
    "    # 学習ループ\n",
    "    epochs = 50000\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_tensor)\n",
    "        mse = criterion(outputs, Y_tensor)\n",
    "\n",
    "        # L1正則化\n",
    "        l1 = torch.tensor(0.0, requires_grad=False)\n",
    "        for name, p in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                l1 = l1 + p.abs().sum()\n",
    "        loss = mse + lambda_l1 * l1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f'PARAM={PARAMETER}  Epoch [{epoch}/{epochs}]  MSE: {mse.item():.6f}  L1: {l1.item():.2f}  Loss: {loss.item():.6f}')\n",
    "\n",
    "    # 予測結果\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_tensor)\n",
    "        print(f\"\\nPARAM={PARAMETER} 予測結果：\")\n",
    "        print(predictions.numpy())\n",
    "\n",
    "    # C++用に変換\n",
    "    def convert_to_cpp_array(tensor: torch.Tensor, name: str, dtype: str = \"float\"):\n",
    "        flat = tensor.detach().numpy().flatten()\n",
    "        array_str = f\"{dtype} {name}[] = {{\"\n",
    "        array_str += \", \".join(map(str, flat))\n",
    "        array_str += \"};\"\n",
    "        return array_str\n",
    "\n",
    "    cpp_code = \"\"\n",
    "    layer_idx = 1\n",
    "    for layer in model.model:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            cpp_code += convert_to_cpp_array(layer.weight, f\"weight_{layer_idx}\") + \"\\n\"\n",
    "            cpp_code += convert_to_cpp_array(layer.bias,   f\"bias_{layer_idx}\") + \"\\n\"\n",
    "            layer_idx += 1\n",
    "\n",
    "    #filename = f\"h_faile_data/nomal_model_parameters_{PARAMETER}.h\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(cpp_code)\n",
    "\n",
    "    print(f\"C++ 用のパラメータファイル (nomal_model_parameters_{PARAMETER}.h) を作成しました。\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48362203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50000]  MSE: 0.418853  L1: 300.60  Loss: 0.419154\n",
      "Epoch [500/50000]  MSE: 0.000592  L1: 235.06  Loss: 0.000827\n",
      "Epoch [1000/50000]  MSE: 0.000080  L1: 220.60  Loss: 0.000300\n",
      "Epoch [1500/50000]  MSE: 0.000088  L1: 199.71  Loss: 0.000288\n",
      "Epoch [2000/50000]  MSE: 0.000018  L1: 181.23  Loss: 0.000199\n",
      "Epoch [2500/50000]  MSE: 0.000035  L1: 166.33  Loss: 0.000201\n",
      "Epoch [3000/50000]  MSE: 0.000016  L1: 154.43  Loss: 0.000170\n",
      "Epoch [3500/50000]  MSE: 0.000030  L1: 143.56  Loss: 0.000174\n",
      "Epoch [4000/50000]  MSE: 0.000008  L1: 136.68  Loss: 0.000144\n",
      "Epoch [4500/50000]  MSE: 0.000086  L1: 127.94  Loss: 0.000214\n",
      "Epoch [5000/50000]  MSE: 0.000015  L1: 121.57  Loss: 0.000137\n",
      "Epoch [5500/50000]  MSE: 0.000005  L1: 116.37  Loss: 0.000122\n",
      "Epoch [6000/50000]  MSE: 0.000040  L1: 112.64  Loss: 0.000153\n",
      "Epoch [6500/50000]  MSE: 0.000082  L1: 108.61  Loss: 0.000191\n",
      "Epoch [7000/50000]  MSE: 0.000021  L1: 105.39  Loss: 0.000127\n",
      "Epoch [7500/50000]  MSE: 0.000031  L1: 102.12  Loss: 0.000133\n",
      "Epoch [8000/50000]  MSE: 0.000001  L1: 99.87  Loss: 0.000101\n",
      "Epoch [8500/50000]  MSE: 0.000009  L1: 97.84  Loss: 0.000107\n",
      "Epoch [9000/50000]  MSE: 0.000006  L1: 95.93  Loss: 0.000102\n",
      "Epoch [9500/50000]  MSE: 0.000009  L1: 93.91  Loss: 0.000103\n",
      "Epoch [10000/50000]  MSE: 0.000008  L1: 91.95  Loss: 0.000100\n",
      "Epoch [10500/50000]  MSE: 0.000005  L1: 90.12  Loss: 0.000095\n",
      "Epoch [11000/50000]  MSE: 0.000034  L1: 88.35  Loss: 0.000122\n",
      "Epoch [11500/50000]  MSE: 0.000012  L1: 87.23  Loss: 0.000099\n",
      "Epoch [12000/50000]  MSE: 0.000009  L1: 85.41  Loss: 0.000095\n",
      "Epoch [12500/50000]  MSE: 0.000009  L1: 84.51  Loss: 0.000094\n",
      "Epoch [13000/50000]  MSE: 0.000020  L1: 83.69  Loss: 0.000104\n",
      "Epoch [13500/50000]  MSE: 0.000057  L1: 82.52  Loss: 0.000139\n",
      "Epoch [14000/50000]  MSE: 0.000004  L1: 81.76  Loss: 0.000086\n",
      "Epoch [14500/50000]  MSE: 0.000055  L1: 81.68  Loss: 0.000136\n",
      "Epoch [15000/50000]  MSE: 0.000009  L1: 80.05  Loss: 0.000089\n",
      "Epoch [15500/50000]  MSE: 0.000013  L1: 79.10  Loss: 0.000092\n",
      "Epoch [16000/50000]  MSE: 0.000015  L1: 78.73  Loss: 0.000094\n",
      "Epoch [16500/50000]  MSE: 0.000002  L1: 78.68  Loss: 0.000081\n",
      "Epoch [17000/50000]  MSE: 0.000006  L1: 77.69  Loss: 0.000084\n",
      "Epoch [17500/50000]  MSE: 0.000018  L1: 76.96  Loss: 0.000095\n",
      "Epoch [18000/50000]  MSE: 0.000003  L1: 76.62  Loss: 0.000080\n",
      "Epoch [18500/50000]  MSE: 0.000016  L1: 75.98  Loss: 0.000092\n",
      "Epoch [19000/50000]  MSE: 0.000002  L1: 74.90  Loss: 0.000076\n",
      "Epoch [19500/50000]  MSE: 0.000004  L1: 74.45  Loss: 0.000078\n",
      "Epoch [20000/50000]  MSE: 0.000004  L1: 74.18  Loss: 0.000078\n",
      "Epoch [20500/50000]  MSE: 0.000017  L1: 73.59  Loss: 0.000090\n",
      "Epoch [21000/50000]  MSE: 0.000005  L1: 73.34  Loss: 0.000078\n",
      "Epoch [21500/50000]  MSE: 0.000012  L1: 72.66  Loss: 0.000085\n",
      "Epoch [22000/50000]  MSE: 0.000009  L1: 72.34  Loss: 0.000082\n",
      "Epoch [22500/50000]  MSE: 0.000016  L1: 72.33  Loss: 0.000088\n",
      "Epoch [23000/50000]  MSE: 0.000005  L1: 71.66  Loss: 0.000076\n",
      "Epoch [23500/50000]  MSE: 0.000016  L1: 71.13  Loss: 0.000087\n",
      "Epoch [24000/50000]  MSE: 0.000014  L1: 70.64  Loss: 0.000084\n",
      "Epoch [24500/50000]  MSE: 0.000001  L1: 70.05  Loss: 0.000071\n",
      "Epoch [25000/50000]  MSE: 0.000025  L1: 69.70  Loss: 0.000095\n",
      "Epoch [25500/50000]  MSE: 0.000019  L1: 69.68  Loss: 0.000089\n",
      "Epoch [26000/50000]  MSE: 0.000007  L1: 69.20  Loss: 0.000077\n",
      "Epoch [26500/50000]  MSE: 0.000023  L1: 69.12  Loss: 0.000092\n",
      "Epoch [27000/50000]  MSE: 0.000001  L1: 68.83  Loss: 0.000070\n",
      "Epoch [27500/50000]  MSE: 0.000002  L1: 68.58  Loss: 0.000071\n",
      "Epoch [28000/50000]  MSE: 0.000004  L1: 68.33  Loss: 0.000072\n",
      "Epoch [28500/50000]  MSE: 0.000004  L1: 68.18  Loss: 0.000072\n",
      "Epoch [29000/50000]  MSE: 0.000020  L1: 68.14  Loss: 0.000088\n",
      "Epoch [29500/50000]  MSE: 0.000013  L1: 67.95  Loss: 0.000081\n",
      "Epoch [30000/50000]  MSE: 0.000016  L1: 67.60  Loss: 0.000083\n",
      "Epoch [30500/50000]  MSE: 0.000008  L1: 67.41  Loss: 0.000075\n",
      "Epoch [31000/50000]  MSE: 0.000006  L1: 66.84  Loss: 0.000073\n",
      "Epoch [31500/50000]  MSE: 0.000004  L1: 66.75  Loss: 0.000071\n",
      "Epoch [32000/50000]  MSE: 0.000003  L1: 66.28  Loss: 0.000069\n",
      "Epoch [32500/50000]  MSE: 0.000051  L1: 66.10  Loss: 0.000117\n",
      "Epoch [33000/50000]  MSE: 0.000008  L1: 65.81  Loss: 0.000074\n",
      "Epoch [33500/50000]  MSE: 0.000010  L1: 65.45  Loss: 0.000076\n",
      "Epoch [34000/50000]  MSE: 0.000002  L1: 65.18  Loss: 0.000067\n",
      "Epoch [34500/50000]  MSE: 0.000004  L1: 65.42  Loss: 0.000069\n",
      "Epoch [35000/50000]  MSE: 0.000008  L1: 65.08  Loss: 0.000073\n",
      "Epoch [35500/50000]  MSE: 0.000008  L1: 65.06  Loss: 0.000073\n",
      "Epoch [36000/50000]  MSE: 0.000034  L1: 65.24  Loss: 0.000099\n",
      "Epoch [36500/50000]  MSE: 0.000010  L1: 64.90  Loss: 0.000075\n",
      "Epoch [37000/50000]  MSE: 0.000001  L1: 64.92  Loss: 0.000066\n",
      "Epoch [37500/50000]  MSE: 0.000007  L1: 64.99  Loss: 0.000072\n",
      "Epoch [38000/50000]  MSE: 0.000004  L1: 64.79  Loss: 0.000069\n",
      "Epoch [38500/50000]  MSE: 0.000006  L1: 64.81  Loss: 0.000071\n",
      "Epoch [39000/50000]  MSE: 0.000001  L1: 64.71  Loss: 0.000066\n",
      "Epoch [39500/50000]  MSE: 0.000009  L1: 64.71  Loss: 0.000073\n",
      "Epoch [40000/50000]  MSE: 0.000012  L1: 64.73  Loss: 0.000077\n",
      "Epoch [40500/50000]  MSE: 0.000103  L1: 65.33  Loss: 0.000168\n",
      "Epoch [41000/50000]  MSE: 0.000004  L1: 64.83  Loss: 0.000069\n",
      "Epoch [41500/50000]  MSE: 0.000008  L1: 65.32  Loss: 0.000073\n",
      "Epoch [42000/50000]  MSE: 0.000005  L1: 64.69  Loss: 0.000070\n",
      "Epoch [42500/50000]  MSE: 0.000028  L1: 64.49  Loss: 0.000092\n",
      "Epoch [43000/50000]  MSE: 0.000017  L1: 65.17  Loss: 0.000082\n",
      "Epoch [43500/50000]  MSE: 0.000002  L1: 64.65  Loss: 0.000066\n",
      "Epoch [44000/50000]  MSE: 0.000037  L1: 64.74  Loss: 0.000101\n",
      "Epoch [44500/50000]  MSE: 0.000033  L1: 65.77  Loss: 0.000098\n",
      "Epoch [45000/50000]  MSE: 0.000030  L1: 65.14  Loss: 0.000095\n",
      "Epoch [45500/50000]  MSE: 0.000029  L1: 64.37  Loss: 0.000093\n",
      "Epoch [46000/50000]  MSE: 0.000031  L1: 64.09  Loss: 0.000095\n",
      "Epoch [46500/50000]  MSE: 0.000026  L1: 64.04  Loss: 0.000090\n",
      "Epoch [47000/50000]  MSE: 0.000033  L1: 63.77  Loss: 0.000097\n",
      "Epoch [47500/50000]  MSE: 0.000025  L1: 63.94  Loss: 0.000089\n",
      "Epoch [48000/50000]  MSE: 0.000029  L1: 63.93  Loss: 0.000093\n",
      "Epoch [48500/50000]  MSE: 0.000024  L1: 64.03  Loss: 0.000088\n",
      "Epoch [49000/50000]  MSE: 0.000036  L1: 64.12  Loss: 0.000100\n",
      "Epoch [49500/50000]  MSE: 0.000022  L1: 64.31  Loss: 0.000086\n",
      "\n",
      "予測結果：\n",
      "[[0.9278692  0.11491199 0.13877326]\n",
      " [0.41062713 0.73973995 0.26787472]\n",
      " [0.21901728 0.32439506 0.64028263]\n",
      " [0.96187353 0.91909456 0.07821557]\n",
      " [0.43477577 0.79873    0.86394286]\n",
      " [0.72030675 0.31261992 0.6175981 ]\n",
      " [0.00408089 0.00195013 0.00300442]\n",
      " [0.5058205  0.4957285  0.49388134]\n",
      " [0.9795735  0.9843354  0.9794196 ]\n",
      " [0.4520933  0.30799916 0.25587058]\n",
      " [0.7637813  0.558844   0.4888602 ]\n",
      " [0.3560276  0.47174275 0.60731816]\n",
      " [0.35776985 0.42181414 0.25035644]\n",
      " [0.5173577  0.4960659  0.6848042 ]\n",
      " [0.37706614 0.73957115 0.67538816]\n",
      " [0.8704238  0.47568095 0.18551072]\n",
      " [0.26928645 0.35385638 0.6527013 ]\n",
      " [0.7762206  0.3096304  0.37169802]\n",
      " [0.36445928 0.23062986 0.40853655]\n",
      " [0.61409813 0.7342352  0.23102802]\n",
      " [0.89138556 0.62549466 0.15591809]\n",
      " [0.15213288 0.2447977  0.5606904 ]\n",
      " [0.23472606 0.5771095  0.273581  ]\n",
      " [0.70162123 0.21308172 0.22558019]\n",
      " [0.91576946 0.773239   0.10446203]\n",
      " [0.7513888  0.30746657 0.5683895 ]\n",
      " [0.00488004 0.5186711  0.6448067 ]\n",
      " [0.96009564 0.95677197 0.93190277]\n",
      " [0.78635615 0.7888657  0.7848634 ]\n",
      " [0.62756205 0.63318    0.63362086]\n",
      " [0.47532862 0.47719103 0.47796494]\n",
      " [0.3304373  0.33009684 0.33601213]\n",
      " [0.19582362 0.19327849 0.19319302]]\n",
      "\n",
      "C++ 用のパラメータファイル (nomal_model_parameters_1.h.h) を作成しました。\n"
     ]
    }
   ],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "PARAMETER = \n",
    "\n",
    "# モデル定義\n",
    "class ColorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3,PARAMETER),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(PARAMETER, PARAMETER),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(PARAMETER, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = ColorNet()\n",
    "\n",
    "# 損失関数（MSE）\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# L1正則化の係数\n",
    "lambda_l1 = 1e-6\n",
    "\n",
    "# 学習ループ\n",
    "epochs = 50000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_tensor)\n",
    "    mse = criterion(outputs, Y_tensor)\n",
    "\n",
    "    # --- L1 正則化-----------------------------\n",
    "    l1 = torch.tensor(0.0, requires_grad=False)\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            l1 = l1 + p.abs().sum()\n",
    "    loss = mse + lambda_l1 * l1\n",
    "    # -----------------------------------------\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch [{epoch}/{epochs}]  MSE: {mse.item():.6f}  L1: {l1.item():.2f}  Loss: {loss.item():.6f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_tensor)\n",
    "    print(\"\\n予測結果：\")\n",
    "    print(predictions.numpy())\n",
    "\n",
    "def convert_to_cpp_array(tensor: torch.Tensor, name: str, dtype: str = \"float\"):\n",
    "    flat = tensor.detach().numpy().flatten()\n",
    "    array_str = f\"{dtype} {name}[] = {{\"\n",
    "    array_str += \", \".join(map(str, flat))\n",
    "    array_str += \"};\"\n",
    "    return array_str\n",
    "\n",
    "cpp_code = \"\"\n",
    "\n",
    "layer_idx = 1\n",
    "for layer in model.model:\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        cpp_code += convert_to_cpp_array(layer.weight, f\"weight_{layer_idx}\") + \"\\n\"\n",
    "        cpp_code += convert_to_cpp_array(layer.bias,   f\"bias_{layer_idx}\") + \"\\n\"\n",
    "        layer_idx += 1\n",
    "\n",
    "filename = f\"h_faile_data/nomal_model_parameters_{PARAMETER}.h\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(cpp_code)\n",
    "\n",
    "print(\"\\nC++ 用のパラメータファイル (nomal_model_parameters_1.h.h) を作成しました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686528fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80038137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e3e85ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C++ 用のCSR + int8 量子化パラメータファイルを作成しました。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "THRESHOLD = 0.005\n",
    "filename = f\"model_sparse_csr_int8/model_sparse{THRESHOLD}_csr_int8_param60.h\"\n",
    "\n",
    "# --- スパース化（小さい値を0にする） ---\n",
    "def sparsify_array(arr: np.ndarray, threshold: float = THRESHOLD):\n",
    "    arr_copy = arr.copy()\n",
    "    arr_copy[np.abs(arr_copy) < threshold] = 0\n",
    "    return arr_copy\n",
    "\n",
    "# --- int8量子化関数 ---\n",
    "def int8_quantize(arr: np.ndarray, scale=None):\n",
    "    if scale is None:\n",
    "        max_abs = float(np.max(np.abs(arr)))\n",
    "        scale = max_abs / 127.0 if max_abs != 0 else 1.0\n",
    "    q = np.round(arr / scale).astype(np.int32)\n",
    "    q = np.clip(q, -128, 127).astype(np.int8)\n",
    "    return q, scale\n",
    "\n",
    "\n",
    "# --- 配列を量子化する共通関数 ---\n",
    "def quantize_array(arr: np.ndarray, threshold: float = THRESHOLD):\n",
    "    arr_sparse = sparsify_array(arr, threshold)\n",
    "    nonzero_mask = arr_sparse != 0\n",
    "    q_data, scale = int8_quantize(arr_sparse[nonzero_mask])\n",
    "    arr_quantized = np.zeros_like(arr_sparse, dtype=np.int8)\n",
    "    arr_quantized[nonzero_mask] = q_data\n",
    "    return arr_quantized, scale\n",
    "\n",
    "# --- CSR配列生成関数 ---\n",
    "def to_csr(tensor: torch.Tensor, name: str, threshold: float = THRESHOLD):\n",
    "    arr = tensor.detach().numpy()\n",
    "    arr_quantized, scale = quantize_array(arr, threshold)\n",
    "    csr = csr_matrix(arr_quantized)\n",
    "\n",
    "    data_str = f\"int8_t {name}_data[] = {{ \" + \", \".join(map(str, csr.data)) + \" };\"\n",
    "    indices_str = f\"int {name}_indices[] = {{ \" + \", \".join(map(str, csr.indices)) + \" };\"\n",
    "    indptr_str = f\"int {name}_indptr[] = {{ \" + \", \".join(map(str, csr.indptr)) + \" };\"\n",
    "    shape_str = f\"int {name}_shape[2] = {{{csr.shape[0]}, {csr.shape[1]}}};\"\n",
    "    scale_str = f\"float {name}_scale = {scale};\"\n",
    "\n",
    "    return \"\\n\".join([data_str, indices_str, indptr_str, shape_str, scale_str])\n",
    "\n",
    "# --- Dense配列生成関数 ---\n",
    "def to_dense(tensor: torch.Tensor, name: str, threshold: float = THRESHOLD):\n",
    "    arr = tensor.detach().numpy().flatten()\n",
    "    arr_quantized, scale = quantize_array(arr, threshold)\n",
    "    array_str = f\"int8_t {name}[] = {{ \" + \", \".join(map(str, arr_quantized)) + \" };\"\n",
    "    scale_str = f\"float {name}_scale = {scale};\"\n",
    "    return \"\\n\".join([array_str, scale_str])\n",
    "\n",
    "# --- C++コード生成 ---\n",
    "cpp_code = \"\"\n",
    "layer_idx = 1\n",
    "\n",
    "for layer in model.model:\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        cpp_code += f\"// Layer {layer_idx}\\n\"\n",
    "        cpp_code += to_csr(layer.weight, f\"weight_{layer_idx}\") + \"\\n\"\n",
    "        cpp_code += to_dense(layer.bias, f\"bias_{layer_idx}\") + \"\\n\\n\"\n",
    "        layer_idx += 1\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(cpp_code)\n",
    "\n",
    "print(\"\\nC++ 用のCSR + int8 量子化パラメータファイルを作成しました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fd438d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "スパース化のみ適用: model_parameters_sparse_only.h を作成しました。\n",
      "量子化のみ適用: model_parameters_quantize_only.h を作成しました。\n",
      "CSR化のみ適用: model_parameters_csr_only.h を作成しました。\n"
     ]
    }
   ],
   "source": [
    "# --- 個別適用用の関数群 ---\n",
    "\n",
    "# 1. スパース化のみを適用したものを出力する関数\n",
    "def to_sparse_only(tensor: torch.Tensor, name: str, threshold: float = 0.005):\n",
    "    \"\"\"\n",
    "    スパース化のみを適用（float型のまま）\n",
    "    \"\"\"\n",
    "    arr = tensor.detach().numpy()\n",
    "    arr_sparse = sparsify_array(arr, threshold)\n",
    "    \n",
    "    # Dense形式で出力（float型）\n",
    "    flat = arr_sparse.flatten()\n",
    "    array_str = f\"float {name}[] = {{ \" + \", \".join(map(str, flat)) + \" };\"\n",
    "    shape_str = f\"int {name}_shape[2] = {{{arr_sparse.shape[0]}, {arr_sparse.shape[1]}}};\"\n",
    "    \n",
    "    return \"\\n\".join([array_str, shape_str])\n",
    "\n",
    "# 2. 量子化のみを適用したものを出力する関数（スパース化なし）\n",
    "def to_quantize_only(tensor: torch.Tensor, name: str, scale=None):\n",
    "    \"\"\"\n",
    "    量子化のみを適用（スパース化なし、元の配列をそのまま量子化）\n",
    "    \"\"\"\n",
    "    arr = tensor.detach().numpy()\n",
    "    \n",
    "    # スパース化せずに直接量子化\n",
    "    if scale is None:\n",
    "        max_abs = float(np.max(np.abs(arr)))\n",
    "        scale = max_abs / 127.0 if max_abs != 0 else 1.0\n",
    "    \n",
    "    arr_quantized, scale = int8_quantize(arr, scale)\n",
    "    \n",
    "    # Dense形式で出力（int8型）\n",
    "    flat = arr_quantized.flatten()\n",
    "    array_str = f\"int8_t {name}[] = {{ \" + \", \".join(map(str, flat)) + \" };\"\n",
    "    scale_str = f\"float {name}_scale = {scale};\"\n",
    "    shape_str = f\"int {name}_shape[2] = {{{arr_quantized.shape[0]}, {arr_quantized.shape[1]}}};\"\n",
    "    \n",
    "    return \"\\n\".join([array_str, scale_str, shape_str])\n",
    "\n",
    "# 3. CSR化のみを適用したものを出力する関数（スパース化・量子化なし、float型のまま）\n",
    "def to_csr_only(tensor: torch.Tensor, name: str):\n",
    "    \"\"\"\n",
    "    CSR化のみを適用（スパース化・量子化なし、元のfloat型のまま）\n",
    "    \"\"\"\n",
    "    arr = tensor.detach().numpy()\n",
    "    csr = csr_matrix(arr)\n",
    "\n",
    "    data_str = f\"float {name}_data[] = {{ \" + \", \".join(map(str, csr.data)) + \" };\"\n",
    "    indices_str = f\"int {name}_indices[] = {{ \" + \", \".join(map(str, csr.indices)) + \" };\"\n",
    "    indptr_str = f\"int {name}_indptr[] = {{ \" + \", \".join(map(str, csr.indptr)) + \" };\"\n",
    "    shape_str = f\"int {name}_shape[2] = {{{csr.shape[0]}, {csr.shape[1]}}};\"\n",
    "\n",
    "    return \"\\n\".join([data_str, indices_str, indptr_str, shape_str])\n",
    "\n",
    "# --- 各手法を個別に適用してファイル出力 ---\n",
    "\n",
    "# スパース化のみ\n",
    "cpp_code_sparse = \"\"\n",
    "layer_idx = 1\n",
    "for layer in model.model:\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        cpp_code_sparse += f\"// Layer {layer_idx} - Sparse Only\\n\"\n",
    "        cpp_code_sparse += to_sparse_only(layer.weight, f\"weight_{layer_idx}\") + \"\\n\"\n",
    "        # biasはスパース化しない（通常はそのまま）\n",
    "        bias_arr = layer.bias.detach().numpy()\n",
    "        bias_str = f\"float bias_{layer_idx}[] = {{ \" + \", \".join(map(str, bias_arr)) + \" };\"\n",
    "        cpp_code_sparse += bias_str + \"\\n\\n\"\n",
    "        layer_idx += 1\n",
    "\n",
    "with open(\"model_sparse//model_parameters_sparse_only_0.005.h\", \"w\") as f:\n",
    "    f.write(cpp_code_sparse)\n",
    "print(\"スパース化のみ適用: model_parameters_sparse_only.h を作成しました。\")\n",
    "\n",
    "# 量子化のみ\n",
    "cpp_code_quantize = \"\"\n",
    "layer_idx = 1\n",
    "for layer in model.model:\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        cpp_code_quantize += f\"// Layer {layer_idx} - Quantize Only\\n\"\n",
    "        cpp_code_quantize += to_quantize_only(layer.weight, f\"weight_{layer_idx}\") + \"\\n\"\n",
    "        # biasも量子化\n",
    "        bias_arr = layer.bias.detach().numpy()\n",
    "        bias_quantized, bias_scale = int8_quantize(bias_arr)\n",
    "        bias_str = f\"int8_t bias_{layer_idx}[] = {{ \" + \", \".join(map(str, bias_quantized)) + \" };\"\n",
    "        bias_scale_str = f\"float bias_{layer_idx}_scale = {bias_scale};\"\n",
    "        cpp_code_quantize += bias_str + \"\\n\" + bias_scale_str + \"\\n\\n\"\n",
    "        layer_idx += 1\n",
    "\n",
    "with open(\"models/model_parameters_quantize_only.h\", \"w\") as f:\n",
    "    f.write(cpp_code_quantize)\n",
    "print(\"量子化のみ適用: model_parameters_quantize_only.h を作成しました。\")\n",
    "\n",
    "# CSR化のみ\n",
    "cpp_code_csr = \"\"\n",
    "layer_idx = 1\n",
    "for layer in model.model:\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        cpp_code_csr += f\"// Layer {layer_idx} - CSR Only\\n\"\n",
    "        cpp_code_csr += to_csr_only(layer.weight, f\"weight_{layer_idx}\") + \"\\n\"\n",
    "        # biasはDense形式のまま\n",
    "        bias_arr = layer.bias.detach().numpy()\n",
    "        bias_str = f\"float bias_{layer_idx}[] = {{ \" + \", \".join(map(str, bias_arr)) + \" };\"\n",
    "        cpp_code_csr += bias_str + \"\\n\\n\"\n",
    "        layer_idx += 1\n",
    "\n",
    "with open(\"models/model_parameters_csr_only.h\", \"w\") as f:\n",
    "    f.write(cpp_code_csr)\n",
    "print(\"CSR化のみ適用: model_parameters_csr_only.h を作成しました。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
